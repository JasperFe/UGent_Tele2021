{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Teledetectie Practica 2020 Welkom bij de cursussite voor de practica van Teledetectie 2020. Doorheen de practica wordt deze site aangevuld met nieuwe documentatie, extra informatie en FAQ's. Remote Sensing | Spatial Analysis lab (REMOSA)","title":"Home"},{"location":"index.html#teledetectie-practica-2020","text":"Welkom bij de cursussite voor de practica van Teledetectie 2020. Doorheen de practica wordt deze site aangevuld met nieuwe documentatie, extra informatie en FAQ's. Remote Sensing | Spatial Analysis lab (REMOSA)","title":"Teledetectie Practica 2020"},{"location":"P3-Sentinel2.html","text":"The ESA Copernicus programme Copernicus is the EU's Earth Observation Programme, looking at our planet and its environment for the ultimate benefit of all European citizens. The overall goal is achieving a global, continuous, autonomous, high quality, wide range Earth observation capacity. Under the copernicus programme, ESA is developing a series of next-generation Earth observation missions under the name of 'Sentinel' programme. This Sentinel Programme, consists of multiple satellites, each focussing on a different aspect of Earth observation: atmospheric, Oceanic and Land monitoring: Current Sentinel satellites, with their main goal. (Source: ESA) In this practical will focus on the multispectral imagery taken by Sentinel 2 satellites. The Sentinel-2 mission Sentinel-2 is the copernicus Earth observation mission by ESA with the goal to perform terrestrial observations in support of services such as forest monitoring, land cover changes detection, and natural disaster management. It consists of two identical satellites, Sentinel-2A and Sentinel-2B. An interesting infograph about the Sentinel-2 mission can be found here . The Sentinel-2 mission has the following capabilities: Multi-spectral data with 13 bands in the visible, near infrared, and short wave infrared part of the spectrum Systematic global coverage of land surfaces from 56\u00b0 S to 84\u00b0 N, coastal waters, and all of the Mediterranean Sea Revisiting every 5 days under the same viewing angles. Spatial resolution of 10 m, 20 m and 60 m 290 km field of view Free and open data policy To achieve frequent revisits and high mission availability, the two identical Sentinel-2 satellites (Sentinel-2A and Sentinel-2B) operate simultaneously. The orbit is Sun synchronous at 786 km (488 mi) altitude. Sentinel 2 data download All data captured by the ESA copernicus Sentinel program are completely freely available to the public. The most convinient way to download Sentinel data is through the Copernicus Open Access Hub, a platform dedicate to provide easy acces to the user. For this, an user account is required. To register go to registration page . To acces the data hub, go to https://scihub.copernicus.eu/ . Ex 3.1 - Downloading a Sentinel 2 Level 1C image In the first exercise, you will download an image from the Copernicus Open Access Hub. Go to https://scihub.copernicus.eu/ Klick \u2018Open hub\u2019 to access the Interactive Graphical User Interface Log in (or create an account) Zoom to Bel\u00e8m, a city in the north of Brazil, close to the gateway of the Amazon river Switch the \u2018Open street\u2019 view to \u2018sentinel-2 cloudless + Overlay\u2019 view Switch to \u2018navigation mode\u2019 Draw a rectangle around Bel\u00e8m: At the button \u2018Insert search criteria\u2019: go for \u2018advanced search\u2019 Look for a 2020 image (sensing period), Sentinel-2A, level 1C (product type) with a cloud cover of maximum 10%. Then click on the search button: Click on the search button Search for an image that contains the major part of the city (inspect the image in a quick look view ) Download this image to a folder on your computer. Sentinel file naming convention The naming of the Sentinel products follows the Compact Naming Convention: MMM_MSIXXX_YYYYMMDDHHMMSS_Nxxyy_ROOO_Txxxxx_\"Product Discriminator\".SAFE Where: MMM : is the mission ID (S2A/S2B) MSIXXX : MSIL1C denotes the Level-1C product level/ MSIL2A denotes the Level-2A product level (see \u2018radiometric correction\u2019). YYYYMMDDTHHMMSS : the datatake sensing start time Nxxyy : the Processing Baseline number (e.g. N0204) ROOO : Relative Orbit number (R001 - R143) Txxxxx : Tile Number field .SAFE : Product Format (Standard Archive Format for Europe) The products contain two dates. The first date (YYYYMMDDHHMMSS) is the datatake sensing time. The second date is the \"Product Discriminator\" field, which is 15 characters in length, and is used to distinguish between different end user products from the same datatake. Depending on the instance, the time in this field can be earlier or slightly later than the datatake sensing time. Thus, the following filename \u2018S2A_MSIL1C_20170105T013442_N0204_R031_T53NMJ_20170105T013443.SAFE\u2019 identifies a Level-1C product acquired by Sentinel-2A on the 5th of January, 2017 at 1:34:42 AM. It was acquired over Tile 53NMJ(2) during Relative Orbit 031, and processed with PDGS Processing Baseline 02.04. Ex 3.2 - naming convention Explain the different components of the name: S2A_MSIL1C_20180812T143751_N0206_R096_T19KGA_20180812T182110 (example) Other useful RS data sources Earth Explorer ESA has Sentinel-2, NASA has Landsat. However Landsat has a lower spatial resolution of 30m compared to the 10m of Sentinel-2 and Sentinel 2 has more spectral bands, Landsat imagery is probably the most used EO-data in science. This is because the Landsat program is the longest-running Earth Observation program of the entire Earth. Landsat-1 was already launched on July 23, 1972 resulting. Due to this difference, Landsat is on this moment more useful for historic land-change assessments than Sentinel-2 (launched in 2015). Landsat data is also freely avaible to the public. For this, the United States Geological Survey has created a data portal with extensive collections of EO data, with Landsat satellite imagery, Radar data, UAS data, digital line graphs, digital elevation model data, aerial photos, Sentinel satellite data, ... Link: earthexplorer.usgs.gov Other data sources Following website contains a nice overview of online free EO data sources: https://www.geoawesomeness.com/list-of-top-10-sources-of-free-remote-sensing-data/","title":"Sentinel-2 intro and download"},{"location":"P3-Sentinel2.html#the-esa-copernicus-programme","text":"Copernicus is the EU's Earth Observation Programme, looking at our planet and its environment for the ultimate benefit of all European citizens. The overall goal is achieving a global, continuous, autonomous, high quality, wide range Earth observation capacity. Under the copernicus programme, ESA is developing a series of next-generation Earth observation missions under the name of 'Sentinel' programme. This Sentinel Programme, consists of multiple satellites, each focussing on a different aspect of Earth observation: atmospheric, Oceanic and Land monitoring: Current Sentinel satellites, with their main goal. (Source: ESA) In this practical will focus on the multispectral imagery taken by Sentinel 2 satellites.","title":"The ESA Copernicus programme"},{"location":"P3-Sentinel2.html#the-sentinel-2-mission","text":"Sentinel-2 is the copernicus Earth observation mission by ESA with the goal to perform terrestrial observations in support of services such as forest monitoring, land cover changes detection, and natural disaster management. It consists of two identical satellites, Sentinel-2A and Sentinel-2B. An interesting infograph about the Sentinel-2 mission can be found here . The Sentinel-2 mission has the following capabilities: Multi-spectral data with 13 bands in the visible, near infrared, and short wave infrared part of the spectrum Systematic global coverage of land surfaces from 56\u00b0 S to 84\u00b0 N, coastal waters, and all of the Mediterranean Sea Revisiting every 5 days under the same viewing angles. Spatial resolution of 10 m, 20 m and 60 m 290 km field of view Free and open data policy To achieve frequent revisits and high mission availability, the two identical Sentinel-2 satellites (Sentinel-2A and Sentinel-2B) operate simultaneously. The orbit is Sun synchronous at 786 km (488 mi) altitude.","title":"The Sentinel-2 mission"},{"location":"P3-Sentinel2.html#sentinel-2-data-download","text":"All data captured by the ESA copernicus Sentinel program are completely freely available to the public. The most convinient way to download Sentinel data is through the Copernicus Open Access Hub, a platform dedicate to provide easy acces to the user. For this, an user account is required. To register go to registration page . To acces the data hub, go to https://scihub.copernicus.eu/ . Ex 3.1 - Downloading a Sentinel 2 Level 1C image In the first exercise, you will download an image from the Copernicus Open Access Hub. Go to https://scihub.copernicus.eu/ Klick \u2018Open hub\u2019 to access the Interactive Graphical User Interface Log in (or create an account) Zoom to Bel\u00e8m, a city in the north of Brazil, close to the gateway of the Amazon river Switch the \u2018Open street\u2019 view to \u2018sentinel-2 cloudless + Overlay\u2019 view Switch to \u2018navigation mode\u2019 Draw a rectangle around Bel\u00e8m: At the button \u2018Insert search criteria\u2019: go for \u2018advanced search\u2019 Look for a 2020 image (sensing period), Sentinel-2A, level 1C (product type) with a cloud cover of maximum 10%. Then click on the search button: Click on the search button Search for an image that contains the major part of the city (inspect the image in a quick look view ) Download this image to a folder on your computer.","title":"Sentinel 2 data download"},{"location":"P3-Sentinel2.html#sentinel-file-naming-convention","text":"The naming of the Sentinel products follows the Compact Naming Convention: MMM_MSIXXX_YYYYMMDDHHMMSS_Nxxyy_ROOO_Txxxxx_\"Product Discriminator\".SAFE Where: MMM : is the mission ID (S2A/S2B) MSIXXX : MSIL1C denotes the Level-1C product level/ MSIL2A denotes the Level-2A product level (see \u2018radiometric correction\u2019). YYYYMMDDTHHMMSS : the datatake sensing start time Nxxyy : the Processing Baseline number (e.g. N0204) ROOO : Relative Orbit number (R001 - R143) Txxxxx : Tile Number field .SAFE : Product Format (Standard Archive Format for Europe) The products contain two dates. The first date (YYYYMMDDHHMMSS) is the datatake sensing time. The second date is the \"Product Discriminator\" field, which is 15 characters in length, and is used to distinguish between different end user products from the same datatake. Depending on the instance, the time in this field can be earlier or slightly later than the datatake sensing time. Thus, the following filename \u2018S2A_MSIL1C_20170105T013442_N0204_R031_T53NMJ_20170105T013443.SAFE\u2019 identifies a Level-1C product acquired by Sentinel-2A on the 5th of January, 2017 at 1:34:42 AM. It was acquired over Tile 53NMJ(2) during Relative Orbit 031, and processed with PDGS Processing Baseline 02.04. Ex 3.2 - naming convention Explain the different components of the name: S2A_MSIL1C_20180812T143751_N0206_R096_T19KGA_20180812T182110 (example)","title":"Sentinel file naming convention"},{"location":"P3-Sentinel2.html#other-useful-rs-data-sources","text":"","title":"Other useful RS data sources"},{"location":"P3-Sentinel2.html#earth-explorer","text":"ESA has Sentinel-2, NASA has Landsat. However Landsat has a lower spatial resolution of 30m compared to the 10m of Sentinel-2 and Sentinel 2 has more spectral bands, Landsat imagery is probably the most used EO-data in science. This is because the Landsat program is the longest-running Earth Observation program of the entire Earth. Landsat-1 was already launched on July 23, 1972 resulting. Due to this difference, Landsat is on this moment more useful for historic land-change assessments than Sentinel-2 (launched in 2015). Landsat data is also freely avaible to the public. For this, the United States Geological Survey has created a data portal with extensive collections of EO data, with Landsat satellite imagery, Radar data, UAS data, digital line graphs, digital elevation model data, aerial photos, Sentinel satellite data, ... Link: earthexplorer.usgs.gov","title":"Earth Explorer"},{"location":"P3-Sentinel2.html#other-data-sources","text":"Following website contains a nice overview of online free EO data sources: https://www.geoawesomeness.com/list-of-top-10-sources-of-free-remote-sensing-data/","title":"Other data sources"},{"location":"P3-Snap-intro.html","text":"About SNAP SNAP, the SeNtinel Application Platform is developed by the ESA specifically to process Sentinel-imagery, however also other remotey sensed images can be read. The current version is 8.0.0 . SNAP is a relatively new software especially designed for the analysis of Sentinel products (Sentinel 2A was launched in 2015) and hence still contains some bugs (especially for mac-users, might try the older version 7.0.0). Not all applications are supported that you will find in classic Image Processing programs such as ENVI, but it is very user friendly and ideal to introduce you to satellite image processing. Also, it is free! Overview We will use SNAP to examine some image composites, and necessary preprocessing steps. After that, we will do most other processing with Google Earth Engine. Excercise: Opening a Sentinel-2 image in snap Open the sentinel image that you have downloaded (you do not need to unzip it). You can do this in several ways: Drag and drop the zip folder in the Products explorer Click file > Open Products and browse to your zip-folder Click and browse to your zip-folder. Unfold the image folder. Explore the files included. Open the Blue, Green, Red and NIR image. Test the tile buttons. Make sure you can see the four images simultaneously: Explore the navigation panel. Sentinel 2 Bands Let's have a quick look at the specifications of a Sentinel-2 image. There are 13 Sentinel 2 bands in total, with a resolution of 10, 20 or 60m: The navigation window The Navigation Window is used to move the viewport of an Image View, to zoom in and out of it and to rotate the image in steps of 5 degrees using the spinner control below the image preview. The current viewport is depicted by a semi-transparent rectangle, which can be dragged in order to move the viewport to another location. The navigation window In the bottom left, you will find the zoom factor: zoom is relative to the drawing extents. A scale factor of: 1 shows a part of the image 2 shows entities twice as large 0.5 shows entities half as large The text box at the left side of slider can be used to adjust the zoom factor manually. The Navigation window additionally provides the following features via its tool buttons (top right): Zoom In : Zooms in by a factor of 1.2. Zoom Out : Zooms out by a factor of 1/1.2. Zoom Actual Pixel : Sets the zoom factor to the default value so that the size of an image pixel has the same size of a display pixel. Zoom All : Adjusts the viewport to cover the entire image. Synchronise Views : Synchronises the viewports of all compatible image views. Synchronise Cursor : Displays a synchronised cursor on all opened image views. You can also zoom the images by scrolling on the image, or by clicking in the toolbar. Zoom factor vs Representative Fraction The zoom factor is not the same as a Representative Fraction (RF) , which is often used to indicate the scale of a map. The RF indicates the ratio between the number of units on the map to the number of units on the ground. The RF factor 1:100000 e.g. implies that one cm on map is equal to 1 km on land. Maps are described as either large-scale or small-scale. Large-scale maps show a smaller amount of area with a greater amount of detail. The geographic extent shown on a large-scale map is small. A large scaled map expressed as a representative scale would have a smaller number to the right of the ratio. For example, a large-scale map could have a RF scale of 1: 1,000. Large-scale maps are typically used to show neighbourhoods, a localize area, small towns, etc. Small-scale maps show a larger geographic area with few details on them. The RF scale of a small-scale map would have a much larger number to the right of the colon such as 1: 1,000,000. Small-scale maps are used to show the extent of an entire country, region, or continent. Zoom to the Airport Explore the World View panel . The red rectangle indicates the position of the image on the globe. The Colour Manipulation tool The colour manipulation tool window window is used to modify the colours used in the image. If you are opening an Image View of a data product's band, the Sentinel Toolbox either loads image settings from the product itself (BEAM-DIMAP format only) or uses default colour settings. In the Colour manipulation panel, explore the histogram. On the image, zoom to the airport and adjust the contrast. Restore the contrast afterwards. Pixel info view If you click on the tab 'Pixel View' (right to the product explorer), pixel information will be displayed while you move the mouse over the band image view.","title":"Introduction to SNAP"},{"location":"P3-Snap-intro.html#about-snap","text":"SNAP, the SeNtinel Application Platform is developed by the ESA specifically to process Sentinel-imagery, however also other remotey sensed images can be read. The current version is 8.0.0 . SNAP is a relatively new software especially designed for the analysis of Sentinel products (Sentinel 2A was launched in 2015) and hence still contains some bugs (especially for mac-users, might try the older version 7.0.0). Not all applications are supported that you will find in classic Image Processing programs such as ENVI, but it is very user friendly and ideal to introduce you to satellite image processing. Also, it is free!","title":"About SNAP"},{"location":"P3-Snap-intro.html#overview","text":"We will use SNAP to examine some image composites, and necessary preprocessing steps. After that, we will do most other processing with Google Earth Engine. Excercise: Opening a Sentinel-2 image in snap Open the sentinel image that you have downloaded (you do not need to unzip it). You can do this in several ways: Drag and drop the zip folder in the Products explorer Click file > Open Products and browse to your zip-folder Click and browse to your zip-folder. Unfold the image folder. Explore the files included. Open the Blue, Green, Red and NIR image. Test the tile buttons. Make sure you can see the four images simultaneously: Explore the navigation panel. Sentinel 2 Bands Let's have a quick look at the specifications of a Sentinel-2 image. There are 13 Sentinel 2 bands in total, with a resolution of 10, 20 or 60m:","title":"Overview"},{"location":"P3-Snap-intro.html#the-navigation-window","text":"The Navigation Window is used to move the viewport of an Image View, to zoom in and out of it and to rotate the image in steps of 5 degrees using the spinner control below the image preview. The current viewport is depicted by a semi-transparent rectangle, which can be dragged in order to move the viewport to another location. The navigation window In the bottom left, you will find the zoom factor: zoom is relative to the drawing extents. A scale factor of: 1 shows a part of the image 2 shows entities twice as large 0.5 shows entities half as large The text box at the left side of slider can be used to adjust the zoom factor manually. The Navigation window additionally provides the following features via its tool buttons (top right): Zoom In : Zooms in by a factor of 1.2. Zoom Out : Zooms out by a factor of 1/1.2. Zoom Actual Pixel : Sets the zoom factor to the default value so that the size of an image pixel has the same size of a display pixel. Zoom All : Adjusts the viewport to cover the entire image. Synchronise Views : Synchronises the viewports of all compatible image views. Synchronise Cursor : Displays a synchronised cursor on all opened image views. You can also zoom the images by scrolling on the image, or by clicking in the toolbar. Zoom factor vs Representative Fraction The zoom factor is not the same as a Representative Fraction (RF) , which is often used to indicate the scale of a map. The RF indicates the ratio between the number of units on the map to the number of units on the ground. The RF factor 1:100000 e.g. implies that one cm on map is equal to 1 km on land. Maps are described as either large-scale or small-scale. Large-scale maps show a smaller amount of area with a greater amount of detail. The geographic extent shown on a large-scale map is small. A large scaled map expressed as a representative scale would have a smaller number to the right of the ratio. For example, a large-scale map could have a RF scale of 1: 1,000. Large-scale maps are typically used to show neighbourhoods, a localize area, small towns, etc. Small-scale maps show a larger geographic area with few details on them. The RF scale of a small-scale map would have a much larger number to the right of the colon such as 1: 1,000,000. Small-scale maps are used to show the extent of an entire country, region, or continent. Zoom to the Airport Explore the World View panel . The red rectangle indicates the position of the image on the globe.","title":"The navigation window"},{"location":"P3-Snap-intro.html#the-colour-manipulation-tool","text":"The colour manipulation tool window window is used to modify the colours used in the image. If you are opening an Image View of a data product's band, the Sentinel Toolbox either loads image settings from the product itself (BEAM-DIMAP format only) or uses default colour settings. In the Colour manipulation panel, explore the histogram. On the image, zoom to the airport and adjust the contrast. Restore the contrast afterwards.","title":"The Colour Manipulation tool"},{"location":"P3-Snap-intro.html#pixel-info-view","text":"If you click on the tab 'Pixel View' (right to the product explorer), pixel information will be displayed while you move the mouse over the band image view.","title":"Pixel info view"},{"location":"P3-colour-composites-ctd.html","text":"Displaying more band combinations When you have performed an image resampling, open again the RGB image window in SNAP. You will notice that the list with possible band combinations is larger. Test some of the following band combinations and explore the colour differences. Which features are most clear on the following band combinations? Natural Colours: 4 3 2 False colour Infrared: 8 4 3 False colour Urban: 12 11 4 Agriculture: 11 8 2 Atmospheric penetration: 12 11 8a Healthy vegetation: 8 11 2 Land/Water: 8 11 4 Natural Colours with Atmospheric Removal: 12 8 3 Shortwave Infrared: 12 8 4 Vegetation Analysis: 11 8 4","title":"RGB colour composites 2"},{"location":"P3-colour-composites-ctd.html#displaying-more-band-combinations","text":"When you have performed an image resampling, open again the RGB image window in SNAP. You will notice that the list with possible band combinations is larger. Test some of the following band combinations and explore the colour differences. Which features are most clear on the following band combinations? Natural Colours: 4 3 2 False colour Infrared: 8 4 3 False colour Urban: 12 11 4 Agriculture: 11 8 2 Atmospheric penetration: 12 11 8a Healthy vegetation: 8 11 2 Land/Water: 8 11 4 Natural Colours with Atmospheric Removal: 12 8 3 Shortwave Infrared: 12 8 4 Vegetation Analysis: 11 8 4","title":"Displaying more band combinations"},{"location":"P3-colour-composites.html","text":"About colour composites Multispectral imagery, such as Sentinel-2, consists of several bands of data. As seen during in previous chapter, these bands can be displayed individually as a grey scale image (black = low reflectance, white = high reflectance), but they can also be displayed as a combination of three bands: a colour composite (NL: kleurcomposiet). When creating a colour composite: the three primary colours are used: red, green and blue. When they are combined in various proportions, different colours are produced per pixel. When 3 spectral bands (both visible as non-visible bands) are assigned to a primary colour, a colour composite is formed. By combining different proportions of the three primary colours Red, Green and Blue, various colours are created Two \"famous\" colour composites True Colour Composite The most straightforward colour composite is the true colour composite (also natural colour composite ), where the three visual primary colour bands of a multispectral image are assigned to their corresponding colour. For Sentinel 2, this composite is created as: Red: B4, Green: B3, Blue: B2. Sentinel-2 Normal Composite of Ghent. False Colour Composite Beside the 'normal' colour composites, any band of a multispectral satellite image can be assigned to the primary colour bands in a composite. In all those other cases, the colour of a target object on the image, will have a different colour compared to it's actual colour. The most famous of these is the False Colour Composite , where the NIR-band is assigned to the red colour, the red band to the green colour and the green band to the blue colour. It is very suitable to detect vegetation, since vegetation has a high reflectance in the NIR band. Clear water will appear dark-bluish, while turbid water (with a lot of sediments) will be cyan. Bare soils, roads and buildings may appear in various shades of blue, yellow or grey, depending on their composition. For Sentinel 2, this composite is created as: Red: B8, Green: B4, Blue: B3. Sentinel-2 False Colour Composite of Ghent. Opening a RGB image in SNAP Let's create our own image composites in SNAP! This is actually very easy to do. Just right-click on the image folder and click on 'Open RGB Image window': A window will appear with some possible S2 band combinations, but you can also create your own. Some typical S2 band combinations have their own name, such as (Red, Green, Blue): Natural Colour: 4 3 2 * False colour Infrared: 8 4 3 * False colour Urban: 12 11 4 Agriculture: 11 8 2 Atmospheric penetration: 12 11 8a * Healthy vegetation: 8 11 2 Land/Water: 8 11 4 Natural Colours with Atmospheric Removal: 12 8 3 Shortwave Infrared: 12 8 4 Vegetation Analysis: 11 8 4 With the current Sentinel-2 Level 1C-product open, only the band combinations with a * can now be displayed. Why is that? Excercise: open band composites Open the image as a natural colour composite Open the image as a false colour infrared composite Tile the images evenly and explore the difference in colour (for example in the areas with green vegetation).","title":"RGB colour composites"},{"location":"P3-colour-composites.html#about-colour-composites","text":"Multispectral imagery, such as Sentinel-2, consists of several bands of data. As seen during in previous chapter, these bands can be displayed individually as a grey scale image (black = low reflectance, white = high reflectance), but they can also be displayed as a combination of three bands: a colour composite (NL: kleurcomposiet). When creating a colour composite: the three primary colours are used: red, green and blue. When they are combined in various proportions, different colours are produced per pixel. When 3 spectral bands (both visible as non-visible bands) are assigned to a primary colour, a colour composite is formed. By combining different proportions of the three primary colours Red, Green and Blue, various colours are created","title":"About colour composites"},{"location":"P3-colour-composites.html#two-famous-colour-composites","text":"","title":"Two \"famous\" colour composites"},{"location":"P3-colour-composites.html#true-colour-composite","text":"The most straightforward colour composite is the true colour composite (also natural colour composite ), where the three visual primary colour bands of a multispectral image are assigned to their corresponding colour. For Sentinel 2, this composite is created as: Red: B4, Green: B3, Blue: B2. Sentinel-2 Normal Composite of Ghent.","title":"True Colour Composite"},{"location":"P3-colour-composites.html#false-colour-composite","text":"Beside the 'normal' colour composites, any band of a multispectral satellite image can be assigned to the primary colour bands in a composite. In all those other cases, the colour of a target object on the image, will have a different colour compared to it's actual colour. The most famous of these is the False Colour Composite , where the NIR-band is assigned to the red colour, the red band to the green colour and the green band to the blue colour. It is very suitable to detect vegetation, since vegetation has a high reflectance in the NIR band. Clear water will appear dark-bluish, while turbid water (with a lot of sediments) will be cyan. Bare soils, roads and buildings may appear in various shades of blue, yellow or grey, depending on their composition. For Sentinel 2, this composite is created as: Red: B8, Green: B4, Blue: B3. Sentinel-2 False Colour Composite of Ghent.","title":"False Colour Composite"},{"location":"P3-colour-composites.html#opening-a-rgb-image-in-snap","text":"Let's create our own image composites in SNAP! This is actually very easy to do. Just right-click on the image folder and click on 'Open RGB Image window': A window will appear with some possible S2 band combinations, but you can also create your own. Some typical S2 band combinations have their own name, such as (Red, Green, Blue): Natural Colour: 4 3 2 * False colour Infrared: 8 4 3 * False colour Urban: 12 11 4 Agriculture: 11 8 2 Atmospheric penetration: 12 11 8a * Healthy vegetation: 8 11 2 Land/Water: 8 11 4 Natural Colours with Atmospheric Removal: 12 8 3 Shortwave Infrared: 12 8 4 Vegetation Analysis: 11 8 4 With the current Sentinel-2 Level 1C-product open, only the band combinations with a * can now be displayed. Why is that? Excercise: open band composites Open the image as a natural colour composite Open the image as a false colour infrared composite Tile the images evenly and explore the difference in colour (for example in the areas with green vegetation).","title":"Opening a RGB image in SNAP"},{"location":"P3-image-preprocessing.html","text":"Radiometric & atmospheric correction Satellite images obtained by the sensing device are not directly usable. They need to go through a series of pre-processing before they are ready to use. The scheme below illustrates the pre-processing steps that Sentinel-images undergo before they are made available for the user. This includes geometric correction, some radiometric correction (noise reduction, defective pixels identification) the computation of cloud masks, etc. The outcome is a level 1C product, which is Top-Of-the-Atmosphere (TOA). TOA reflectances are subjected to radiometric bias caused by different lighting conditions, atmospheric interactions and viewing geometry. In order to relate reflectances to physical field properties, TOA reflectance values are conversed to BOA (Bottom Of Atmosphere) corrected reflectance values. This radiometric correction is an essential part in image processing. BOA, Sentinel processing level 2A, is available for the user (except for recent images) or can be created by the user itself, using the Sen2Cor freeware. Figure: A true color comparison of the surface reflectance product (top) and a top of atmosphere reflectance image (bottom) in adjacent scenes captured by the same satellite (Planet.com) In Snap, the conversion of level 1C TOA-reflectance to level 2A BOA-reflectance can be done through Sen2Cor (plug-in or stand-alone). Sen2Cor corrects the reflectance values based on (among others) \u2018look-up tables\u2019, these are tables that relate physical parameters to model coefficients. Parameters such as inclination and product type are sensor dependent (different for Landsat as for Sentinel or Spot). On board, optical satellites have some meteorological sensors that measure atmosphere features such as the air thickness and the amount of aerosols among others. This information is available as a \u2018header file\u2019 for each image. Since December 2018, users can download Level-2A processed products directly. In case of this exercise, we downloaded a Level 1C product. Thus, let\u2019s perform an atmospheric correction! Excercise: atmospheric correction with Sen2Cor In the folder where you have saved the image, unzip the Sentinel-image. Go to \u2018Optical\u2019 > \u2018Thematic Land Processing\u2019 > \u2018Sen2Cor processor\u2019 > \u2018Sen2Cor280\u2019 When you choose the source product, click on the \u2018\u2026\u2019, browse to the image and navigate to the \u2018MTD_MSIL1C.xml\u2019 product. In the tab \u2018processing parameters\u2019, set the resolution to \u2018ALL\u2019 The other processing parameters are by default taken from a combination of the image metadata (header file) and look-up tables. This is why you will normally use the default processing parameters. However, if you want to adjust these parameters, you can do that manually. Run Sen2Cor (!be patient, it will take a while to process the entire image.) Explore the outcome image (RGB). What differences do you see according to the original image? Installing 'Sen2Cor' plugin Possibly sen2cor isn\u2019t installed yet. To do this, go to \u2018Tools\u2019 > \u2018Plugins\u2019. During the first run, you\u2019ll get an error, after which an extra bundle will be installed. ). Intermezzo: Cloud Masks The image contain clouds. This means that there are some blind pixels, which lack information on the reflectance of the earth\u2019s surface at the sensing time. This phenomenon is very common in tropical areas with a rainy season. It is possible that over the whole period of the rainy season, you will not be able to obtain images with a cloud cover of less than 90%. In such cases, Radar imaging can be useful, but are complexer. An introduction to radar imaging will be given later in these practicals. Included in a Sentinel-2 image folder you can find some cloud masks at a resolution of 10m, 20m and 60m. These cloud masks enable the user to identify cloudy and cloud-free pixels. The masks include both dense clouds (opaque clouds) and cirrus clouds. These cloud masks are computed by a threshold algorithm. Below, the methods are described that identify the cloud pixels (for your information). Identification of dense clouds Dense clouds, also called opaque clouds, are characterised by a high reflectance in the blue spectral region (B2). The method used to identify dense cloud pixels is based on B2 reflectance threshold. To avoid false detection, mainly due to snow/cloud confusion, SWIR reflectance in B11 and B12 are also used. Snow and clouds both have a high reflectance in the blue. Cloud reflectance is high in the SWIR, whereas snow presents a low reflectance. Additional criteria based on B10 reflectance are added to avoid high altitude ice cloud and snow confusion (both having a low reflectance in the SWIR bands B11 and B12). At B10, there is a high atmospheric absorption band and only high altitude clouds are detected. However, this last criterion is only applied after a first detection of cloud pixel in the blue band where cirrus is transparent. Identification of cirrus clouds Cirrus clouds are thin, transparent or semi-transparent clouds, forming at high altitudes, approximately 6-7 km above the Earth's surface. The method of identifying cirrus cloud pixels from dense cloud pixel is based on two spectral criteria: (1) B10 corresponds to a high atmospheric absorption band: only high altitude clouds can be detected, (2) cirrus clouds, being semi-transparent, cannot be detected in the B2 blue band. A pixel with low reflectance in the B2 band and high reflectance in the B10 band has a good probability of being cirrus cloud but this is not a certainty. Some opaque clouds have a low reflectance in the blue and can be identified as cirrus cloud. To limit false detections (due to high reflectance in the blue or due to the fact that clouds are not spectrally registered), a filter using morphology-based operations is applied on both dense and cirrus cloud masks: (1) erosion, to remove isolated pixels, (2) dilatation, to fill the gap and extend clouds. If after morphology operations, a pixel is both dense and cirrus, the dense cloud mask prevails. Sen2Cor scene classification The Sen2Cor-processor you've runned for the atmospheric correction from the level 1C to the level 2A product also contains a scene classification algorithm. This algorithm creates a scene classification, where pixels als classified in some broad classes: Here, clouds are classified into 'cloud probability masks', which are in general more precise than the level 1C cloud masks. Excercise: Visualize cloud masks Visualize the cloud masks. If you look at the cloud masks, you will see that these are not very precise. These cloud masks are useful for rough estimations. Later we will see alternative ways to identify cloud pixels more precise. Resampling In order to display the other band combinations, some geometrical pre-processing is necessary. The bands have to be resampled to an equal resolution. The goal is to resample the image bands to 10m (you can take B2, B3, B4 or B8 as a reference band). This means that all other bands will be upsampled. Image resampling scheme. Top: upsampling (nearest neighbor). Bottom: Downsampling (minimum). Exercise: resampling In the product explorer, select the outcome image of Sen2Cor. Go to Raster > Geometric operations > Resampling . Select the \u2018Save as\u2026 BEAM-DIMAP\u2019 box. Browse to your directory. Choose a logical name for the target product. Resampling Parameters: Choose a reference band that has a resolution of 10m, or choose for a pixel resolution of 10m. Use an upsampling method of your choice (Read the help for more details on the different algorithms). Run resampling . Saving the images takes a lot of time. Again, be patient! Image Subsetting Processing an entire Sentinel image takes a lot of processing capacity and time (as you probably have noted already). Therefore, you will now learn how to only process a small part of the image. You can choose to reduce the spatial extent of the image, or you can choose to reduce the amount of bands in the image, or a combination of both. An important aspect is that creating a subset is only possible for bands that have the same size. Thus, this will only be possible after resampling . Excercise: subsetting an image Select the resampled image in the product explorer. Go to Raster > Subset. Select a spatial subset by choice (by adjusting the scene start and end). Make sure your spatial extent is substantially smaller than the original image. Snap Subsetting screen. Select only following bands: [B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12] You can see an estimation of the new required storage space. Snap Subsetting screen. Click OK Another option to make a subset is \u2018Spatial subset from view\u2019. Zoom in on your image. Rightclick and select \u2018Spatial subset from view\u2019. FYI: it is also possible to take a subset of an image, based on a vector layer. Mosaicing Mosaicing is the merging of several arbitrarily shaped images and often used to merge two neighbouring satellite images. Excercise: mosaicing Download an image that is located next to the image you are already working with, dating from the same time as the original image was taken. You can download it directly in Level 2A, thus skipping the sen2cor atmospheric correction. Resample the image. Go to raster > Geometric operations > Mosaicing Snap mosaicing screen. Add the two source products. Choose the directory in which you want to save the mosaic image. In the Map Projection Definition you can choose the Coordinate Reference System (CRS). Choose for UTM/WGS84 (automatic) Choose for a resolution of 10m. The input products don\u2019t need to be orthorectified (because they already are). In the tab \u2018Variables and Conditions\u2019, click the - symbol. Select Band 2,3,4 and 8 Run Mosaicing. Open the RGB-image of the product. Compare it to the two original images. Extra: Examine the example of Landsat satellite image after merging below. What went wrong when mosaicing images 1 and 2? Why is there a colour difference in 2 and 3? Why is there no observable colour difference in 2 and 4? Have you any idea how to eliminate the colour difference between 2 and 3, given that neighbouring satellite images always partly overlap? Landsat images mosaic","title":"Image preprocessing in SNAP"},{"location":"P3-image-preprocessing.html#radiometric-atmospheric-correction","text":"Satellite images obtained by the sensing device are not directly usable. They need to go through a series of pre-processing before they are ready to use. The scheme below illustrates the pre-processing steps that Sentinel-images undergo before they are made available for the user. This includes geometric correction, some radiometric correction (noise reduction, defective pixels identification) the computation of cloud masks, etc. The outcome is a level 1C product, which is Top-Of-the-Atmosphere (TOA). TOA reflectances are subjected to radiometric bias caused by different lighting conditions, atmospheric interactions and viewing geometry. In order to relate reflectances to physical field properties, TOA reflectance values are conversed to BOA (Bottom Of Atmosphere) corrected reflectance values. This radiometric correction is an essential part in image processing. BOA, Sentinel processing level 2A, is available for the user (except for recent images) or can be created by the user itself, using the Sen2Cor freeware. Figure: A true color comparison of the surface reflectance product (top) and a top of atmosphere reflectance image (bottom) in adjacent scenes captured by the same satellite (Planet.com) In Snap, the conversion of level 1C TOA-reflectance to level 2A BOA-reflectance can be done through Sen2Cor (plug-in or stand-alone). Sen2Cor corrects the reflectance values based on (among others) \u2018look-up tables\u2019, these are tables that relate physical parameters to model coefficients. Parameters such as inclination and product type are sensor dependent (different for Landsat as for Sentinel or Spot). On board, optical satellites have some meteorological sensors that measure atmosphere features such as the air thickness and the amount of aerosols among others. This information is available as a \u2018header file\u2019 for each image. Since December 2018, users can download Level-2A processed products directly. In case of this exercise, we downloaded a Level 1C product. Thus, let\u2019s perform an atmospheric correction! Excercise: atmospheric correction with Sen2Cor In the folder where you have saved the image, unzip the Sentinel-image. Go to \u2018Optical\u2019 > \u2018Thematic Land Processing\u2019 > \u2018Sen2Cor processor\u2019 > \u2018Sen2Cor280\u2019 When you choose the source product, click on the \u2018\u2026\u2019, browse to the image and navigate to the \u2018MTD_MSIL1C.xml\u2019 product. In the tab \u2018processing parameters\u2019, set the resolution to \u2018ALL\u2019 The other processing parameters are by default taken from a combination of the image metadata (header file) and look-up tables. This is why you will normally use the default processing parameters. However, if you want to adjust these parameters, you can do that manually. Run Sen2Cor (!be patient, it will take a while to process the entire image.) Explore the outcome image (RGB). What differences do you see according to the original image? Installing 'Sen2Cor' plugin Possibly sen2cor isn\u2019t installed yet. To do this, go to \u2018Tools\u2019 > \u2018Plugins\u2019. During the first run, you\u2019ll get an error, after which an extra bundle will be installed. ).","title":"Radiometric &amp; atmospheric correction"},{"location":"P3-image-preprocessing.html#intermezzo-cloud-masks","text":"The image contain clouds. This means that there are some blind pixels, which lack information on the reflectance of the earth\u2019s surface at the sensing time. This phenomenon is very common in tropical areas with a rainy season. It is possible that over the whole period of the rainy season, you will not be able to obtain images with a cloud cover of less than 90%. In such cases, Radar imaging can be useful, but are complexer. An introduction to radar imaging will be given later in these practicals. Included in a Sentinel-2 image folder you can find some cloud masks at a resolution of 10m, 20m and 60m. These cloud masks enable the user to identify cloudy and cloud-free pixels. The masks include both dense clouds (opaque clouds) and cirrus clouds. These cloud masks are computed by a threshold algorithm. Below, the methods are described that identify the cloud pixels (for your information). Identification of dense clouds Dense clouds, also called opaque clouds, are characterised by a high reflectance in the blue spectral region (B2). The method used to identify dense cloud pixels is based on B2 reflectance threshold. To avoid false detection, mainly due to snow/cloud confusion, SWIR reflectance in B11 and B12 are also used. Snow and clouds both have a high reflectance in the blue. Cloud reflectance is high in the SWIR, whereas snow presents a low reflectance. Additional criteria based on B10 reflectance are added to avoid high altitude ice cloud and snow confusion (both having a low reflectance in the SWIR bands B11 and B12). At B10, there is a high atmospheric absorption band and only high altitude clouds are detected. However, this last criterion is only applied after a first detection of cloud pixel in the blue band where cirrus is transparent. Identification of cirrus clouds Cirrus clouds are thin, transparent or semi-transparent clouds, forming at high altitudes, approximately 6-7 km above the Earth's surface. The method of identifying cirrus cloud pixels from dense cloud pixel is based on two spectral criteria: (1) B10 corresponds to a high atmospheric absorption band: only high altitude clouds can be detected, (2) cirrus clouds, being semi-transparent, cannot be detected in the B2 blue band. A pixel with low reflectance in the B2 band and high reflectance in the B10 band has a good probability of being cirrus cloud but this is not a certainty. Some opaque clouds have a low reflectance in the blue and can be identified as cirrus cloud. To limit false detections (due to high reflectance in the blue or due to the fact that clouds are not spectrally registered), a filter using morphology-based operations is applied on both dense and cirrus cloud masks: (1) erosion, to remove isolated pixels, (2) dilatation, to fill the gap and extend clouds. If after morphology operations, a pixel is both dense and cirrus, the dense cloud mask prevails. Sen2Cor scene classification The Sen2Cor-processor you've runned for the atmospheric correction from the level 1C to the level 2A product also contains a scene classification algorithm. This algorithm creates a scene classification, where pixels als classified in some broad classes: Here, clouds are classified into 'cloud probability masks', which are in general more precise than the level 1C cloud masks. Excercise: Visualize cloud masks Visualize the cloud masks. If you look at the cloud masks, you will see that these are not very precise. These cloud masks are useful for rough estimations. Later we will see alternative ways to identify cloud pixels more precise.","title":"Intermezzo: Cloud Masks"},{"location":"P3-image-preprocessing.html#resampling","text":"In order to display the other band combinations, some geometrical pre-processing is necessary. The bands have to be resampled to an equal resolution. The goal is to resample the image bands to 10m (you can take B2, B3, B4 or B8 as a reference band). This means that all other bands will be upsampled. Image resampling scheme. Top: upsampling (nearest neighbor). Bottom: Downsampling (minimum). Exercise: resampling In the product explorer, select the outcome image of Sen2Cor. Go to Raster > Geometric operations > Resampling . Select the \u2018Save as\u2026 BEAM-DIMAP\u2019 box. Browse to your directory. Choose a logical name for the target product. Resampling Parameters: Choose a reference band that has a resolution of 10m, or choose for a pixel resolution of 10m. Use an upsampling method of your choice (Read the help for more details on the different algorithms). Run resampling . Saving the images takes a lot of time. Again, be patient!","title":"Resampling"},{"location":"P3-image-preprocessing.html#image-subsetting","text":"Processing an entire Sentinel image takes a lot of processing capacity and time (as you probably have noted already). Therefore, you will now learn how to only process a small part of the image. You can choose to reduce the spatial extent of the image, or you can choose to reduce the amount of bands in the image, or a combination of both. An important aspect is that creating a subset is only possible for bands that have the same size. Thus, this will only be possible after resampling . Excercise: subsetting an image Select the resampled image in the product explorer. Go to Raster > Subset. Select a spatial subset by choice (by adjusting the scene start and end). Make sure your spatial extent is substantially smaller than the original image. Snap Subsetting screen. Select only following bands: [B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12] You can see an estimation of the new required storage space. Snap Subsetting screen. Click OK Another option to make a subset is \u2018Spatial subset from view\u2019. Zoom in on your image. Rightclick and select \u2018Spatial subset from view\u2019. FYI: it is also possible to take a subset of an image, based on a vector layer.","title":"Image Subsetting"},{"location":"P3-image-preprocessing.html#mosaicing","text":"Mosaicing is the merging of several arbitrarily shaped images and often used to merge two neighbouring satellite images. Excercise: mosaicing Download an image that is located next to the image you are already working with, dating from the same time as the original image was taken. You can download it directly in Level 2A, thus skipping the sen2cor atmospheric correction. Resample the image. Go to raster > Geometric operations > Mosaicing Snap mosaicing screen. Add the two source products. Choose the directory in which you want to save the mosaic image. In the Map Projection Definition you can choose the Coordinate Reference System (CRS). Choose for UTM/WGS84 (automatic) Choose for a resolution of 10m. The input products don\u2019t need to be orthorectified (because they already are). In the tab \u2018Variables and Conditions\u2019, click the - symbol. Select Band 2,3,4 and 8 Run Mosaicing. Open the RGB-image of the product. Compare it to the two original images. Extra: Examine the example of Landsat satellite image after merging below. What went wrong when mosaicing images 1 and 2? Why is there a colour difference in 2 and 3? Why is there no observable colour difference in 2 and 4? Have you any idea how to eliminate the colour difference between 2 and 3, given that neighbouring satellite images always partly overlap? Landsat images mosaic","title":"Mosaicing"},{"location":"P3-intro.html","text":"Practicum 3: Image download & preprocessing Doel van het practicum Downloaden van remote sensing data: via ESA sentinel hub via andere bronnen Introductie tot ESA SNAP: Inlezen van RS beelden Basisfunctionaliteiten Aanmaken van beeldcomposieten SNAP vs andere software Beeldvoorbewerking in SNAP (Sentinel 2): Radiometrische/atmospherische correctie Resampling Subsetting Mosaicing","title":"Introduction"},{"location":"P3-intro.html#practicum-3-image-download-preprocessing","text":"","title":"Practicum 3: Image download &amp; preprocessing"},{"location":"P3-intro.html#doel-van-het-practicum","text":"Downloaden van remote sensing data: via ESA sentinel hub via andere bronnen Introductie tot ESA SNAP: Inlezen van RS beelden Basisfunctionaliteiten Aanmaken van beeldcomposieten SNAP vs andere software Beeldvoorbewerking in SNAP (Sentinel 2): Radiometrische/atmospherische correctie Resampling Subsetting Mosaicing","title":"Doel van het practicum"},{"location":"P4/P4-Cloud_masking.html","text":"Cloud Masking Wolkbedekking is een grote barri\u00e8re tijdens het analyseren en processen van (spectrale) satellietbeelden. Recente satellietdata komen veelal ook met automatische classificaties van de wolkbedekking, waardoor deze relatief eenvoudig uit het beeld verwijderd kunnen worden (zie ook P3: cloud masking ). Earth engine bevat naast deze standaard \u2018cloud masks\u2019 ook eigen algoritmes om de wolken en wolkschaduw te verwijderen uit het beeld, maar bevat ook enkele andere mogelijkheden om de aanwezigheid ervan veel mogelijk te minimaliseren zoals het toepassen van reducties op beeldcollecties. 1 Filteren van de ImageCollection op wolkbedekking Een eerste optie is om een beeldcollectie te filteren (zie voorgaand ) op wolkbedekking, waardoor enkel de beelden binnen een paalde range van wolkenpercentages worden weerhouden: FilterMetaData Bij de voorgande filters gebruikten we de redelijk eenvoudige functies .filterBounds() en .filterDate() , twee standaardfilters om op respectievelijk locatie (van een geometrie) en datum te filter. De functie .filterMetadata() wordt gebruikt om te filteren op eender welke Metadata-eigenschap dat een beeld bevat. Gebruik de Docs om het gebruik van deze functie verder te bekijken. 2 Cloud Masking met Cloudmasks Als 2e stap kunnen de overgebleven wolken/wolkschaduwen per beeld worden \u2018geknipt\u2019 (cloudmask) door deze pixels naar een waarde 0 om te zetten. Een standaard algoritme is bij de meeste beelden reeds gegeven als voorbeeld onderaan in de catalogus: Voor Landsat-8 : https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C01_T1_SR Voor Sentinel 2 : https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR ). Voor Landsat 8 wordt dit bijgevolg: L8 = L8 . filterMetadata ( 'CLOUD_COVER' , 'less_than' , 30 ) function maskL8sr ( image ) { // Bits 3 and 5 are cloud shadow and cloud, respectively. var cloudShadowBitMask = ( 1 << 3 ); var cloudsBitMask = ( 1 << 5 ); // Get the pixel QA band. var qa = image . select ( 'pixel_qa' ); // Both flags should be set to zero, indicating clear conditions. var mask = qa . bitwiseAnd ( cloudShadowBitMask ). eq ( 0 ) . and ( qa . bitwiseAnd ( cloudsBitMask ). eq ( 0 )); return image . updateMask ( mask ); } // Pas de functie over elk beeld binnen de collectie toe: var L8_masked = L8 . map ( maskL8sr ); Resulterend is een ImageCollectie met dezelfde beelden, maar waaruit de wolken gemaskeerd zijn (mask toegepast). Echter kunnen wel sommige wolkenranden nog zichtbaar zijn, die de mask-functies hebben gemist. OPDRACHT Maak de L8_masked collectie aan, en neem hiervan een .median() reducer. Visualiseer dit beeld. Merk je een verbetering in vergelijking met de voorgande .median()-gereduceerde beelden, zonder de cloudmask ? De .map()-functie In bovenstaand voorbeeld werd de cloudmask-functie toegepast door gebruik te maken van .map() . D .map() wordt steeds gebruikt om een functie (die op afzonderlijke beelden dient toegepast te worden, zoals maskL8sr ) toe te passen over elk beeld binnen een ImageCollection afzonderlijk. Het is als het ware een veel effici\u00ebnte manier dan de aangemaakte functie te itereren via een for-loop. Betekening EXTRA Onderstaande Sentinel-2 cloudmask-procedure is ter aanvulling van bovenstaande principes. Er kan voor komende oefeningen/ het praktisch examen met van beide procedures gebruikt worden, dus zoals wat in Cloud Masking met Cloudmasks werd gebruikt.","title":"Cloud masking"},{"location":"P4/P4-Cloud_masking.html#cloud-masking","text":"Wolkbedekking is een grote barri\u00e8re tijdens het analyseren en processen van (spectrale) satellietbeelden. Recente satellietdata komen veelal ook met automatische classificaties van de wolkbedekking, waardoor deze relatief eenvoudig uit het beeld verwijderd kunnen worden (zie ook P3: cloud masking ). Earth engine bevat naast deze standaard \u2018cloud masks\u2019 ook eigen algoritmes om de wolken en wolkschaduw te verwijderen uit het beeld, maar bevat ook enkele andere mogelijkheden om de aanwezigheid ervan veel mogelijk te minimaliseren zoals het toepassen van reducties op beeldcollecties.","title":"Cloud Masking"},{"location":"P4/P4-Cloud_masking.html#1-filteren-van-de-imagecollection-op-wolkbedekking","text":"Een eerste optie is om een beeldcollectie te filteren (zie voorgaand ) op wolkbedekking, waardoor enkel de beelden binnen een paalde range van wolkenpercentages worden weerhouden: FilterMetaData Bij de voorgande filters gebruikten we de redelijk eenvoudige functies .filterBounds() en .filterDate() , twee standaardfilters om op respectievelijk locatie (van een geometrie) en datum te filter. De functie .filterMetadata() wordt gebruikt om te filteren op eender welke Metadata-eigenschap dat een beeld bevat. Gebruik de Docs om het gebruik van deze functie verder te bekijken.","title":"1 Filteren van de ImageCollection op wolkbedekking"},{"location":"P4/P4-Cloud_masking.html#2-cloud-masking-met-cloudmasks","text":"Als 2e stap kunnen de overgebleven wolken/wolkschaduwen per beeld worden \u2018geknipt\u2019 (cloudmask) door deze pixels naar een waarde 0 om te zetten. Een standaard algoritme is bij de meeste beelden reeds gegeven als voorbeeld onderaan in de catalogus: Voor Landsat-8 : https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C01_T1_SR Voor Sentinel 2 : https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR ). Voor Landsat 8 wordt dit bijgevolg: L8 = L8 . filterMetadata ( 'CLOUD_COVER' , 'less_than' , 30 ) function maskL8sr ( image ) { // Bits 3 and 5 are cloud shadow and cloud, respectively. var cloudShadowBitMask = ( 1 << 3 ); var cloudsBitMask = ( 1 << 5 ); // Get the pixel QA band. var qa = image . select ( 'pixel_qa' ); // Both flags should be set to zero, indicating clear conditions. var mask = qa . bitwiseAnd ( cloudShadowBitMask ). eq ( 0 ) . and ( qa . bitwiseAnd ( cloudsBitMask ). eq ( 0 )); return image . updateMask ( mask ); } // Pas de functie over elk beeld binnen de collectie toe: var L8_masked = L8 . map ( maskL8sr ); Resulterend is een ImageCollectie met dezelfde beelden, maar waaruit de wolken gemaskeerd zijn (mask toegepast). Echter kunnen wel sommige wolkenranden nog zichtbaar zijn, die de mask-functies hebben gemist.","title":"2 Cloud Masking met Cloudmasks"},{"location":"P4/P4-Cloud_masking.html#opdracht","text":"Maak de L8_masked collectie aan, en neem hiervan een .median() reducer. Visualiseer dit beeld. Merk je een verbetering in vergelijking met de voorgande .median()-gereduceerde beelden, zonder de cloudmask ? De .map()-functie In bovenstaand voorbeeld werd de cloudmask-functie toegepast door gebruik te maken van .map() . D .map() wordt steeds gebruikt om een functie (die op afzonderlijke beelden dient toegepast te worden, zoals maskL8sr ) toe te passen over elk beeld binnen een ImageCollection afzonderlijk. Het is als het ware een veel effici\u00ebnte manier dan de aangemaakte functie te itereren via een for-loop. Betekening EXTRA Onderstaande Sentinel-2 cloudmask-procedure is ter aanvulling van bovenstaande principes. Er kan voor komende oefeningen/ het praktisch examen met van beide procedures gebruikt worden, dus zoals wat in Cloud Masking met Cloudmasks werd gebruikt.","title":"OPDRACHT"},{"location":"P4/P4-DataCatalog.html","text":"De Earth Engine Data Catalog Om het aanbod aan Aardobservatie-data in GEE te bekijken en te doorzoeken, kan gebruik gemaakt worden van de Data Catalog: https://developers.google.com/earth-engine/datasets . Via deze catalogus kun je eenvoudig rasterdata allerhande opzoeken en de noodzakelijke code om de beelden in je script te importeren vinden. In de komende voorbeeldoefening maken we gebruik van Landsat-data. Zoek in de Earth Engine Data Catalog naar een geschikte Landsat datacollectie, dat het jaar 2020 omvat. In Earth Engine zijn Landsatbeelden eerste instantie opgedeeld op basis van de uitgevoerde correcties ( 'Surface reflectance' , 'Top-Of-Atmosphere' , 'Raw Images' ), anderzijds op basis van de kwaliteit: Tier 1 : De meest kwalitatieve beelden, geschikt voor tijdserie-analyse. De beelden zijn zowel geometrisch als radiometrisch kwalitatief goed bevonden. Tier 2 : De beelden zijn geometrisch en/of radiometrisch minder kwalitatief bevonden, maar kunnen voor bepaalde doeleinden wel nog inzetbaar zijn. Tier 1 + Real-Time : De Tier-1 database uitgebreid met de meest recente data die nog niet kwalitatief zijn gekeurd en bijgevolg dus nog fouten kunnen bevatten. Het Landsat programma Landsat is het langst lopende aardobservatie satellietprogramma en is sinds 1972 continue operationeel. Het is een samenwerking tussen de United States Geological Survey (USGS) en de NASA. De meest recente lancering was deze van Landsat 8 in 2013, de lancering van Landsat 9 gepland staat voor 2022. Onderstaande grafiek geeft een vergelijking tussen de bandverdeling van de huidige operationele Landsatsatellieten: Landsat 8 (OLI/TIRS) en Landsat 7 (Bron: NASA )","title":"EarthEngine data catalog"},{"location":"P4/P4-DataCatalog.html#de-earth-engine-data-catalog","text":"Om het aanbod aan Aardobservatie-data in GEE te bekijken en te doorzoeken, kan gebruik gemaakt worden van de Data Catalog: https://developers.google.com/earth-engine/datasets . Via deze catalogus kun je eenvoudig rasterdata allerhande opzoeken en de noodzakelijke code om de beelden in je script te importeren vinden. In de komende voorbeeldoefening maken we gebruik van Landsat-data. Zoek in de Earth Engine Data Catalog naar een geschikte Landsat datacollectie, dat het jaar 2020 omvat. In Earth Engine zijn Landsatbeelden eerste instantie opgedeeld op basis van de uitgevoerde correcties ( 'Surface reflectance' , 'Top-Of-Atmosphere' , 'Raw Images' ), anderzijds op basis van de kwaliteit: Tier 1 : De meest kwalitatieve beelden, geschikt voor tijdserie-analyse. De beelden zijn zowel geometrisch als radiometrisch kwalitatief goed bevonden. Tier 2 : De beelden zijn geometrisch en/of radiometrisch minder kwalitatief bevonden, maar kunnen voor bepaalde doeleinden wel nog inzetbaar zijn. Tier 1 + Real-Time : De Tier-1 database uitgebreid met de meest recente data die nog niet kwalitatief zijn gekeurd en bijgevolg dus nog fouten kunnen bevatten. Het Landsat programma Landsat is het langst lopende aardobservatie satellietprogramma en is sinds 1972 continue operationeel. Het is een samenwerking tussen de United States Geological Survey (USGS) en de NASA. De meest recente lancering was deze van Landsat 8 in 2013, de lancering van Landsat 9 gepland staat voor 2022. Onderstaande grafiek geeft een vergelijking tussen de bandverdeling van de huidige operationele Landsatsatellieten: Landsat 8 (OLI/TIRS) en Landsat 7 (Bron: NASA )","title":"De Earth Engine Data Catalog"},{"location":"P4/P4-Goals.html","text":"Doel van dit practicum In dit practicum maken we kennis met het Google Earth Engine. We behandelen hoe data kan opgezocht en gevisualiseerd worden en hoe data over een bepaalde tijdsperiode te aggregeren. We maken hierbij gebruik van zowel Landsat-8 beelden als Sentinel-2. Voorbereiding Voor dit practicum heb je enkel een laptop nodig waarop - bij voorkeur - Google Chrome op is ge\u00efnstalleerd. Verder heb je ook een Google Earth Engine account nodig. Deze kun je gratis aanmaken via https://earthengine.google.com/new_signup/ . Indien je dit nog niet hebt gedaan, gelieve dit dan te doen.","title":"Doel van het practicum"},{"location":"P4/P4-Goals.html#doel-van-dit-practicum","text":"In dit practicum maken we kennis met het Google Earth Engine. We behandelen hoe data kan opgezocht en gevisualiseerd worden en hoe data over een bepaalde tijdsperiode te aggregeren. We maken hierbij gebruik van zowel Landsat-8 beelden als Sentinel-2.","title":"Doel van dit practicum"},{"location":"P4/P4-Goals.html#voorbereiding","text":"Voor dit practicum heb je enkel een laptop nodig waarop - bij voorkeur - Google Chrome op is ge\u00efnstalleerd. Verder heb je ook een Google Earth Engine account nodig. Deze kun je gratis aanmaken via https://earthengine.google.com/new_signup/ . Indien je dit nog niet hebt gedaan, gelieve dit dan te doen.","title":"Voorbereiding"},{"location":"P4/P4-ImageVisualization.html","text":"Visualisatie van een enkelvoudig satellietbeeld Laten we simpel starten met het afbeelden van een enkel rasterbeeld. In Practicum 3 gingen we te werk met een Sentinel-2 beeld van de Braziliaanse stad B\u00e9lem uit 2020. Aangezien de volledige Sentinel-bibliotheek beschikbaar is binnen Earth Engine, kan dit beeld eenvoudig worden ingeladen. Bekijk hiervoor eerst de naam nog eens van je gedownload S2-bestand, bijvoorbeeld: S2B_MSIL1C_ 20200808T134219_N0209_R124_T22MGD _20200808T153444.SAFE In Earth Engine is het vette gedeelte van de filenaam belangrijk. Dit wordt als volgt in earth-engine ingeladen, via 'ee.Image' : //Voorbeeld: Sentinel-2 beeld van vorig practicum var S2_Belem = ee . Image ( 'COPERNICUS/S2_SR/20200808T134219_20200808T134214_T22MGD' ) print ( S2_Belem ) // Zoom in de Map-view in naar het beeld, met Zoom-factor 9 Map . centerObject ( S2_Belem , 9 ); Hiermee werd slechts een variabele aangemaakt die het beeld omvat. Om het beeld te visualiseren wordt gebruik gemaakt van de functie Map.addLayer() : //Visualiseren van het satellietbeeld Map . addLayer ( S2_Belem ); Bij het uitvoeren van bovenstaande code bekomen we een zwart vlak, niet bepaald de visualisatie die we wensen. Bij het uitvoeren van bovenstaande code bekomen we een zwart vlak, niet bepaald de visualisatie die we wensen. Dit komt omdat we nog geen visualisatieparameters hebben aangegeven, waardoor de eerste 3 banden naar de rode, groene en blauwe band respectievelijk worden toegekend en de pixelrange zo groot is dat alle pixels een zwarte kleur krijgen. Om dit manueel aan te passen, zoek je je toegevoegde laag in 'Layers' in de Map-view. Klik op het tandwieltje. Een visualisatie-scherm springt open. Pas de parameters aan, zodat je een normale kleurencomposiet verkrijgt, met een stretch van 3 gamma en druk op 'Apply'. Een visueel beter resultaat wordt verkregen. Handmatig instellen van de visualisatieparameters kan via 'Layers' in de Map view Het is echter niet handig om steeds opnieuw de visualisatie handmatig in te stellen. Gelukkig kan deze ook als code ge\u00efmporteerd worden in GEE (klik op 'Import'). De visualisatieparameters worden toegevoegd in de Imports. Deze kunnen dan in de Map.addLayer() -functie worden meegeven tijdens het visualiseren. In de code-editor zelf kunnen de visualisatieparameters eveneens gedefinieerd worden. // Aanmaken van visualizatieparameters var visualization = { min : 0 , max : 3000 , bands : [ 'B4' , 'B3' , 'B2' ], }; Map . centerObject ( S2_Belem , 9 ); Map . addLayer ( S2_Belem , visualization , 'B\u00e8lem_met_Vis' ); Beeldcollecties zoeken en filteren In voorgaande paragraaf visualiseerden we een Sentinel-2 beeld die we reeds hadden opgezocht waarvan wisten dat de kwaliteit goed zat \u00e9n waarvan we de bestandsnaam reeds kenden. Het is natuurlijk niet handig om steeds een filenaam te moeten kennen om verder te kunnen werken in Earth Engine. Daarmee zouden we ook de geweldige kracht van het programma om doorheen vele petabytes aan Aardobservatiedata te zoeken onbenut laten. In wat volgt gaan we op basis van een locatie op zoek gaan naar geschikte satellietbeelden, door het filteren van gehele beeldcollecties. Area of Interest (AOI) Starten doen we met het intekenen van een gewenste Area Of Interest (AOI) in de Map View. Een AOI is niets anders dan de afbakening van het studiegebied, waarbinnen we onze data wensen te verkrijgen. Er kan rechtstreeks gezoomd worden naar een locatie via de zoekbalk bovenaan of door het scrollen met de muis. Teken vervolgs een gewenste gebied in door gebruik te maken van de toolknoppen in de \"Map View\": . In dit voorbeeld kiezen we voor de Konigin der badsteden, Oostende, als studiegebied: Automatisch wordt een nieuwe variabele aangemaakt onder de naam 'geometry', welke eenvoudig hernoemd kan worden naar een eenvoudig te gebruiken variabelenaam: Bekijk de eigenschappen van de polygoon door het naar de console te printen: //Polygoon-informatie naar de console schrijven: print ( Oostende ) Datacollecties Filteren en Visualiseren Voor deze oefening maken we als afwisseling gebruik van Landsat-8 beelden (zie ook het stukje omtrent de Earth Engine data catalog ). De importeer-code kan gekopieerd worden uit de data catalog en ziet er als volgt uit: var L8 = ee . ImageCollection ( 'LANDSAT/LC08/C01/T1_SR' ) print ( 'Grootte van de L8-collectie :' , L8 . size ()) Hiermee verwijst de variabele 'L8' naar de volledige Landsat-8 collectie (surface reflectance). De '.size()'-functie geeft het aantal beelden dat in deze collectie zijn begrepen. Een hele hoop, sinds ze collectie veel beelden van de volledige aarde omvat. Deze verzameling dient bijgevolg gefilterd te worden. Filteren kan op basis van de metadata: //Filteren o.b.v. datum, locatie: var L8 = L8 . filterDate ( '2020-01-01' , '2020-10-30' ) //Op basis van datum . filterBounds ( Oostende ) //op basis van locatie (de AOI); //Printen van de nieuwe grootte print ( 'L8 size na filtering' , L8 . size ()) // Printen van de collectie voor inspectie print ( 'Filtered collection: ' , L8 ) De beelden in de collectie zijn standaard gesorteerd op datum, indien we dus het bovenste beeld eruit halen, zal dit het eerste Landsat-8 beeld zijn gemaakt in 2020. Met de functie .first() , halen we deze eruit. Print deze naar de console en bekijk het verschil met de de Imagecollectie. // Krijg het eerste (standaard oudste) beeld uit de collectie: var L8_first = L8 . first () print ( 'Eerste Beeld:' , L8_first ) In een volgende stap kunnen we dit beeld ook gaan visualiseren, met javascript Map.addLayer() . 6) Ook nu kunnen we dit als een echte kleurencomposiet visualiseren (voor Landsat 8 betekent dit dus B2 (blauw), B3 (groen) en B4 (rood)). // Landsat-8 visualisatie instellen. var trueColor = { bands : [ 'B4' , 'B3' , 'B2' ], min : 0 , max : 3000 , gamma : 1.4 , }; Map . addLayer ( L8_first , trueColor , 'L8_TrueColorComposite' ) Eerste Landsat 8 beeld binnen de gefilterede collectie Mogelijk is dit eerste beeld niet het meest ideale wat betreft de wolkbedekking, waardoor er weinig te zien valt. Laten we nu op zoek gaan naar het beeld met de laagste wolkenbedekking binnen de collectie. Dit doen we in eerste instantie door de collectie te sorteren volgens het percentage cloudcover, wat standaard tot de metadata van een Landsatbeeld behoort. Bekijk het beeld. Wat valt je op? Wordt het volledige gebied bedekt? //Sorteren van de collectie obv cloud cover var L8_sortedCC = L8 . sort ( 'CLOUD_COVER' , true ); Map . addLayer ( L8_sortedCC . first (), trueColor , 'Least Cloud cover 2020' ) Landsat 8-beeld met laagste wolkbedekking binnen de gefilterede collectie Bekijk op welke dag de sensor dit beeld heeft genomen. Gebruik hiervoor de \u2018inspector\u2019 om de beeldeigenschappen verder te bekijken. De inspector Opdracht Visualiseer in een nieuw script een valse kleurencomposiet van een Sentinel-2 beeld (Tier 1, Surface Reflectance). Neem hierbij Gent als AOI, met een beeld uit de zomer van 2019 met een zo laag mogelijke wolkenbedekking. Voor het sorteren van de wolkenbedekking, zoek je de gepaste eigenschap om op te sorteren. Deze kun je hier vinden. Bewaar je script.","title":"Satellietdata oproepen, filteren en visualiseren"},{"location":"P4/P4-ImageVisualization.html#visualisatie-van-een-enkelvoudig-satellietbeeld","text":"Laten we simpel starten met het afbeelden van een enkel rasterbeeld. In Practicum 3 gingen we te werk met een Sentinel-2 beeld van de Braziliaanse stad B\u00e9lem uit 2020. Aangezien de volledige Sentinel-bibliotheek beschikbaar is binnen Earth Engine, kan dit beeld eenvoudig worden ingeladen. Bekijk hiervoor eerst de naam nog eens van je gedownload S2-bestand, bijvoorbeeld: S2B_MSIL1C_ 20200808T134219_N0209_R124_T22MGD _20200808T153444.SAFE In Earth Engine is het vette gedeelte van de filenaam belangrijk. Dit wordt als volgt in earth-engine ingeladen, via 'ee.Image' : //Voorbeeld: Sentinel-2 beeld van vorig practicum var S2_Belem = ee . Image ( 'COPERNICUS/S2_SR/20200808T134219_20200808T134214_T22MGD' ) print ( S2_Belem ) // Zoom in de Map-view in naar het beeld, met Zoom-factor 9 Map . centerObject ( S2_Belem , 9 ); Hiermee werd slechts een variabele aangemaakt die het beeld omvat. Om het beeld te visualiseren wordt gebruik gemaakt van de functie Map.addLayer() : //Visualiseren van het satellietbeeld Map . addLayer ( S2_Belem ); Bij het uitvoeren van bovenstaande code bekomen we een zwart vlak, niet bepaald de visualisatie die we wensen. Bij het uitvoeren van bovenstaande code bekomen we een zwart vlak, niet bepaald de visualisatie die we wensen. Dit komt omdat we nog geen visualisatieparameters hebben aangegeven, waardoor de eerste 3 banden naar de rode, groene en blauwe band respectievelijk worden toegekend en de pixelrange zo groot is dat alle pixels een zwarte kleur krijgen. Om dit manueel aan te passen, zoek je je toegevoegde laag in 'Layers' in de Map-view. Klik op het tandwieltje. Een visualisatie-scherm springt open. Pas de parameters aan, zodat je een normale kleurencomposiet verkrijgt, met een stretch van 3 gamma en druk op 'Apply'. Een visueel beter resultaat wordt verkregen. Handmatig instellen van de visualisatieparameters kan via 'Layers' in de Map view Het is echter niet handig om steeds opnieuw de visualisatie handmatig in te stellen. Gelukkig kan deze ook als code ge\u00efmporteerd worden in GEE (klik op 'Import'). De visualisatieparameters worden toegevoegd in de Imports. Deze kunnen dan in de Map.addLayer() -functie worden meegeven tijdens het visualiseren. In de code-editor zelf kunnen de visualisatieparameters eveneens gedefinieerd worden. // Aanmaken van visualizatieparameters var visualization = { min : 0 , max : 3000 , bands : [ 'B4' , 'B3' , 'B2' ], }; Map . centerObject ( S2_Belem , 9 ); Map . addLayer ( S2_Belem , visualization , 'B\u00e8lem_met_Vis' );","title":"Visualisatie van een enkelvoudig satellietbeeld"},{"location":"P4/P4-ImageVisualization.html#beeldcollecties-zoeken-en-filteren","text":"In voorgaande paragraaf visualiseerden we een Sentinel-2 beeld die we reeds hadden opgezocht waarvan wisten dat de kwaliteit goed zat \u00e9n waarvan we de bestandsnaam reeds kenden. Het is natuurlijk niet handig om steeds een filenaam te moeten kennen om verder te kunnen werken in Earth Engine. Daarmee zouden we ook de geweldige kracht van het programma om doorheen vele petabytes aan Aardobservatiedata te zoeken onbenut laten. In wat volgt gaan we op basis van een locatie op zoek gaan naar geschikte satellietbeelden, door het filteren van gehele beeldcollecties.","title":"Beeldcollecties zoeken en filteren"},{"location":"P4/P4-ImageVisualization.html#area-of-interest-aoi","text":"Starten doen we met het intekenen van een gewenste Area Of Interest (AOI) in de Map View. Een AOI is niets anders dan de afbakening van het studiegebied, waarbinnen we onze data wensen te verkrijgen. Er kan rechtstreeks gezoomd worden naar een locatie via de zoekbalk bovenaan of door het scrollen met de muis. Teken vervolgs een gewenste gebied in door gebruik te maken van de toolknoppen in de \"Map View\": . In dit voorbeeld kiezen we voor de Konigin der badsteden, Oostende, als studiegebied: Automatisch wordt een nieuwe variabele aangemaakt onder de naam 'geometry', welke eenvoudig hernoemd kan worden naar een eenvoudig te gebruiken variabelenaam: Bekijk de eigenschappen van de polygoon door het naar de console te printen: //Polygoon-informatie naar de console schrijven: print ( Oostende )","title":"Area of Interest (AOI)"},{"location":"P4/P4-ImageVisualization.html#datacollecties-filteren-en-visualiseren","text":"Voor deze oefening maken we als afwisseling gebruik van Landsat-8 beelden (zie ook het stukje omtrent de Earth Engine data catalog ). De importeer-code kan gekopieerd worden uit de data catalog en ziet er als volgt uit: var L8 = ee . ImageCollection ( 'LANDSAT/LC08/C01/T1_SR' ) print ( 'Grootte van de L8-collectie :' , L8 . size ()) Hiermee verwijst de variabele 'L8' naar de volledige Landsat-8 collectie (surface reflectance). De '.size()'-functie geeft het aantal beelden dat in deze collectie zijn begrepen. Een hele hoop, sinds ze collectie veel beelden van de volledige aarde omvat. Deze verzameling dient bijgevolg gefilterd te worden. Filteren kan op basis van de metadata: //Filteren o.b.v. datum, locatie: var L8 = L8 . filterDate ( '2020-01-01' , '2020-10-30' ) //Op basis van datum . filterBounds ( Oostende ) //op basis van locatie (de AOI); //Printen van de nieuwe grootte print ( 'L8 size na filtering' , L8 . size ()) // Printen van de collectie voor inspectie print ( 'Filtered collection: ' , L8 ) De beelden in de collectie zijn standaard gesorteerd op datum, indien we dus het bovenste beeld eruit halen, zal dit het eerste Landsat-8 beeld zijn gemaakt in 2020. Met de functie .first() , halen we deze eruit. Print deze naar de console en bekijk het verschil met de de Imagecollectie. // Krijg het eerste (standaard oudste) beeld uit de collectie: var L8_first = L8 . first () print ( 'Eerste Beeld:' , L8_first ) In een volgende stap kunnen we dit beeld ook gaan visualiseren, met javascript Map.addLayer() . 6) Ook nu kunnen we dit als een echte kleurencomposiet visualiseren (voor Landsat 8 betekent dit dus B2 (blauw), B3 (groen) en B4 (rood)). // Landsat-8 visualisatie instellen. var trueColor = { bands : [ 'B4' , 'B3' , 'B2' ], min : 0 , max : 3000 , gamma : 1.4 , }; Map . addLayer ( L8_first , trueColor , 'L8_TrueColorComposite' ) Eerste Landsat 8 beeld binnen de gefilterede collectie Mogelijk is dit eerste beeld niet het meest ideale wat betreft de wolkbedekking, waardoor er weinig te zien valt. Laten we nu op zoek gaan naar het beeld met de laagste wolkenbedekking binnen de collectie. Dit doen we in eerste instantie door de collectie te sorteren volgens het percentage cloudcover, wat standaard tot de metadata van een Landsatbeeld behoort. Bekijk het beeld. Wat valt je op? Wordt het volledige gebied bedekt? //Sorteren van de collectie obv cloud cover var L8_sortedCC = L8 . sort ( 'CLOUD_COVER' , true ); Map . addLayer ( L8_sortedCC . first (), trueColor , 'Least Cloud cover 2020' ) Landsat 8-beeld met laagste wolkbedekking binnen de gefilterede collectie Bekijk op welke dag de sensor dit beeld heeft genomen. Gebruik hiervoor de \u2018inspector\u2019 om de beeldeigenschappen verder te bekijken. De inspector Opdracht Visualiseer in een nieuw script een valse kleurencomposiet van een Sentinel-2 beeld (Tier 1, Surface Reflectance). Neem hierbij Gent als AOI, met een beeld uit de zomer van 2019 met een zo laag mogelijke wolkenbedekking. Voor het sorteren van de wolkenbedekking, zoek je de gepaste eigenschap om op te sorteren. Deze kun je hier vinden. Bewaar je script.","title":"Datacollecties Filteren en Visualiseren"},{"location":"P4/P4-Intro.html","text":"De Google Earth Engine Interface The Google Earth Engine code editor interface. De interface van de Earth Engine code editor is op zich vrij simpel. Er kunnen 5 grote blokken onderscheden worden: Het linkerpaneel, met 3 tabs: Scripts : je eigen bibliotheek met scripts, onder te verdelen in repositories, folders en scripts. Ook de scripts waar je schrijf- en leesrechten hebt kun je hierin terugvinden. Docs : Bevat informatie over de functies die beschikbaar zijn in Earth Engine. Hier kun je snel de functionaliteiten en beschrijving van de input- en outputparameters terugvinden. Assets : oplijsting met de 'assets' die je opgeladen/aangemaakt hebt in Earth Engine. Assets kunnen rasters of vectoren (met bijvoorbeeld trainingsdata of studiegebied). De code editor zelf in het middenpaneel, waar je scripts kunt aanmaken/bewerken, delen en opslaan. Het rechterpaneel, met 3 tabs: De Console : waar eventuele output of foutmeldingen naar geschreven worden. de 'print()'-functie wordt steeds gebruikt om hier informatie te bekijken. De Inspector : hiermee kun je op specifieke pixels in de 'map view' klikken, waarna de overeenkomstige pixelinformatie wordt gevisualiseerd. De Tasks : bevat een oplijsting van de 'exports' die in het script werden aangemaakt (als je bijvoorbeeld een satellietbeeld naar je Google Drive wenst te sturen). Een export moet hier steeds nog manueel gestart worden. De Map View : waar het beeldmateriaal wordt gevisualiseerd. De zoekfunctie waarmee beeldmateriaal beschikbaar binnen de Google Cloud kan worden opgezocht. Earth Engine code: Javascript 101 Google Earth Engine maakt voor zijn code-editor gebruik van Javascript als programmeertaal, maar om vertrouwd te geraken met GEE hoef je geen Javascript-expert te worden. GEE gebruikt namelijk hoofdzakelijk eigen 'classes' en functionaliteiten, waardoor je slechts een basiskennis javascript nodig hebt. Daarom starten we eerst met een spoedcursus Javascript, waarop we onze verdere 'Earth Engine'-magie kunnen bouwen. \"Hello World\" De 'print'-functie Zoals gebruikelijk is bij het leren van een programmeertaal, groeten we de wereld met ons eerste lijntje code. Open https://code.earthengine.google.com/ , en voeg volgend lijntje toe aan het nieuwe script. //Printen van Hello World print ( 'Hello World' ) Klik daarna op 'Run'. Proficiat! Het eerste scriptje is geschreven. Hiermee heb je onmiddellijk ook een eerste uitermate handige functie gezien. De \u2018print\u2019-functie kun je gebruiken om bepaalde informatie naar de Console te schrijven, zoals metadata, ... . Verder valt hieruit ook op te merken dat een dubbele voorwaartse slash '// ' gebruikt wordt om notities te nemen binnen de code. Strings Proficiat! Het eerste scriptje is geschreven. Laat ons deze string nu onderbrengen in een variabele. In Javascript dient een variabele altijd ge\u00efniteerd te worden met var statement. Indien je dit zou weglaten, zal je op een 'error' stoten. //Aanmaken van de variabele 'aString' var aString = 'Hello World' print ( aString ) Om het datatype van de variabele aString na te gaan, kun je dit oproepen met de functie \u2018typeof()\u2019-statement: // Type van de variabele aString naar de Console schrijven print ( typeof ( aString )) Functies In een volgende stap maken we een functie aan, waarbij je een string naar keuze kunt groeten. Een functie in Google Earth Engine ziet er uit volgens volgende opbouw: var functienaam = function ( inputvariabelen ) { //Hier de functie-bewerkingen //output = a + b Return output }; Bijvoorbeeld: //Hello Function: var hello_function = function ( String ) { var goeindag = 'Hello ' + String return goeindag }; //Functie uitvoeren: var hallo = hello_function ( 'Boerekot' ); print ( hallo ) //Variabelen aangemaakt binnen de functie worden enkel daar gebruikt: print ( goeindag ) Lijsten Een lijst in Javascript wordt steeds opgegeven met [ en ]. Een lijstindex begint steeds vanaf '0', waarbij de eerste waarde dus op positie 0 staat. var lievelingsnummers = [ 8 , 6 , 3 , 27 ] print ( 'Eerste lievelingsnummer in de lijst = ' , lievelingsnummers [ 0 ]) //Lijstelementen aanpassen var automerken = [ \"BMW\" , \"Volkswagen\" , \"Minerva\" ] automerken [ 2 ] = [ \"Opel\" ] print ( automerken ) Objecten Een Object wordt aangegeven met '{' en '}'. Aan een object hangen steeds enkele variabelen die tot het object behoren. //object var beelden = { Sensor : \"Sentinel 2\" , Regios : [ \"Belgium\" , \"France\" , \"Vaticano\" ], Aantalbeelden : 2 , 1 : \"Ja\" } Om een eigenschap van een object op te roepen, wordt stees een puntje '.' gebruikt: object.eigenschap. Indien we bijvoorbeeld de sensor van ons aangemaakte beeldmateriaal willen nagaan: // Sensor bekijken print ( beelden . Sensor ) // Andere methode via haakjes [] print ( 'Regios: ' , beelden [ 'Regios' ]) Specifieke Earth Engine objecten ee.Thing Om objecten/elementen/processen richting de google server te sturen, wordt gebruik gemaakt van zogenaamde 'containers'. Om dit aan te duiden wordt gebruik gemaakt van een ee.Thing structuur. Dit zal doorheen de komende practica wat duidelijker worden. ee.Images Een Image is rasterdata bestaande uit \u00e9\u00e9n of meerdere banden, waarvan elke band een eigen naam, datatype, resolutie en projectie heeft. Een enkel Sentinel-2 beeld zoals in Practicum 3 gedownload werd, zal als \u00e9\u00e9n Image kunnen worden opgeslagen. Om een Image in te laden en Earth Engine wordt gebruik gemaakt van ee.Image . In volgend hoofdstuk wordt dit ge\u00efllustreerd. ee.ImageCollections Een ImageCollection is een collectie van meerdere Image 's, zoals bijvoorbeeld de volledige Sentinel-2 collectie. Het bevat m.a.w. heel wat beelden die in een bepaalde volgorde gesorteerd zijn. Standaard is dit o.b.v. datum, maar aangepaste sorteringen zijn eveneens mogelijk, zoals we in een komende oefening gaan zien.","title":"Introductie tot Earth Engine"},{"location":"P4/P4-Intro.html#de-google-earth-engine-interface","text":"The Google Earth Engine code editor interface. De interface van de Earth Engine code editor is op zich vrij simpel. Er kunnen 5 grote blokken onderscheden worden: Het linkerpaneel, met 3 tabs: Scripts : je eigen bibliotheek met scripts, onder te verdelen in repositories, folders en scripts. Ook de scripts waar je schrijf- en leesrechten hebt kun je hierin terugvinden. Docs : Bevat informatie over de functies die beschikbaar zijn in Earth Engine. Hier kun je snel de functionaliteiten en beschrijving van de input- en outputparameters terugvinden. Assets : oplijsting met de 'assets' die je opgeladen/aangemaakt hebt in Earth Engine. Assets kunnen rasters of vectoren (met bijvoorbeeld trainingsdata of studiegebied). De code editor zelf in het middenpaneel, waar je scripts kunt aanmaken/bewerken, delen en opslaan. Het rechterpaneel, met 3 tabs: De Console : waar eventuele output of foutmeldingen naar geschreven worden. de 'print()'-functie wordt steeds gebruikt om hier informatie te bekijken. De Inspector : hiermee kun je op specifieke pixels in de 'map view' klikken, waarna de overeenkomstige pixelinformatie wordt gevisualiseerd. De Tasks : bevat een oplijsting van de 'exports' die in het script werden aangemaakt (als je bijvoorbeeld een satellietbeeld naar je Google Drive wenst te sturen). Een export moet hier steeds nog manueel gestart worden. De Map View : waar het beeldmateriaal wordt gevisualiseerd. De zoekfunctie waarmee beeldmateriaal beschikbaar binnen de Google Cloud kan worden opgezocht.","title":"De Google Earth Engine Interface"},{"location":"P4/P4-Intro.html#earth-engine-code-javascript-101","text":"Google Earth Engine maakt voor zijn code-editor gebruik van Javascript als programmeertaal, maar om vertrouwd te geraken met GEE hoef je geen Javascript-expert te worden. GEE gebruikt namelijk hoofdzakelijk eigen 'classes' en functionaliteiten, waardoor je slechts een basiskennis javascript nodig hebt. Daarom starten we eerst met een spoedcursus Javascript, waarop we onze verdere 'Earth Engine'-magie kunnen bouwen.","title":"Earth Engine code: Javascript 101"},{"location":"P4/P4-Intro.html#hello-world","text":"","title":"\"Hello World\""},{"location":"P4/P4-Intro.html#de-print-functie","text":"Zoals gebruikelijk is bij het leren van een programmeertaal, groeten we de wereld met ons eerste lijntje code. Open https://code.earthengine.google.com/ , en voeg volgend lijntje toe aan het nieuwe script. //Printen van Hello World print ( 'Hello World' ) Klik daarna op 'Run'. Proficiat! Het eerste scriptje is geschreven. Hiermee heb je onmiddellijk ook een eerste uitermate handige functie gezien. De \u2018print\u2019-functie kun je gebruiken om bepaalde informatie naar de Console te schrijven, zoals metadata, ... . Verder valt hieruit ook op te merken dat een dubbele voorwaartse slash '// ' gebruikt wordt om notities te nemen binnen de code.","title":"De 'print'-functie"},{"location":"P4/P4-Intro.html#strings","text":"Proficiat! Het eerste scriptje is geschreven. Laat ons deze string nu onderbrengen in een variabele. In Javascript dient een variabele altijd ge\u00efniteerd te worden met var statement. Indien je dit zou weglaten, zal je op een 'error' stoten. //Aanmaken van de variabele 'aString' var aString = 'Hello World' print ( aString ) Om het datatype van de variabele aString na te gaan, kun je dit oproepen met de functie \u2018typeof()\u2019-statement: // Type van de variabele aString naar de Console schrijven print ( typeof ( aString ))","title":"Strings"},{"location":"P4/P4-Intro.html#functies","text":"In een volgende stap maken we een functie aan, waarbij je een string naar keuze kunt groeten. Een functie in Google Earth Engine ziet er uit volgens volgende opbouw: var functienaam = function ( inputvariabelen ) { //Hier de functie-bewerkingen //output = a + b Return output }; Bijvoorbeeld: //Hello Function: var hello_function = function ( String ) { var goeindag = 'Hello ' + String return goeindag }; //Functie uitvoeren: var hallo = hello_function ( 'Boerekot' ); print ( hallo ) //Variabelen aangemaakt binnen de functie worden enkel daar gebruikt: print ( goeindag )","title":"Functies"},{"location":"P4/P4-Intro.html#lijsten","text":"Een lijst in Javascript wordt steeds opgegeven met [ en ]. Een lijstindex begint steeds vanaf '0', waarbij de eerste waarde dus op positie 0 staat. var lievelingsnummers = [ 8 , 6 , 3 , 27 ] print ( 'Eerste lievelingsnummer in de lijst = ' , lievelingsnummers [ 0 ]) //Lijstelementen aanpassen var automerken = [ \"BMW\" , \"Volkswagen\" , \"Minerva\" ] automerken [ 2 ] = [ \"Opel\" ] print ( automerken )","title":"Lijsten"},{"location":"P4/P4-Intro.html#objecten","text":"Een Object wordt aangegeven met '{' en '}'. Aan een object hangen steeds enkele variabelen die tot het object behoren. //object var beelden = { Sensor : \"Sentinel 2\" , Regios : [ \"Belgium\" , \"France\" , \"Vaticano\" ], Aantalbeelden : 2 , 1 : \"Ja\" } Om een eigenschap van een object op te roepen, wordt stees een puntje '.' gebruikt: object.eigenschap. Indien we bijvoorbeeld de sensor van ons aangemaakte beeldmateriaal willen nagaan: // Sensor bekijken print ( beelden . Sensor ) // Andere methode via haakjes [] print ( 'Regios: ' , beelden [ 'Regios' ])","title":"Objecten"},{"location":"P4/P4-Intro.html#specifieke-earth-engine-objecten","text":"","title":"Specifieke Earth Engine objecten"},{"location":"P4/P4-Intro.html#eething","text":"Om objecten/elementen/processen richting de google server te sturen, wordt gebruik gemaakt van zogenaamde 'containers'. Om dit aan te duiden wordt gebruik gemaakt van een ee.Thing structuur. Dit zal doorheen de komende practica wat duidelijker worden.","title":"ee.Thing"},{"location":"P4/P4-Intro.html#eeimages","text":"Een Image is rasterdata bestaande uit \u00e9\u00e9n of meerdere banden, waarvan elke band een eigen naam, datatype, resolutie en projectie heeft. Een enkel Sentinel-2 beeld zoals in Practicum 3 gedownload werd, zal als \u00e9\u00e9n Image kunnen worden opgeslagen. Om een Image in te laden en Earth Engine wordt gebruik gemaakt van ee.Image . In volgend hoofdstuk wordt dit ge\u00efllustreerd.","title":"ee.Images"},{"location":"P4/P4-Intro.html#eeimagecollections","text":"Een ImageCollection is een collectie van meerdere Image 's, zoals bijvoorbeeld de volledige Sentinel-2 collectie. Het bevat m.a.w. heel wat beelden die in een bepaalde volgorde gesorteerd zijn. Standaard is dit o.b.v. datum, maar aangepaste sorteringen zijn eveneens mogelijk, zoals we in een komende oefening gaan zien.","title":"ee.ImageCollections"},{"location":"P4/P4-Oefeningen.html","text":"Onderstaande oefeningen kunnen gebruikt worden om P4 verder in te oefenen. Oefening 4.1 - Area 51 Niveau : gemakkelijk Stappen: Open een nieuw script Laad een Landsat-8 collectie in. Ga voor de beelden met de hoogste kwaliteit. Welke collectie kies je dan? Filter je collectie op basis van volgende gevens: Periode : september 2019 Locatie : Area 51. Hiervoor kun je volgende punt-locatie in je script gebruiken: var Area51 = ee . Geometry . Point ([ - 115.81441562461978 , 37.2386297535804 ]); Wolkbedekking : minder of gelijk aan 10% Hoeveel beelden blijven er nog over die aan bovenstaande criteria voldoen? Van de resterende beeldencollectie neem je het eerste beeld (niet gesorteerd). Bekijk de metadata. Van welke datum is dit beeld afkomstig? Visualiseer het beeld als een Normale Kleuren composiet, een Valse Kleurencomposiet. Je kunt zelf je visualisatieparameters defeni\u00ebren door ze handmatig aan in te stellen. Analyseer het volledige door jezelf volgende vragen te stellen: Waar is er vegetatie te vinden? Waar is dit natuurlijk, waar onnatuurlijk? Welke features herken je thv Area 51? Het gebied geeft ook een ideale aanleiding om de 'Geology'-composiet eens uit te testen. De combinaties is als volgt: RGB = SWIR2-SWIR1-BLUE . Vertaal dit zelf naar de Landsat-8 banden. (Maak gebruik van de Landsat 8 bandentabel ). Deze composiet maakt visuele inspectie van grote structurele eigenschappen van gesteenten (zoals plooien en breuken) gemakkelijker. Oplossing Script: https://code.earthengine.google.com/724bbee7796ebd4ff9657dc5f0c1baaa Oefening 4.2 -","title":"(Extra) Oefeningen"},{"location":"P4/P4-Oefeningen.html#oefening-41-area-51","text":"Niveau : gemakkelijk Stappen: Open een nieuw script Laad een Landsat-8 collectie in. Ga voor de beelden met de hoogste kwaliteit. Welke collectie kies je dan? Filter je collectie op basis van volgende gevens: Periode : september 2019 Locatie : Area 51. Hiervoor kun je volgende punt-locatie in je script gebruiken: var Area51 = ee . Geometry . Point ([ - 115.81441562461978 , 37.2386297535804 ]); Wolkbedekking : minder of gelijk aan 10% Hoeveel beelden blijven er nog over die aan bovenstaande criteria voldoen? Van de resterende beeldencollectie neem je het eerste beeld (niet gesorteerd). Bekijk de metadata. Van welke datum is dit beeld afkomstig? Visualiseer het beeld als een Normale Kleuren composiet, een Valse Kleurencomposiet. Je kunt zelf je visualisatieparameters defeni\u00ebren door ze handmatig aan in te stellen. Analyseer het volledige door jezelf volgende vragen te stellen: Waar is er vegetatie te vinden? Waar is dit natuurlijk, waar onnatuurlijk? Welke features herken je thv Area 51? Het gebied geeft ook een ideale aanleiding om de 'Geology'-composiet eens uit te testen. De combinaties is als volgt: RGB = SWIR2-SWIR1-BLUE . Vertaal dit zelf naar de Landsat-8 banden. (Maak gebruik van de Landsat 8 bandentabel ). Deze composiet maakt visuele inspectie van grote structurele eigenschappen van gesteenten (zoals plooien en breuken) gemakkelijker. Oplossing Script: https://code.earthengine.google.com/724bbee7796ebd4ff9657dc5f0c1baaa","title":"Oefening 4.1 - Area 51"},{"location":"P4/P4-Oefeningen.html#oefening-42-","text":"","title":"Oefening 4.2 -"},{"location":"P4/P4-Reducing.html","text":"Over Reducing \u2018Reducing\u2019 een beeld- of datacollectie in Google Earth Engine is het proces waarbij de beeldcollectie wordt geaggregeerd over tijd, ruimte, banden, .... In dit proces wordt een beeldcomposiet aangemaakt van de beschikbare beelden in de collectie, waarbij per pixel een bepaalde vooropgestelde waarde wordt gekozen, zoals het min, max, gemiddelde, mediaan,\u2026 De collectie wordt als het ware 'gereduceerd' tot \u00e9\u00e9n enkel visualiseerbaar beeld. Reducing an ImageCollection: principe. Een voorbeeld: neem de eerste pixel van de gefilterde en gesorteerde Landsat 8 collectie L8_sortedCC . Visualiseer het resultaat. Wat valt je op? Is ditmaal het volledige gebied bedekt? // Reducer over de L8_sortedCC collectie, waarbij steeds de eerste pixel genomen wordt. var L8_first_red = L8_sortedCC . reduce ( ee . Reducer . first ()); //Bekijk de eigenschappen van het gereduceerd beeld print ( L8_first_red ) var visParams_first = { bands : [ 'B4_first' , 'B3_first' , 'B2_first' ], min : 0 , max : 3000 , gamma : 1.4 , }; // Visualiseren als een normale kleurencomposiet Map . addLayer ( L8_first_red , visParams_first , 'L8_First pixels' ) Bandbenaming na reducing Let ook, bij het aanroepen van de reducer functie, worden ook de banden hernoemd. Houd hier rekening mee bij het visualiseren. Het eventueel hernoemen van banden kan via de functie .rename() Bandnamen bij de 'First'-gereduceerde collectie. ee.Reducer.first() VS .first() In een eerdere oefening namen we reeds het eerste beeld uit een hele collectie met de .first() functie. Dit is dus niet hetzelfde, gezien een reducer zicht niet beperkt tot \u00e9\u00e9n enkel beeld, maar de volledige collectie gaat reduceren. Shortcut syntax Bepaalde \u2013 veel gebruikte \u2013 reducers hebben ook een zogenaamde \u2018shortcut\u2019 syntax in Earth engine zoals mean() , median() , min() en sum() . Deze shortcut syntax zorgt ervoor dat een collectie eenvoudiger te reduceren is, zonder de hele .reduce()(ee.Reducer.mean()) syntax te moeten gebruiken. Een voorbeeld: //Een Median() Reducer over the Landsat-8 collectie var L8_median = L8 . reduce ( ee . Reducer . median ()); //Of via de short-syntax (geeft zelfde resultaat) var L8_median = L8 . median (); // Visualiseren Map . addLayer ( L8_median , trueColor , 'L8_median' ) Voorbeeld mediane reducer over de L8_sortedCC-collectie Opdracht Probeer enkele van de Reducers uit op je Sentinel-2 collectie van Gent. Bewaar je script.","title":"Reducing ImageCollections"},{"location":"P4/P4-Reducing.html#over-reducing","text":"\u2018Reducing\u2019 een beeld- of datacollectie in Google Earth Engine is het proces waarbij de beeldcollectie wordt geaggregeerd over tijd, ruimte, banden, .... In dit proces wordt een beeldcomposiet aangemaakt van de beschikbare beelden in de collectie, waarbij per pixel een bepaalde vooropgestelde waarde wordt gekozen, zoals het min, max, gemiddelde, mediaan,\u2026 De collectie wordt als het ware 'gereduceerd' tot \u00e9\u00e9n enkel visualiseerbaar beeld. Reducing an ImageCollection: principe. Een voorbeeld: neem de eerste pixel van de gefilterde en gesorteerde Landsat 8 collectie L8_sortedCC . Visualiseer het resultaat. Wat valt je op? Is ditmaal het volledige gebied bedekt? // Reducer over de L8_sortedCC collectie, waarbij steeds de eerste pixel genomen wordt. var L8_first_red = L8_sortedCC . reduce ( ee . Reducer . first ()); //Bekijk de eigenschappen van het gereduceerd beeld print ( L8_first_red ) var visParams_first = { bands : [ 'B4_first' , 'B3_first' , 'B2_first' ], min : 0 , max : 3000 , gamma : 1.4 , }; // Visualiseren als een normale kleurencomposiet Map . addLayer ( L8_first_red , visParams_first , 'L8_First pixels' ) Bandbenaming na reducing Let ook, bij het aanroepen van de reducer functie, worden ook de banden hernoemd. Houd hier rekening mee bij het visualiseren. Het eventueel hernoemen van banden kan via de functie .rename() Bandnamen bij de 'First'-gereduceerde collectie. ee.Reducer.first() VS .first() In een eerdere oefening namen we reeds het eerste beeld uit een hele collectie met de .first() functie. Dit is dus niet hetzelfde, gezien een reducer zicht niet beperkt tot \u00e9\u00e9n enkel beeld, maar de volledige collectie gaat reduceren.","title":"Over Reducing"},{"location":"P4/P4-Reducing.html#shortcut-syntax","text":"Bepaalde \u2013 veel gebruikte \u2013 reducers hebben ook een zogenaamde \u2018shortcut\u2019 syntax in Earth engine zoals mean() , median() , min() en sum() . Deze shortcut syntax zorgt ervoor dat een collectie eenvoudiger te reduceren is, zonder de hele .reduce()(ee.Reducer.mean()) syntax te moeten gebruiken. Een voorbeeld: //Een Median() Reducer over the Landsat-8 collectie var L8_median = L8 . reduce ( ee . Reducer . median ()); //Of via de short-syntax (geeft zelfde resultaat) var L8_median = L8 . median (); // Visualiseren Map . addLayer ( L8_median , trueColor , 'L8_median' ) Voorbeeld mediane reducer over de L8_sortedCC-collectie Opdracht Probeer enkele van de Reducers uit op je Sentinel-2 collectie van Gent. Bewaar je script.","title":"Shortcut syntax"},{"location":"P5/P5-Colormanipulation.html","text":"","title":"P5 Colormanipulation"},{"location":"P5/P5-ImageStatistics.html","text":"Histogram Een histogram is, binnen de remote sensing, een grafische weerave van de statistische frequentie van de pixelwaarden binnen een satellietbeeld. Deze pixelwaarden verspreiden zich tussen de waarden 0 en 255. In een histogram worden deze waarden op de x -as geplot, terwijl de overeenkomstige frequentie voor elke waarde binnen het beeld op de Y-as wordt geplot. In wat volgt maken we een histogram aan van het aangemaakte ndvi-beeld. Hiervoor is steeds een regio (dus polygoon) noodzakelijk, waarvoor een histogram wordt aangemaakt. In een eerste fase doen we dit voor het volledige weerhouden satellietbeeld. Op de geometrie van dit beeld naar earth engine te vertalen naar een polygoon maken we gebruik van de functie .geometry() . var ROI = ndvi . geometry (); Bekijk de resulterende ROI eventueel door deze te mappen met Map.addLayer(ROI) . Uiteraard kun je ook handmatig een polygoon intekenen. Om een histogram aan te maken wordt gebruik gemaakt van de ui.Chart.image.histogram() -functie binnen Earth Engine. Deze functie neemt volgende elementen aan (ook te checken via de 'Docs'): het beeld, de ROI, de schaal waarover de histogram wordt berekend, het aantal te plotten histogrambalkjes. Layout-opties worden afzonderlijk toegekend via .setOptions() . //Initialiseren van een historgram via de ui.Chart functie var ndviHist = ui . Chart . image . histogram ({ image : ndvi , region : ROI , scale : 10 , maxBuckets : 50 , maxPixels : 1e12 }); //Histogram updaten met stijlopties ndviHist = ndviHist . setOptions ({ title : 'Histogram van NDVI in de Gentse Haven' }); //Histogram schrijven naar de console print ( ndviHist ) Parameters meegeven aan een functie Als je parameters meegeeft aan een functie in Javascript, kun je dit op 2 manieren doen: De (noodzakelijke) parameters meegeven in volgorde aan de functie. Bijvoorbeeld: ui.Chart.image.histogram(ndvi, ROI, 10, 50) . Hierbij is het noodzakelijk dat de paramters in juist volgorde worden meegeven en er geen parameters worden overgeslagen. Opstellen van een dictionary , waarbij de parameters expliciet worden toegekend, zoals in het voorbeeld hierboven. Dit zorgt voor wat extra overzicht. var ndviHist = ui . Chart . image . histogram ({ image : ndvi , region : ROI , scale : 10 , maxBuckets : 50 , maxPixels : 1e12 }); NDVI Histogram van Haven Gent/Vlaanderen Als je kijkt naar het resulterend histogram, dan kun je enkele pieken opmerken: rond -0.3, rond 0.1 en rond 0.8. Verklaar de oorsprong van deze pieken. Bandstatistieken Om beeldstatistieken binnen een bepaalde ROI te berekenen binnen Earth Engine wordt gebruik gemaakt van image.reduceRegion() . Het principe van deze 'beeldreducer' is hetzelfde als een recuder van een ImageCollection , met dat verschil dat de pixels binnen een regio van eenzelfde beeld worden gereduceerd, zoals in onderstaande figuur wordt ge\u00efllustreerd. Hiermee kan m.a.w. - binnen een bepaalde ROI - bepaalde statistieken berekend worden, zoals het minimum, gemiddelde pixelwaarde, maximum, mediane waarde, ... Illustratie van een Reducer toegepast op een beeld (Image) binnen een bepaalde regio. //Statistieken van NDVI: Toepassen van een Reducer var ndvi_mean = ndvi . reduceRegion ({ reducer : ee . Reducer . mean (), geometry : ROI , scale : 10 , maxPixels : 1e12 }); print ( 'Gemiddelde NDVI-waarde' , ndvi_mean )","title":"Beeldstatistieken"},{"location":"P5/P5-ImageStatistics.html#histogram","text":"Een histogram is, binnen de remote sensing, een grafische weerave van de statistische frequentie van de pixelwaarden binnen een satellietbeeld. Deze pixelwaarden verspreiden zich tussen de waarden 0 en 255. In een histogram worden deze waarden op de x -as geplot, terwijl de overeenkomstige frequentie voor elke waarde binnen het beeld op de Y-as wordt geplot. In wat volgt maken we een histogram aan van het aangemaakte ndvi-beeld. Hiervoor is steeds een regio (dus polygoon) noodzakelijk, waarvoor een histogram wordt aangemaakt. In een eerste fase doen we dit voor het volledige weerhouden satellietbeeld. Op de geometrie van dit beeld naar earth engine te vertalen naar een polygoon maken we gebruik van de functie .geometry() . var ROI = ndvi . geometry (); Bekijk de resulterende ROI eventueel door deze te mappen met Map.addLayer(ROI) . Uiteraard kun je ook handmatig een polygoon intekenen. Om een histogram aan te maken wordt gebruik gemaakt van de ui.Chart.image.histogram() -functie binnen Earth Engine. Deze functie neemt volgende elementen aan (ook te checken via de 'Docs'): het beeld, de ROI, de schaal waarover de histogram wordt berekend, het aantal te plotten histogrambalkjes. Layout-opties worden afzonderlijk toegekend via .setOptions() . //Initialiseren van een historgram via de ui.Chart functie var ndviHist = ui . Chart . image . histogram ({ image : ndvi , region : ROI , scale : 10 , maxBuckets : 50 , maxPixels : 1e12 }); //Histogram updaten met stijlopties ndviHist = ndviHist . setOptions ({ title : 'Histogram van NDVI in de Gentse Haven' }); //Histogram schrijven naar de console print ( ndviHist ) Parameters meegeven aan een functie Als je parameters meegeeft aan een functie in Javascript, kun je dit op 2 manieren doen: De (noodzakelijke) parameters meegeven in volgorde aan de functie. Bijvoorbeeld: ui.Chart.image.histogram(ndvi, ROI, 10, 50) . Hierbij is het noodzakelijk dat de paramters in juist volgorde worden meegeven en er geen parameters worden overgeslagen. Opstellen van een dictionary , waarbij de parameters expliciet worden toegekend, zoals in het voorbeeld hierboven. Dit zorgt voor wat extra overzicht. var ndviHist = ui . Chart . image . histogram ({ image : ndvi , region : ROI , scale : 10 , maxBuckets : 50 , maxPixels : 1e12 }); NDVI Histogram van Haven Gent/Vlaanderen Als je kijkt naar het resulterend histogram, dan kun je enkele pieken opmerken: rond -0.3, rond 0.1 en rond 0.8. Verklaar de oorsprong van deze pieken.","title":"Histogram"},{"location":"P5/P5-ImageStatistics.html#bandstatistieken","text":"Om beeldstatistieken binnen een bepaalde ROI te berekenen binnen Earth Engine wordt gebruik gemaakt van image.reduceRegion() . Het principe van deze 'beeldreducer' is hetzelfde als een recuder van een ImageCollection , met dat verschil dat de pixels binnen een regio van eenzelfde beeld worden gereduceerd, zoals in onderstaande figuur wordt ge\u00efllustreerd. Hiermee kan m.a.w. - binnen een bepaalde ROI - bepaalde statistieken berekend worden, zoals het minimum, gemiddelde pixelwaarde, maximum, mediane waarde, ... Illustratie van een Reducer toegepast op een beeld (Image) binnen een bepaalde regio. //Statistieken van NDVI: Toepassen van een Reducer var ndvi_mean = ndvi . reduceRegion ({ reducer : ee . Reducer . mean (), geometry : ROI , scale : 10 , maxPixels : 1e12 }); print ( 'Gemiddelde NDVI-waarde' , ndvi_mean )","title":"Bandstatistieken"},{"location":"P5/P5-Intro.html","text":"In Practicum 5 bouwen we verder op de aangeleerde zaken uit Practicum 4. Met de beelden die we uit de `ImageCollections halen, gaan we statistieken halen, indices bereken en handelingen uitvoeren om informatie te benadrukken. Als slot behandelen we methodieken om een Principale Compentenanalyse (PCA) uit te voeren op de (multidimensionale) beelden.","title":"Intro"},{"location":"P5/P5-Oefeningen.html","text":"Onderstaande oefeningen kunnen gebruikt worden om de principes uit Practicum 4 verder in te oefenen. Oefening 5.1 - Herbebossing Regenwoud in Aimores, Brazili\u00eb Info : Examenopdracht 2019-2020. Tip: Gebruik van NDVI als indicator van ontbossing Context Voor deze opdracht trekken we naar Aimores, in de Braziliaanse provincie Minas Gerais. Dit gebied bestond honderden jaren geleden uit uitgestrekt tropisch bos, het Atlantische woud, en bevat een buitengewoon grote biodiversiteit. In de 20e eeuw werd het leeuwendeel van dit gebied echter ontbost, waardoor naar schatting slechts 15% van het Atlantische woud is overgebleven. In 1999 besloot een koppel om het heft in eigen handen te nemen door het starten van een herbebossingsproject in het gebied, met groot succes. Gegeven: Afbakening van het projectgebied (als Shape-file (Studiegebied). ). Tijdstip 1: 2000. Hiervoor dien je gebruik te maken van een Landsat-7 beeld . Tijdstip 2: 2020. Hiervoor maak je gebruik van een Landsat-8 beeld. Gevraagd: Maak een beeld aan, waar voor elke pixel te zien is of er vegetatie is bijgekomen of verdwenen tussen 2000 en 2020 binnen het projectgebied. Tips: Maak zelf een beeld aan per jaar aan waar de wolkbedekking ontbreekt of gemaskeerd is. Gebruik een gepaste index. De bandverdeling van Landsat 7/8 is verschillend! Houd hier rekening mee. Oplossing Script: https://code.earthengine.google.com/2a3fec22e58b9d8cc2f508606b151726 Oefening 5.2 - Brand-index Niveau : gemiddeld In volgende oefening bekijken we een index die de gebieden aangetast door bosbranden belicht: de Normalized Burn Ratio (NBR) . Door de NBI te berekenen voor en na natuurbranden, kan een spatiale inschatting worden gemaakt van de ernst van beschadiging aan de omgeving. De NBR wordt op een vergelijkbare manier als de NDVI berekend, waarbij het de rode band vervangen werd door een SWIR-band: NBR = {NIR - SWIR \\over NIR + SWIR}. In volgende oefening maken we een analyse van een grote bosbrand (genaamd 'Camp Fire') die op 8 november nabij Paradise in Californi\u00eb woedde (info hier ). Dit doen we aan de hand van Landsat-8 beelden. Stappenplan Start met een nieuwe script. Kopieer alvast de Cloud-mask functie (maskL8sr) voor Landsat-8 uit de datacatalog. Filter de L8 (Surface Reflectance, Tier 1) collectie op basis van volgende ROI: var ROI = ee . Geometry . Polygon ( [[[ - 121.63966294798034 , 39.877127888100304 ], [ - 121.63966294798034 , 39.71622101257041 ], [ - 121.35470506223815 , 39.71622101257041 ], [ - 121.35470506223815 , 39.877127888100304 ]]], null , false ); Maak een functie aan waarin de NBR-index wordt berekend voor de Landsat 8 banden . Gebruik de SWIR-2 band. ( tip ). Voeg de index toe aan de collectie met de .map()-functie. Filter de collectie verder met een maximale 'CLOUD_COVER' van 30% en pas de cloud mask toe. Maak nu 2 beelden aan binnen dezelfde periode van het jaar. Clip naar de ROI (met .clip()) en voeg ze toe als Normale Kleuren: 1 Beeld voor de brand: een mediaan binnen de periode 1 mei 2018 - 30 juni 2018 1 Beeld na de brand: een mediaan binnen de periode 1 mei 2019 - 30 juni 2019 Visualiseer nu ook de NBR's voor beide beelden (gebruik de range van [0,1]) Om een indicatie te krijgen van de brandernst, dient het verschil tussen beide NBR's te worden berekend. Voer hiervoor de som NBR_voor - NBR_na. Vermenigvuldig het resultaat met 1000 om een betere schaling te verkrijgen. Geef het de naam \"NBR_verschil\". Voeg de index ook als zwart-wild beeld toe aan het beeld, met een stretch tussen [-200, 1000] Visueel vallen er alvast heel wat zaken af te leiden. Echter voor verdere toepassing is het beter om de resulterende klassen op te delen, zoals in onderstaande tabel, opgesteld door de United States Geological Survey (USGS). Burn severity klassen, met bijhorende grenzen volgens het verschil in NBR-index (dNBR). Opgesteld door de USGS. (Bron: [UN-spider] ) Om het NBR-verschil beeld om te zetten naar deze discrete klassen, dienen we het beeld te reclassificeren . In Earth Engine doen we dit volgens een .where(), functie. Pas onderstaande code aan in je script, om dit toe te passen. Eveneens werd de bijhorende palette meegegeven. // Remap values. gt= greater than (>), lte = less then or equal (<=) var Burn_severity = ee . Image ( 1 ). clip ( ROI ) //Initialiseert een leeg beeld . where ( NBR_verschil . gt ( - 500 ). and ( NBR_verschil . lte ( - 251 )), 1 ) . where ( NBR_verschil . gt ( - 250 ). and ( NBR_verschil . lte ( - 101 )), 2 ) . where ( NBR_verschil . gt ( - 100 ). and ( NBR_verschil . lte ( 99 )), 3 ) . where ( NBR_verschil . gt ( 100 ). and ( NBR_verschil . lte ( 269 )), 4 ) . where ( NBR_verschil . gt ( 270 ). and ( NBR_verschil . lte ( 439 )), 5 ) . where ( NBR_verschil . gt ( 440 ). and ( NBR_verschil . lte ( 659 )), 6 ) . where ( NBR_verschil . gt ( 660 ). and ( NBR_verschil . lte ( 1300 )), 7 ) var BurnSeverity_VIS = { bands : [ \"constant\" ], palette : [ \"1b930c\" , \"98c825\" , \"00ff37\" , \"fff708\" , \"ffb716\" , \"ff7310\" , \"c20665\" ]} Map . addLayer ( Burn_severity , BurnSeverity_VIS , 'Burn Severity classes' ) Oplossing Script met volledige uitwerking: https://code.earthengine.google.com/3db2a972e2bfebef5a8615c8c2fa8c9a Oefening 5.2 - De Enhanced Vegetation Index (EVI) De EVI index De EVI is gelijkaardig aan de NDVI daar het gebruikt wordt om de aanwezigheid (of \u2018greenness\u2019) van vegetatie a.d.h.v. satellietbeelden te kwantificeren. Het werd ontwikkeld om aan enkele \u201climitaties\u201d van de ndvi te voldoen: EVI is gevoeliger voor gebieden met hogere biomassa EVI reduceert de invloed van de atmosferische condities EVI corrigeert de \u2018canopy background noise\u2019 , die bij NDVI voorkomt De EVI is het meest gebruikte alternatief voor de NDVI. De EVI wordt berekend als volgt: EVI = G * {NIR - R \\over NIR + C1 * RED \u2013 C2*BLUE + L}. (waarbij G : een versterkende constante, C1,C2 co\u00ebfficienten en L een \u2018canopy background adjusment factor\u2019 ) Voor Sentinel 2, wordt deze formule: EVI_{S2} = 2.5 * {B8 - B4 \\over B8 + 6 * B4 \u2013 7.5*B2 + 1}.","title":"Oefeningen"},{"location":"P5/P5-Oefeningen.html#oefening-51-herbebossing-regenwoud-in-aimores-brazilie","text":"Info : Examenopdracht 2019-2020. Tip: Gebruik van NDVI als indicator van ontbossing","title":"Oefening 5.1 - Herbebossing Regenwoud in Aimores, Brazili\u00eb"},{"location":"P5/P5-Oefeningen.html#context","text":"Voor deze opdracht trekken we naar Aimores, in de Braziliaanse provincie Minas Gerais. Dit gebied bestond honderden jaren geleden uit uitgestrekt tropisch bos, het Atlantische woud, en bevat een buitengewoon grote biodiversiteit. In de 20e eeuw werd het leeuwendeel van dit gebied echter ontbost, waardoor naar schatting slechts 15% van het Atlantische woud is overgebleven. In 1999 besloot een koppel om het heft in eigen handen te nemen door het starten van een herbebossingsproject in het gebied, met groot succes.","title":"Context"},{"location":"P5/P5-Oefeningen.html#gegeven","text":"Afbakening van het projectgebied (als Shape-file (Studiegebied). ). Tijdstip 1: 2000. Hiervoor dien je gebruik te maken van een Landsat-7 beeld . Tijdstip 2: 2020. Hiervoor maak je gebruik van een Landsat-8 beeld.","title":"Gegeven:"},{"location":"P5/P5-Oefeningen.html#gevraagd","text":"Maak een beeld aan, waar voor elke pixel te zien is of er vegetatie is bijgekomen of verdwenen tussen 2000 en 2020 binnen het projectgebied.","title":"Gevraagd:"},{"location":"P5/P5-Oefeningen.html#tips","text":"Maak zelf een beeld aan per jaar aan waar de wolkbedekking ontbreekt of gemaskeerd is. Gebruik een gepaste index. De bandverdeling van Landsat 7/8 is verschillend! Houd hier rekening mee. Oplossing Script: https://code.earthengine.google.com/2a3fec22e58b9d8cc2f508606b151726","title":"Tips:"},{"location":"P5/P5-Oefeningen.html#oefening-52-brand-index","text":"Niveau : gemiddeld In volgende oefening bekijken we een index die de gebieden aangetast door bosbranden belicht: de Normalized Burn Ratio (NBR) . Door de NBI te berekenen voor en na natuurbranden, kan een spatiale inschatting worden gemaakt van de ernst van beschadiging aan de omgeving. De NBR wordt op een vergelijkbare manier als de NDVI berekend, waarbij het de rode band vervangen werd door een SWIR-band: NBR = {NIR - SWIR \\over NIR + SWIR}. In volgende oefening maken we een analyse van een grote bosbrand (genaamd 'Camp Fire') die op 8 november nabij Paradise in Californi\u00eb woedde (info hier ). Dit doen we aan de hand van Landsat-8 beelden.","title":"Oefening 5.2 - Brand-index"},{"location":"P5/P5-Oefeningen.html#stappenplan","text":"Start met een nieuwe script. Kopieer alvast de Cloud-mask functie (maskL8sr) voor Landsat-8 uit de datacatalog. Filter de L8 (Surface Reflectance, Tier 1) collectie op basis van volgende ROI: var ROI = ee . Geometry . Polygon ( [[[ - 121.63966294798034 , 39.877127888100304 ], [ - 121.63966294798034 , 39.71622101257041 ], [ - 121.35470506223815 , 39.71622101257041 ], [ - 121.35470506223815 , 39.877127888100304 ]]], null , false ); Maak een functie aan waarin de NBR-index wordt berekend voor de Landsat 8 banden . Gebruik de SWIR-2 band. ( tip ). Voeg de index toe aan de collectie met de .map()-functie. Filter de collectie verder met een maximale 'CLOUD_COVER' van 30% en pas de cloud mask toe. Maak nu 2 beelden aan binnen dezelfde periode van het jaar. Clip naar de ROI (met .clip()) en voeg ze toe als Normale Kleuren: 1 Beeld voor de brand: een mediaan binnen de periode 1 mei 2018 - 30 juni 2018 1 Beeld na de brand: een mediaan binnen de periode 1 mei 2019 - 30 juni 2019 Visualiseer nu ook de NBR's voor beide beelden (gebruik de range van [0,1]) Om een indicatie te krijgen van de brandernst, dient het verschil tussen beide NBR's te worden berekend. Voer hiervoor de som NBR_voor - NBR_na. Vermenigvuldig het resultaat met 1000 om een betere schaling te verkrijgen. Geef het de naam \"NBR_verschil\". Voeg de index ook als zwart-wild beeld toe aan het beeld, met een stretch tussen [-200, 1000] Visueel vallen er alvast heel wat zaken af te leiden. Echter voor verdere toepassing is het beter om de resulterende klassen op te delen, zoals in onderstaande tabel, opgesteld door de United States Geological Survey (USGS). Burn severity klassen, met bijhorende grenzen volgens het verschil in NBR-index (dNBR). Opgesteld door de USGS. (Bron: [UN-spider] ) Om het NBR-verschil beeld om te zetten naar deze discrete klassen, dienen we het beeld te reclassificeren . In Earth Engine doen we dit volgens een .where(), functie. Pas onderstaande code aan in je script, om dit toe te passen. Eveneens werd de bijhorende palette meegegeven. // Remap values. gt= greater than (>), lte = less then or equal (<=) var Burn_severity = ee . Image ( 1 ). clip ( ROI ) //Initialiseert een leeg beeld . where ( NBR_verschil . gt ( - 500 ). and ( NBR_verschil . lte ( - 251 )), 1 ) . where ( NBR_verschil . gt ( - 250 ). and ( NBR_verschil . lte ( - 101 )), 2 ) . where ( NBR_verschil . gt ( - 100 ). and ( NBR_verschil . lte ( 99 )), 3 ) . where ( NBR_verschil . gt ( 100 ). and ( NBR_verschil . lte ( 269 )), 4 ) . where ( NBR_verschil . gt ( 270 ). and ( NBR_verschil . lte ( 439 )), 5 ) . where ( NBR_verschil . gt ( 440 ). and ( NBR_verschil . lte ( 659 )), 6 ) . where ( NBR_verschil . gt ( 660 ). and ( NBR_verschil . lte ( 1300 )), 7 ) var BurnSeverity_VIS = { bands : [ \"constant\" ], palette : [ \"1b930c\" , \"98c825\" , \"00ff37\" , \"fff708\" , \"ffb716\" , \"ff7310\" , \"c20665\" ]} Map . addLayer ( Burn_severity , BurnSeverity_VIS , 'Burn Severity classes' ) Oplossing Script met volledige uitwerking: https://code.earthengine.google.com/3db2a972e2bfebef5a8615c8c2fa8c9a","title":"Stappenplan"},{"location":"P5/P5-Oefeningen.html#oefening-52-de-enhanced-vegetation-index-evi","text":"","title":"Oefening 5.2 - De Enhanced Vegetation Index (EVI)"},{"location":"P5/P5-Oefeningen.html#de-evi-index","text":"De EVI is gelijkaardig aan de NDVI daar het gebruikt wordt om de aanwezigheid (of \u2018greenness\u2019) van vegetatie a.d.h.v. satellietbeelden te kwantificeren. Het werd ontwikkeld om aan enkele \u201climitaties\u201d van de ndvi te voldoen: EVI is gevoeliger voor gebieden met hogere biomassa EVI reduceert de invloed van de atmosferische condities EVI corrigeert de \u2018canopy background noise\u2019 , die bij NDVI voorkomt De EVI is het meest gebruikte alternatief voor de NDVI. De EVI wordt berekend als volgt: EVI = G * {NIR - R \\over NIR + C1 * RED \u2013 C2*BLUE + L}. (waarbij G : een versterkende constante, C1,C2 co\u00ebfficienten en L een \u2018canopy background adjusment factor\u2019 ) Voor Sentinel 2, wordt deze formule: EVI_{S2} = 2.5 * {B8 - B4 \\over B8 + 6 * B4 \u2013 7.5*B2 + 1}.","title":"De EVI index"},{"location":"P5/P5-PCA.html","text":"De Principale componentenanalyse Een principale componentenanalyse (PCA) is een lineaire transformatie waarbij de banden worden gezocht die het meeste informatie bevatten. Veelal wordt deze gebruikt om processingtijd van grote datasets te beperken, zoals vaak bij hyperspectrale beelden het geval is. Het achterliggende principe is dat de verschillende spectrale banden van een sensor vaak informatie dragen die (gedeeltelijk) gecorreleerd zijn aan elkaar, waardoor er sprake is van onnodige of dubbele informatie. PCA zorgt voor een transformatie van de multispectrale data zodat nieuwe variabelen niet meer of amper met elkaar gecorreleerd zijn. Principe van PCA voor een dataset met 2 banden (rood en groen) die met elkaar gecorreleerd zijn. Een PCA werd uitgevoerd, bestaande uit een translate (van o naar o\u2019) en een rotatie zodoende dat de er 2 nieuwe variabelen ontstaan (PC1 en PC2). De eerste principale component wordt dus gevonden door de richting in de data waar het meeste variatie te vinden is en bijgevolg de meeste data. De 2e principale component komt hier loodrecht op te staan. Het spreekt voor zich dit dit in een 2-dimensionale ruimte zoals in bovenstaande figuur eenvoudig is, maar dit kan eveneens worden toegepast in een meer-dimensionale dataset (zoals bijvoorbeeld een Sentinel-2 beeld met 12 banden). Mathematisch gezien kan een PCA worden gezien als een zoektocht naar een translatie en rotatie van de multidimensionale dataset, waardoor de covariantiematrix van de dataset een diagonale matrix wordt na transformatie. In andere woorden, elke nieuwe variabele is gecorreleerd met zichzelf en niet meer met de andere variabele. De eigenwaarden en eigenvectoren van de originele covariantiematrix worden dus berekend. Elke eigenwaarde met de geassocieerde eigenvector beschrijven dan de nieuwe principale component, waarbij de eigenvector de richting geeft van de nieuwe component en de eigenwaarde als een proxy dient voor de hoeveel informatie dat de component bevat. (voor de specifieke wiskundige details kan worden verwezen naar de cursus van Wiskunde 1 (Lineaire algebra) of naar deze 5-minuten durende opfrissing. . In Earth Engine is het relatief eenvoudig om de PCA-berekeningen door te voeren. Open een Nieuw Script: 'PC5-PCA' We nemen opnieuw het Sentinel-2 beeld uit Belem. Aangezien enkel de banden B2, B3, B4, B5, B6, B6, B8, B8A, B11 en B12 relevant zijn voor verdere analyse, weerwhouden we enkel deze banden met de functie .select() . //Importeren van het Sentinel-2 beeld van Belem var S2_Belem = ee . Image ( 'COPERNICUS/S2_SR/20200808T134219_20200808T134214_T22MGD' ) //Enkel banden relevant voor de PCA weerhouden in de Image var bands = [ 'B2' , 'B3' , 'B4' , 'B5' , 'B6' , 'B7' , 'B8' , 'B8A' , 'B11' , 'B12' ]; var S2_Belem = S2_Belem . select ( bands ) Opstellen van de PCA-functie (met bijhorende 'helper'-functie) //PCA FUNCTIE var getPrincipalComponents = function ( centered , scale , region ) { // Collapse the bands of the image into a 1D array per pixel. var arrays = centered . toArray (); // Berekenen van de covariantie a.d.h.v. een reduceRegion var covar = arrays . reduceRegion ({ reducer : ee . Reducer . centeredCovariance (), geometry : region , scale : scale , maxPixels : 1e9 }); // Get the 'array' covariance result and cast to an array. // This represents the band-to-band covariance within the region. var covarArray = ee . Array ( covar . get ( 'array' )); // Perform an eigen analysis and slice apart the values and vectors. var eigens = covarArray . eigen (); print ( eigens ) // eigenValues is a P-length vector of Eigenvalues. var eigenValues = eigens . slice ( 1 , 0 , 1 ); // This is a PxP matrix with eigenvectors in rows. var eigenVectors = eigens . slice ( 1 , 1 ); // Convert the array image to 2D arrays for matrix computations. var arrayImage = arrays . toArray ( 1 ); // Left multiply the image array by the matrix of eigenvectors. var principalComponents = ee . Image ( eigenVectors ). matrixMultiply ( arrayImage ); // Turn the square roots of the Eigenvalues into a P-band image. var sdImage = ee . Image ( eigenValues . sqrt ()) . arrayProject ([ 0 ]). arrayFlatten ([ getNewBandNames ( 'sd' )]); // Turn the PCs into a P-band image, normalized by SD. return principalComponents // Throw out an an unneeded dimension, [[]] -> []. . arrayProject ([ 0 ]) // Make the one band array image a multi-band image, [] -> ima .arrayFlatten([getNewBandNames('pc')]) // Normalize the PCs by their SDs. . divide ( sdImage ); }; // // De PCAfunctie heeft noog aan een helper-functie, dat een lijst met nieuwe bandnamen samenstelt var getNewBandNames = function ( prefix ) { var seq = ee . List . sequence ( 1 , bandNames . length ()); return seq . map ( function ( b ) { return ee . String ( prefix ). cat ( ee . Number ( b ). int ()); }); }; Toepassen van de PCA-functie. Hiervoor dienen eerst enkele inputparameters met informatie worden aangemaakt /* DE PCA-functie is opgesteld. In wat volgt kunnen we deze toepassen */ // Set some information about the input to be used later. var scale = 30 ; //(30m om processingtijd wat te verminderen => sentinel2 kan tot 10m) var bandNames = S2_Belem . bandNames (); var region = S2_Belem . geometry (); // We nemen de regio van het volledige beeld De gebruikte PCA-functie heeft een 'mean centered' beeld nodig. Dit betekend dat het gemiddelde per band de nieuwe '0-waarde' wordt en elke pixel een waarde krijgt relatief aan deze 0-waarde. Dit zorgt voor een snellere covariantie-berekening. // Mean center the data to enable a faster covariance reducer // and an SD stretch of the principal components. var meanDict = S2_Belem . reduceRegion ({ reducer : ee . Reducer . mean (), geometry : region , scale : scale , maxPixels : 1e12 }); var means = ee . Image . constant ( meanDict . values ( bandNames )); print ( means , 'Gemiddeldes per band' ) var centered = S2_Belem . subtract ( means ); print ( centered , 'Mean centered Image' ) 6. Vervolgens kunnen we de PCA-functie toepassen // Uitvoeren van de PCA-functie => resultaat is beeld met PC's als nieuwe banden var pcaImage = getPrincipalComponents ( centered , scale , region ); //Bekijken van pcaImage //print(pcaImage) 7. De resulterende Image , pcaImage is een beeld met als banden de berekende principale componenten. Via een for -lus kunnen we elk deze banden ook in 1x plotten naar de MA // Plot each PC as a new layer for ( var i = 0 ; i < bandNames . length (). getInfo (); i ++ ) { var band = pcaImage . bandNames (). get ( i ). getInfo (); Map . addLayer ( pcaImage . select ([ band ]), { min : - 2 , max : 2 }, band ); } Resultaten: Principale Component 1 Principale Component 2 Principale Component 3","title":"Principale componenten analyse"},{"location":"P5/P5-PCA.html#de-principale-componentenanalyse","text":"Een principale componentenanalyse (PCA) is een lineaire transformatie waarbij de banden worden gezocht die het meeste informatie bevatten. Veelal wordt deze gebruikt om processingtijd van grote datasets te beperken, zoals vaak bij hyperspectrale beelden het geval is. Het achterliggende principe is dat de verschillende spectrale banden van een sensor vaak informatie dragen die (gedeeltelijk) gecorreleerd zijn aan elkaar, waardoor er sprake is van onnodige of dubbele informatie. PCA zorgt voor een transformatie van de multispectrale data zodat nieuwe variabelen niet meer of amper met elkaar gecorreleerd zijn. Principe van PCA voor een dataset met 2 banden (rood en groen) die met elkaar gecorreleerd zijn. Een PCA werd uitgevoerd, bestaande uit een translate (van o naar o\u2019) en een rotatie zodoende dat de er 2 nieuwe variabelen ontstaan (PC1 en PC2). De eerste principale component wordt dus gevonden door de richting in de data waar het meeste variatie te vinden is en bijgevolg de meeste data. De 2e principale component komt hier loodrecht op te staan. Het spreekt voor zich dit dit in een 2-dimensionale ruimte zoals in bovenstaande figuur eenvoudig is, maar dit kan eveneens worden toegepast in een meer-dimensionale dataset (zoals bijvoorbeeld een Sentinel-2 beeld met 12 banden). Mathematisch gezien kan een PCA worden gezien als een zoektocht naar een translatie en rotatie van de multidimensionale dataset, waardoor de covariantiematrix van de dataset een diagonale matrix wordt na transformatie. In andere woorden, elke nieuwe variabele is gecorreleerd met zichzelf en niet meer met de andere variabele. De eigenwaarden en eigenvectoren van de originele covariantiematrix worden dus berekend. Elke eigenwaarde met de geassocieerde eigenvector beschrijven dan de nieuwe principale component, waarbij de eigenvector de richting geeft van de nieuwe component en de eigenwaarde als een proxy dient voor de hoeveel informatie dat de component bevat. (voor de specifieke wiskundige details kan worden verwezen naar de cursus van Wiskunde 1 (Lineaire algebra) of naar deze 5-minuten durende opfrissing. . In Earth Engine is het relatief eenvoudig om de PCA-berekeningen door te voeren. Open een Nieuw Script: 'PC5-PCA' We nemen opnieuw het Sentinel-2 beeld uit Belem. Aangezien enkel de banden B2, B3, B4, B5, B6, B6, B8, B8A, B11 en B12 relevant zijn voor verdere analyse, weerwhouden we enkel deze banden met de functie .select() . //Importeren van het Sentinel-2 beeld van Belem var S2_Belem = ee . Image ( 'COPERNICUS/S2_SR/20200808T134219_20200808T134214_T22MGD' ) //Enkel banden relevant voor de PCA weerhouden in de Image var bands = [ 'B2' , 'B3' , 'B4' , 'B5' , 'B6' , 'B7' , 'B8' , 'B8A' , 'B11' , 'B12' ]; var S2_Belem = S2_Belem . select ( bands ) Opstellen van de PCA-functie (met bijhorende 'helper'-functie) //PCA FUNCTIE var getPrincipalComponents = function ( centered , scale , region ) { // Collapse the bands of the image into a 1D array per pixel. var arrays = centered . toArray (); // Berekenen van de covariantie a.d.h.v. een reduceRegion var covar = arrays . reduceRegion ({ reducer : ee . Reducer . centeredCovariance (), geometry : region , scale : scale , maxPixels : 1e9 }); // Get the 'array' covariance result and cast to an array. // This represents the band-to-band covariance within the region. var covarArray = ee . Array ( covar . get ( 'array' )); // Perform an eigen analysis and slice apart the values and vectors. var eigens = covarArray . eigen (); print ( eigens ) // eigenValues is a P-length vector of Eigenvalues. var eigenValues = eigens . slice ( 1 , 0 , 1 ); // This is a PxP matrix with eigenvectors in rows. var eigenVectors = eigens . slice ( 1 , 1 ); // Convert the array image to 2D arrays for matrix computations. var arrayImage = arrays . toArray ( 1 ); // Left multiply the image array by the matrix of eigenvectors. var principalComponents = ee . Image ( eigenVectors ). matrixMultiply ( arrayImage ); // Turn the square roots of the Eigenvalues into a P-band image. var sdImage = ee . Image ( eigenValues . sqrt ()) . arrayProject ([ 0 ]). arrayFlatten ([ getNewBandNames ( 'sd' )]); // Turn the PCs into a P-band image, normalized by SD. return principalComponents // Throw out an an unneeded dimension, [[]] -> []. . arrayProject ([ 0 ]) // Make the one band array image a multi-band image, [] -> ima .arrayFlatten([getNewBandNames('pc')]) // Normalize the PCs by their SDs. . divide ( sdImage ); }; // // De PCAfunctie heeft noog aan een helper-functie, dat een lijst met nieuwe bandnamen samenstelt var getNewBandNames = function ( prefix ) { var seq = ee . List . sequence ( 1 , bandNames . length ()); return seq . map ( function ( b ) { return ee . String ( prefix ). cat ( ee . Number ( b ). int ()); }); }; Toepassen van de PCA-functie. Hiervoor dienen eerst enkele inputparameters met informatie worden aangemaakt /* DE PCA-functie is opgesteld. In wat volgt kunnen we deze toepassen */ // Set some information about the input to be used later. var scale = 30 ; //(30m om processingtijd wat te verminderen => sentinel2 kan tot 10m) var bandNames = S2_Belem . bandNames (); var region = S2_Belem . geometry (); // We nemen de regio van het volledige beeld De gebruikte PCA-functie heeft een 'mean centered' beeld nodig. Dit betekend dat het gemiddelde per band de nieuwe '0-waarde' wordt en elke pixel een waarde krijgt relatief aan deze 0-waarde. Dit zorgt voor een snellere covariantie-berekening. // Mean center the data to enable a faster covariance reducer // and an SD stretch of the principal components. var meanDict = S2_Belem . reduceRegion ({ reducer : ee . Reducer . mean (), geometry : region , scale : scale , maxPixels : 1e12 }); var means = ee . Image . constant ( meanDict . values ( bandNames )); print ( means , 'Gemiddeldes per band' ) var centered = S2_Belem . subtract ( means ); print ( centered , 'Mean centered Image' ) 6. Vervolgens kunnen we de PCA-functie toepassen // Uitvoeren van de PCA-functie => resultaat is beeld met PC's als nieuwe banden var pcaImage = getPrincipalComponents ( centered , scale , region ); //Bekijken van pcaImage //print(pcaImage) 7. De resulterende Image , pcaImage is een beeld met als banden de berekende principale componenten. Via een for -lus kunnen we elk deze banden ook in 1x plotten naar de MA // Plot each PC as a new layer for ( var i = 0 ; i < bandNames . length (). getInfo (); i ++ ) { var band = pcaImage . bandNames (). get ( i ). getInfo (); Map . addLayer ( pcaImage . select ([ band ]), { min : - 2 , max : 2 }, band ); } Resultaten: Principale Component 1 Principale Component 2 Principale Component 3","title":"De Principale componentenanalyse"},{"location":"P5/P5-Spectral_indices.html","text":"Spectral indices Spectral indices zijn combinaties van 2 of meerdere spectrale banden die gebruikt worden om bepaalde features extra in de verf te zetten of ze te herberekenen naar een relatieve schaal. NDVI De meest gebruikte index is de Normalized Difference Vegatation Index (NDVI) , en wordt berekend als: NDVI = { NIR - RED \\over NIR + RED}. Waarbij: NIR = reflectie in het nabij-infrarode gebied van het spectrum (oftwel Near-Infrared) RED = reflectie in het rode gebied van het spectrum De resulterende index krijgt waarden binnen tussen -1 en 1. Volgens deze formule is de densiteit van vegetatie (NDVI) op een gegeven plaats in het beeld gelijk aan de verschillen in intensiteit van het gereflecteerde licht in het rood en infrarode deel van het spectrum, gedeeld door de soms van deze intensiteiten. Vegetatie absorbeert immers een groot deel van het zichtbare licht ten behoeve van de fotosynthese (dus lage Rood-reflectie), maar weerkaatst vrijwel al het infrarode licht (hoge IR-reflectiewaarde), waardoor de ndvi stijgt. Hoe denser de vegetatie, hoe hoger de ndvi. Andere lichamen, zoals water, observeren IR dan weer beter tot zeer goed, waardoor de ndvi daalt. In Earth Engine kan de NDVI op verschillende manieren berekend worden. We starten met de \u2018meest conventionele\u2019. We starten deze oefening in de Gentse haven. Maal een puntsymbool aan ergens ter hoogte van de Gentse haven in Evergem. Importeer de Sentinel-2 Surface Reflectance (Tier 1) collection en zoek naar het beeld met de laagste wolkbedekking uit 2019 in de periode mei-juni (= de late lente). Bekijk van welke datum het beeld afkomstig is. Visualiseer als een valse kleurencomposiet //1. Importeren van de Sentinel-2 collectie. var S2 = ee . ImageCollection ( \"COPERNICUS/S2_SR\" ); //Filteren op basis van datum (lente 2019) + beeld met laagste wolkenpercentage selecteren var S2_Gent_Lente19 = S2 . filterBounds ( HavenGent ) . filterDate ( '2019-03-20' , '2019-06-30' ) . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ) . first (); print ( 'Gent_Lente19:' , S2_Gent_Lente19 ) //Visualisatieparameters (of handmatig instellen) var S2_ValseKleuren = { gamma : 2 , min : 275 , max : 2088 , bands : [ 'B8' , 'B4' , 'B3' ], }; //Toevoegen aan Map Map . addLayer ( S2_Gent_Lente19 , S2_ValseKleuren , 'Valse Kleuren lente 2019' ) Een eerste methode om een NDVI aan te maken is via de ingebouwde .normalizedDifference() functie. Ga na welke Sentinel-2 banden je nodig hebt om de ndvi te berekenen. (Maak eventueel gebruik van de \u2018Docs\u2019-tab.) //2. Aanmaken NDVI via NormalDifference()-functie. Vul de '?' in var ndvi = S2_Gent_Lente19 . normalizedDifference ([ '?' , '?' ]). rename ( 'NDVI' ); Map . addLayer ( ndvi ,{}, 'ndvi_lente_2019' ) //Zonder visualisatieparametes Een ndvi wordt meestal afgebeeld met een kleurenschema, zoals onderstaand voorbeeld: // Met visualisatie var ndviParams = { min : - 1 , max : 1 , palette : [ 'red' , 'yellow' , 'darkgreen' ]}; Map . addLayer ( ndvi , ndviParams , 'ndvi_2019_vis' ); Oefening: Connecteer de ndvi-waarden met de gepaste landbedekkingsklasse. NDVI waarde Landbedekking (Lente) Negative values (< 0) rocks, bare soil, clouds Small values (0.1 or less) shrubs and meadows Moderate values (0.2 to 0.3) temperate and tropical forests Large values (0.6 to 0.8) (clouds,) water and snow Antwoord NDVI waarde Landbedekking (Lente) Negative values (< 0) clouds, water and snow Small values (0.1 or less) rocks and bare soil Moderate values (0.2 to 0.3) shrubs and meadows Large values (0.6 to 0.8) temperate and tropical forests Band Math (bandbewerkingen) Bandbewerkingen kunnen worden gebruikt om een nieuw beeld aan te maken van de reeds bestaande banden. Het berekenen van indices zoals de NDVI, is al een treffend voorbeeld hiervan. Andere mogelijkheden zijn ratio\u2019s, het verschil van 2 beelden op 2 verschillende tijdstippen om mogelijke veranderingen visueel te benadrukken, \u2026 Er zijn 2 manieren om in Earth Engine een bewerking uit te voeren. Bewerkingen via operatoren De basisoperators maken gebruik van 2 inputs: ofwel 2 beelden, ofwel 1 beeld en 1 constante. De bewerkingen worden steeds per pixel en per band uitgevoerd. Voorbeeld van operatoren zijn add() , subtract() en divide() . //NDVI berekenen aan de hand van bandwerkingen met operatoren // Lange uitwerking: noodzakelijke banden eerst selecteren en onderbrengen in een nieuwe variabele var nir = S2_Gent_Lente19 . select ( 'B8' ); var red = S2_Gent_Lente19 . select ( 'B4' ); var ndvi2 = nir . subtract ( red ). divide ( nir . add ( red )). rename ( 'NDVI' ); Map . addLayer ( ndvi2 , ndviParams , 'ndvi via operatoren' ); Het resultaat is logischerwijs identiek als de voorgaande ndvi-berekening. Bewerkingen via expressies Het spreekt voor zich dat bovenstaande methode voor complex wiskundige bewerkingen niet handig is. Voor dergelijke bewerkingen wordt aangeraden om gebruik te maken van image.expression() , gezien de inputvariabelen hier afzonderlijk worden aangegeven, waardoor de bewerking gemakkelijker wordt weergegeven en het coderen zo vereenvoudigd wordt. De expressie aanvaardt tevens ook constanten. Variabelen die binnen de expressie worden gebruikt, moeten steeds worden aangegeven, zoals in onderstaande NDVI-berekening; //NDVI aan de hand van een expressie var ndvi3 = SS2_Gent_Lente19 . expression ( '(NIR - RED)/(NIR + RED)' , { 'NIR' : S2_Gent_Lente19 . select ( 'B8' ), 'RED' : S2_Gent_Lente19 . select ( 'B4' ) }); Ook hier is het resultaat hetzelfde als de vorige ndvi-berekeningen. Gebruikte operators binnen expressies Onderstaande tabel geeft de binnen de expressies gehanteerde operators weer (bron: Earth Engine guide ) Andere indices Naast de NDVI bestaan er nog een heleboel andere indices, elk met een eigen toepassing. De Normalized Difference Water Index (NDWI) Er bestaan 2 indices met de naam 'NDWI', beiden gerelateerd aan water: De NDWI ontwikkeld door Gao (1996) , als index voor het watergehalte van vegetatie. De Index is gebaseerd op de NIR (gevoelig voor vegetatie) en SWIR (gevoelig voor water) banden: NDWI = { NIR - SWIR \\over NIR + SWIR}. De NDWI ontwikkeld door McFeeters (1996) , als index voor het verscherpen van verschillen in waterlichamen; NDWI = { GREEN - NIR \\over GREEN + NIR}. Opdracht NDWI Test beide NDWI-indices uit op het Sentinel-beeld van de Gentse Haven en omstreken. Bekijk de verschillen. Kijk hiervoor zeker naar naburige natuurgebieden en waterplassen. Opdrachten NDVI voor elk seizoen Maak een wolkenvrije beeldencollectie aan (via 30%-'CLOUDY_PIXEL_PERCENTAGE'-filter + een cloudmask toepassen) aan van de regio Durbuy binnen volgende periodes. Gebruik onderstaande Cloudmask-functie: function maskS2clouds ( image ) { var qa = image . select ( 'QA60' ); // Bits 10 and 11 are clouds and cirrus, respectively. var cloudBitMask = 1 << 10 ; var cirrusBitMask = 1 << 11 ; // Both flags should be set to zero, indicating clear conditions. var mask = qa . bitwiseAnd ( cloudBitMask ). eq ( 0 ) . and ( qa . bitwiseAnd ( cirrusBitMask ). eq ( 0 )); return image . updateMask ( mask ); } 2. Maak een functie aan om de NDVI te berekenen. Laat de functie dan los op de Imagecollectie via .map() . Maak aan de hand van de collectie 3 beelden aan met een median() -reducer, binnen volgende periodes: A. Jan-Februari (Winter) B. April-Mei (Lente) C. Juli-Augustus (Zomer) Visualiseer voor elk seizoen een Normale Kleurencomposiet en een NDVI-beeld. Gebruik onderstaande visualisatieparameters bij het plotten: //Visualisatieparameters instellen var NormaleKleuren = { min : 0 , max : 1500 , bands : [ 'B4' , 'B3' , 'B2' ], }; var ndviParams = { min : 0 , max : 1 , bands : [ 'NDVI' ], palette : [ 'red' , 'yellow' , 'darkgreen' ] }; Oplossing Oefening NDVI per seizoen Via deze link: https://code.earthengine.google.com/a1243399a079ca44bff6550e960354ed?noload=true EXTRA: Toevoegen van een legende Om een overzichtelijke legende toe te voegen aan je kaart, kun je onderstaande code gebruiken. Hiermee voeg je een legendepaneel toe voor continue ndvi-data. //------------------------- /* LEGENDE TOEVOEGEN */ //------------------------- // set position of panel var legend = ui . Panel ({ style : { position : 'bottom-left' , padding : '8px 15px' } }); // Create legend title var legendTitle = ui . Label ({ value : 'ndvi' , style : { fontWeight : 'bold' , fontSize : '18px' , margin : '0 0 4px 0' , padding : '0' } }); // Add the title to the panel legend . add ( legendTitle ); // create the legend image var lon = ee . Image . pixelLonLat (). select ( 'latitude' ); var gradient = lon . multiply (( ndviParams . max - ndviParams . min ) / 100.0 ). add ( ndviParams . min ); var legendImage = gradient . visualize ( ndviParams ); // create text on top of legend var panel = ui . Panel ({ widgets : [ ui . Label ( ndviParams [ 'max' ]) ], }); legend . add ( panel ); // create thumbnail from the image var thumbnail = ui . Thumbnail ({ image : legendImage , params : { bbox : '0,0,10,100' , dimensions : '10x200' }, style : { padding : '1px' , position : 'bottom-center' } }); // add the thumbnail to the legend legend . add ( thumbnail ); // create text on top of legend var panel = ui . Panel ({ widgets : [ ui . Label ( ndviParams [ 'min' ]) ], }); legend . add ( panel ); Map . add ( legend );","title":"Spectrale indices"},{"location":"P5/P5-Spectral_indices.html#spectral-indices","text":"Spectral indices zijn combinaties van 2 of meerdere spectrale banden die gebruikt worden om bepaalde features extra in de verf te zetten of ze te herberekenen naar een relatieve schaal.","title":"Spectral indices"},{"location":"P5/P5-Spectral_indices.html#ndvi","text":"De meest gebruikte index is de Normalized Difference Vegatation Index (NDVI) , en wordt berekend als: NDVI = { NIR - RED \\over NIR + RED}. Waarbij: NIR = reflectie in het nabij-infrarode gebied van het spectrum (oftwel Near-Infrared) RED = reflectie in het rode gebied van het spectrum De resulterende index krijgt waarden binnen tussen -1 en 1. Volgens deze formule is de densiteit van vegetatie (NDVI) op een gegeven plaats in het beeld gelijk aan de verschillen in intensiteit van het gereflecteerde licht in het rood en infrarode deel van het spectrum, gedeeld door de soms van deze intensiteiten. Vegetatie absorbeert immers een groot deel van het zichtbare licht ten behoeve van de fotosynthese (dus lage Rood-reflectie), maar weerkaatst vrijwel al het infrarode licht (hoge IR-reflectiewaarde), waardoor de ndvi stijgt. Hoe denser de vegetatie, hoe hoger de ndvi. Andere lichamen, zoals water, observeren IR dan weer beter tot zeer goed, waardoor de ndvi daalt. In Earth Engine kan de NDVI op verschillende manieren berekend worden. We starten met de \u2018meest conventionele\u2019. We starten deze oefening in de Gentse haven. Maal een puntsymbool aan ergens ter hoogte van de Gentse haven in Evergem. Importeer de Sentinel-2 Surface Reflectance (Tier 1) collection en zoek naar het beeld met de laagste wolkbedekking uit 2019 in de periode mei-juni (= de late lente). Bekijk van welke datum het beeld afkomstig is. Visualiseer als een valse kleurencomposiet //1. Importeren van de Sentinel-2 collectie. var S2 = ee . ImageCollection ( \"COPERNICUS/S2_SR\" ); //Filteren op basis van datum (lente 2019) + beeld met laagste wolkenpercentage selecteren var S2_Gent_Lente19 = S2 . filterBounds ( HavenGent ) . filterDate ( '2019-03-20' , '2019-06-30' ) . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ) . first (); print ( 'Gent_Lente19:' , S2_Gent_Lente19 ) //Visualisatieparameters (of handmatig instellen) var S2_ValseKleuren = { gamma : 2 , min : 275 , max : 2088 , bands : [ 'B8' , 'B4' , 'B3' ], }; //Toevoegen aan Map Map . addLayer ( S2_Gent_Lente19 , S2_ValseKleuren , 'Valse Kleuren lente 2019' ) Een eerste methode om een NDVI aan te maken is via de ingebouwde .normalizedDifference() functie. Ga na welke Sentinel-2 banden je nodig hebt om de ndvi te berekenen. (Maak eventueel gebruik van de \u2018Docs\u2019-tab.) //2. Aanmaken NDVI via NormalDifference()-functie. Vul de '?' in var ndvi = S2_Gent_Lente19 . normalizedDifference ([ '?' , '?' ]). rename ( 'NDVI' ); Map . addLayer ( ndvi ,{}, 'ndvi_lente_2019' ) //Zonder visualisatieparametes Een ndvi wordt meestal afgebeeld met een kleurenschema, zoals onderstaand voorbeeld: // Met visualisatie var ndviParams = { min : - 1 , max : 1 , palette : [ 'red' , 'yellow' , 'darkgreen' ]}; Map . addLayer ( ndvi , ndviParams , 'ndvi_2019_vis' ); Oefening: Connecteer de ndvi-waarden met de gepaste landbedekkingsklasse. NDVI waarde Landbedekking (Lente) Negative values (< 0) rocks, bare soil, clouds Small values (0.1 or less) shrubs and meadows Moderate values (0.2 to 0.3) temperate and tropical forests Large values (0.6 to 0.8) (clouds,) water and snow Antwoord NDVI waarde Landbedekking (Lente) Negative values (< 0) clouds, water and snow Small values (0.1 or less) rocks and bare soil Moderate values (0.2 to 0.3) shrubs and meadows Large values (0.6 to 0.8) temperate and tropical forests","title":"NDVI"},{"location":"P5/P5-Spectral_indices.html#band-math-bandbewerkingen","text":"Bandbewerkingen kunnen worden gebruikt om een nieuw beeld aan te maken van de reeds bestaande banden. Het berekenen van indices zoals de NDVI, is al een treffend voorbeeld hiervan. Andere mogelijkheden zijn ratio\u2019s, het verschil van 2 beelden op 2 verschillende tijdstippen om mogelijke veranderingen visueel te benadrukken, \u2026 Er zijn 2 manieren om in Earth Engine een bewerking uit te voeren.","title":"Band Math (bandbewerkingen)"},{"location":"P5/P5-Spectral_indices.html#bewerkingen-via-operatoren","text":"De basisoperators maken gebruik van 2 inputs: ofwel 2 beelden, ofwel 1 beeld en 1 constante. De bewerkingen worden steeds per pixel en per band uitgevoerd. Voorbeeld van operatoren zijn add() , subtract() en divide() . //NDVI berekenen aan de hand van bandwerkingen met operatoren // Lange uitwerking: noodzakelijke banden eerst selecteren en onderbrengen in een nieuwe variabele var nir = S2_Gent_Lente19 . select ( 'B8' ); var red = S2_Gent_Lente19 . select ( 'B4' ); var ndvi2 = nir . subtract ( red ). divide ( nir . add ( red )). rename ( 'NDVI' ); Map . addLayer ( ndvi2 , ndviParams , 'ndvi via operatoren' ); Het resultaat is logischerwijs identiek als de voorgaande ndvi-berekening.","title":"Bewerkingen via operatoren"},{"location":"P5/P5-Spectral_indices.html#bewerkingen-via-expressies","text":"Het spreekt voor zich dat bovenstaande methode voor complex wiskundige bewerkingen niet handig is. Voor dergelijke bewerkingen wordt aangeraden om gebruik te maken van image.expression() , gezien de inputvariabelen hier afzonderlijk worden aangegeven, waardoor de bewerking gemakkelijker wordt weergegeven en het coderen zo vereenvoudigd wordt. De expressie aanvaardt tevens ook constanten. Variabelen die binnen de expressie worden gebruikt, moeten steeds worden aangegeven, zoals in onderstaande NDVI-berekening; //NDVI aan de hand van een expressie var ndvi3 = SS2_Gent_Lente19 . expression ( '(NIR - RED)/(NIR + RED)' , { 'NIR' : S2_Gent_Lente19 . select ( 'B8' ), 'RED' : S2_Gent_Lente19 . select ( 'B4' ) }); Ook hier is het resultaat hetzelfde als de vorige ndvi-berekeningen. Gebruikte operators binnen expressies Onderstaande tabel geeft de binnen de expressies gehanteerde operators weer (bron: Earth Engine guide )","title":"Bewerkingen via expressies"},{"location":"P5/P5-Spectral_indices.html#andere-indices","text":"Naast de NDVI bestaan er nog een heleboel andere indices, elk met een eigen toepassing.","title":"Andere indices"},{"location":"P5/P5-Spectral_indices.html#de-normalized-difference-water-index-ndwi","text":"Er bestaan 2 indices met de naam 'NDWI', beiden gerelateerd aan water: De NDWI ontwikkeld door Gao (1996) , als index voor het watergehalte van vegetatie. De Index is gebaseerd op de NIR (gevoelig voor vegetatie) en SWIR (gevoelig voor water) banden: NDWI = { NIR - SWIR \\over NIR + SWIR}. De NDWI ontwikkeld door McFeeters (1996) , als index voor het verscherpen van verschillen in waterlichamen; NDWI = { GREEN - NIR \\over GREEN + NIR}. Opdracht NDWI Test beide NDWI-indices uit op het Sentinel-beeld van de Gentse Haven en omstreken. Bekijk de verschillen. Kijk hiervoor zeker naar naburige natuurgebieden en waterplassen.","title":"De Normalized Difference Water Index (NDWI)"},{"location":"P5/P5-Spectral_indices.html#opdrachten","text":"","title":"Opdrachten"},{"location":"P5/P5-Spectral_indices.html#ndvi-voor-elk-seizoen","text":"Maak een wolkenvrije beeldencollectie aan (via 30%-'CLOUDY_PIXEL_PERCENTAGE'-filter + een cloudmask toepassen) aan van de regio Durbuy binnen volgende periodes. Gebruik onderstaande Cloudmask-functie: function maskS2clouds ( image ) { var qa = image . select ( 'QA60' ); // Bits 10 and 11 are clouds and cirrus, respectively. var cloudBitMask = 1 << 10 ; var cirrusBitMask = 1 << 11 ; // Both flags should be set to zero, indicating clear conditions. var mask = qa . bitwiseAnd ( cloudBitMask ). eq ( 0 ) . and ( qa . bitwiseAnd ( cirrusBitMask ). eq ( 0 )); return image . updateMask ( mask ); } 2. Maak een functie aan om de NDVI te berekenen. Laat de functie dan los op de Imagecollectie via .map() . Maak aan de hand van de collectie 3 beelden aan met een median() -reducer, binnen volgende periodes: A. Jan-Februari (Winter) B. April-Mei (Lente) C. Juli-Augustus (Zomer) Visualiseer voor elk seizoen een Normale Kleurencomposiet en een NDVI-beeld. Gebruik onderstaande visualisatieparameters bij het plotten: //Visualisatieparameters instellen var NormaleKleuren = { min : 0 , max : 1500 , bands : [ 'B4' , 'B3' , 'B2' ], }; var ndviParams = { min : 0 , max : 1 , bands : [ 'NDVI' ], palette : [ 'red' , 'yellow' , 'darkgreen' ] }; Oplossing Oefening NDVI per seizoen Via deze link: https://code.earthengine.google.com/a1243399a079ca44bff6550e960354ed?noload=true","title":"NDVI voor elk seizoen"},{"location":"P5/P5-Spectral_indices.html#extra-toevoegen-van-een-legende","text":"Om een overzichtelijke legende toe te voegen aan je kaart, kun je onderstaande code gebruiken. Hiermee voeg je een legendepaneel toe voor continue ndvi-data. //------------------------- /* LEGENDE TOEVOEGEN */ //------------------------- // set position of panel var legend = ui . Panel ({ style : { position : 'bottom-left' , padding : '8px 15px' } }); // Create legend title var legendTitle = ui . Label ({ value : 'ndvi' , style : { fontWeight : 'bold' , fontSize : '18px' , margin : '0 0 4px 0' , padding : '0' } }); // Add the title to the panel legend . add ( legendTitle ); // create the legend image var lon = ee . Image . pixelLonLat (). select ( 'latitude' ); var gradient = lon . multiply (( ndviParams . max - ndviParams . min ) / 100.0 ). add ( ndviParams . min ); var legendImage = gradient . visualize ( ndviParams ); // create text on top of legend var panel = ui . Panel ({ widgets : [ ui . Label ( ndviParams [ 'max' ]) ], }); legend . add ( panel ); // create thumbnail from the image var thumbnail = ui . Thumbnail ({ image : legendImage , params : { bbox : '0,0,10,100' , dimensions : '10x200' }, style : { padding : '1px' , position : 'bottom-center' } }); // add the thumbnail to the legend legend . add ( thumbnail ); // create text on top of legend var panel = ui . Panel ({ widgets : [ ui . Label ( ndviParams [ 'min' ]) ], }); legend . add ( panel ); Map . add ( legend );","title":"EXTRA: Toevoegen van een legende"},{"location":"P5/P5-Texture.html","text":"Textuuranalyse Beeldtextuur kan worden gedefinieerd als \u2018de set van metrieken die berekend worden om bepaalde waargenomen textuureigenschappen van het beeld te kwantificeren\u2019 . Het geeft informatie over de spatiale verspreiding van kleuren of intensiteiten binnen het beeld of een bepaalde regio. De spectrale reflectie van wolken en ijs kan bijvoorbeeld zeer gelijkend zijn, maar de textuur zeer verschillend. Textuur toevoegen als een extra input-band kan dus helpen bij het optimaliseren van een beeldclassificatie. Textuur wordt steeds berekend binnen een bepaalde zone, ook wel neighborhood genoemd. In volgende paragraaf, zullen we enkele textuur indices berekenen. We keren hiervoor even terug naar ons satellietbeeld uit Bel\u00e9m, gezien dit beeld geschikt is ter illustratie van textuur. Hier nog even de code om tot het beeld te komen: //Inladen van het gekende Bel\u00e9m Sentinel-2 beeld var S2_Belem = ee . Image ( 'COPERNICUS/S2_SR/20200808T134219_20200808T134214_T22MGD' ) // Zoom in de Map-view in naar het beeld, met Zoom-factor 9 Map . centerObject ( S2_Belem , 9 ); Map . addLayer ( S2_Belem , { min : 0 , max : 3000 , bands : [ 'B4' , 'B3' , 'B2' ]}); Standaard deviatie De Standaard Deviatie (SD) berekent de spreiding van de pixelwaarde-distributie binnen de neighborhood. //1) Standaard deviatie // Compute standard deviation (SD) as texture van B8 var stdev = S2_Belem . select ( 'B8' ). reduceNeighborhood ({ reducer : ee . Reducer . stdDev (), kernel : ee . Kernel . circle ( 7 ), }); Map . addLayer ( stdev , { min : 0 , max : 2000 }, 'SD of NDVI' ); Grey Level Co-occurence matrix Voor het beschrijven van de textuur wordt gebruik gemaakt van de grey level co-occurence matrix (GLCM) van Haralick et al. (1973). Het is een matrix dat weergeeft hoeveel verschillende combinaties van pixelgrijswaarden voorkomen in een bepaalde neighborhood . Aan de hand van de GLCM kunnen vervolgens verschillende textuurattributen berekend worden om de textuur te beschrijven. Entropie Een eerste voorbeeld is de entropie. De entropie van een beeld vertaalt de randomness van de intensiteit naar een index. Het kan worden gezien als een maat voor \u2018pixeldiversiteit\u2019 binnen een bepaalde zone ( neighborhood ). In dit voorbeeld wordt een kernel aangemaakt met radius 7m, waardoor de textuurberekening dus steeds binnen een radius van 7m wordt berekend. Gezien een entropy-berekening enkel kan worden uitgevoerd op discrete waarden, dienen we het pixeltype eerst om te zetten naar een integer. // Compute the gray-level co-occurrence matrix (GLCM), get contrast. var glcm = S2_Belem . select ( 'B8' ). glcmTexture ({ size : 4 }); var contrast = glcm . select ( 'B8_contrast' ); //Mappen van contrast: speel met de min, max waarden (via stretching) Map . addLayer ( contrast , {}, 'contrast' ); var variance = glcm . select ( 'B8_var' ) Map . addLayer ( variance , {}, 'variance' ); var ent = glcm . select ( 'B8_ent' ); Map . addLayer ( ent , {}, 'Entropy' ); GLCM-textuur in Earth Engine Je merkt wellicht dat de textuurafbeelding van verschillende textuurmaten uit de GLCM varieert naarmate je in- en uitzoomt. Dit is ligt aan het feit dat Earth Engine dit steeds per 'view window' uitrekend, waardoor de resultaten zo verschillend lijken. Echter, als je het beeld exporteert uit Earth Engine en bijvoorbeeld in SNAP opent, bekom je wel een visueel begrijpbaar resultaat. De berekende banden kunnen in Earth Engine echter wel verder gebruikt worden voor classificatie. EXTRA: Export-script Hieronder kun je een voorbeeldscriptje vinden om een raster naar je Google Drive te exporteren. Na een 10-tal minuten kun je dit terug vinden op je persoonlijke Google Drive en downloaden naar je desktop. Export . image . toDrive ({ image : entropy , description : 'B8_Entropy' , scale : 10 , //RESOLUTIE folder : 'Sentinel_2_export' , maxPixels : 1e12 //Vergroot de exportcapaciteit });","title":"Textuuranalyse"},{"location":"P5/P5-Texture.html#textuuranalyse","text":"Beeldtextuur kan worden gedefinieerd als \u2018de set van metrieken die berekend worden om bepaalde waargenomen textuureigenschappen van het beeld te kwantificeren\u2019 . Het geeft informatie over de spatiale verspreiding van kleuren of intensiteiten binnen het beeld of een bepaalde regio. De spectrale reflectie van wolken en ijs kan bijvoorbeeld zeer gelijkend zijn, maar de textuur zeer verschillend. Textuur toevoegen als een extra input-band kan dus helpen bij het optimaliseren van een beeldclassificatie. Textuur wordt steeds berekend binnen een bepaalde zone, ook wel neighborhood genoemd. In volgende paragraaf, zullen we enkele textuur indices berekenen. We keren hiervoor even terug naar ons satellietbeeld uit Bel\u00e9m, gezien dit beeld geschikt is ter illustratie van textuur. Hier nog even de code om tot het beeld te komen: //Inladen van het gekende Bel\u00e9m Sentinel-2 beeld var S2_Belem = ee . Image ( 'COPERNICUS/S2_SR/20200808T134219_20200808T134214_T22MGD' ) // Zoom in de Map-view in naar het beeld, met Zoom-factor 9 Map . centerObject ( S2_Belem , 9 ); Map . addLayer ( S2_Belem , { min : 0 , max : 3000 , bands : [ 'B4' , 'B3' , 'B2' ]});","title":"Textuuranalyse"},{"location":"P5/P5-Texture.html#standaard-deviatie","text":"De Standaard Deviatie (SD) berekent de spreiding van de pixelwaarde-distributie binnen de neighborhood. //1) Standaard deviatie // Compute standard deviation (SD) as texture van B8 var stdev = S2_Belem . select ( 'B8' ). reduceNeighborhood ({ reducer : ee . Reducer . stdDev (), kernel : ee . Kernel . circle ( 7 ), }); Map . addLayer ( stdev , { min : 0 , max : 2000 }, 'SD of NDVI' );","title":"Standaard deviatie"},{"location":"P5/P5-Texture.html#grey-level-co-occurence-matrix","text":"Voor het beschrijven van de textuur wordt gebruik gemaakt van de grey level co-occurence matrix (GLCM) van Haralick et al. (1973). Het is een matrix dat weergeeft hoeveel verschillende combinaties van pixelgrijswaarden voorkomen in een bepaalde neighborhood . Aan de hand van de GLCM kunnen vervolgens verschillende textuurattributen berekend worden om de textuur te beschrijven.","title":"Grey Level Co-occurence matrix"},{"location":"P5/P5-Texture.html#entropie","text":"Een eerste voorbeeld is de entropie. De entropie van een beeld vertaalt de randomness van de intensiteit naar een index. Het kan worden gezien als een maat voor \u2018pixeldiversiteit\u2019 binnen een bepaalde zone ( neighborhood ). In dit voorbeeld wordt een kernel aangemaakt met radius 7m, waardoor de textuurberekening dus steeds binnen een radius van 7m wordt berekend. Gezien een entropy-berekening enkel kan worden uitgevoerd op discrete waarden, dienen we het pixeltype eerst om te zetten naar een integer. // Compute the gray-level co-occurrence matrix (GLCM), get contrast. var glcm = S2_Belem . select ( 'B8' ). glcmTexture ({ size : 4 }); var contrast = glcm . select ( 'B8_contrast' ); //Mappen van contrast: speel met de min, max waarden (via stretching) Map . addLayer ( contrast , {}, 'contrast' ); var variance = glcm . select ( 'B8_var' ) Map . addLayer ( variance , {}, 'variance' ); var ent = glcm . select ( 'B8_ent' ); Map . addLayer ( ent , {}, 'Entropy' ); GLCM-textuur in Earth Engine Je merkt wellicht dat de textuurafbeelding van verschillende textuurmaten uit de GLCM varieert naarmate je in- en uitzoomt. Dit is ligt aan het feit dat Earth Engine dit steeds per 'view window' uitrekend, waardoor de resultaten zo verschillend lijken. Echter, als je het beeld exporteert uit Earth Engine en bijvoorbeeld in SNAP opent, bekom je wel een visueel begrijpbaar resultaat. De berekende banden kunnen in Earth Engine echter wel verder gebruikt worden voor classificatie.","title":"Entropie"},{"location":"P5/P5-Texture.html#extra-export-script","text":"Hieronder kun je een voorbeeldscriptje vinden om een raster naar je Google Drive te exporteren. Na een 10-tal minuten kun je dit terug vinden op je persoonlijke Google Drive en downloaden naar je desktop. Export . image . toDrive ({ image : entropy , description : 'B8_Entropy' , scale : 10 , //RESOLUTIE folder : 'Sentinel_2_export' , maxPixels : 1e12 //Vergroot de exportcapaciteit });","title":"EXTRA: Export-script"},{"location":"P6/P6-Improvement.html","text":"In de gesuperviseerde classificaties testten we reeds 3 'classifiers', waarvan de Random Forest classifier leidde tot een iets betere algemene accuraatheid. Desondanks ze al tot acceptabele percentages leidden, bestaan er enkele opties om de classificatie nog te verbeteren. Aanpassen van het aantal trainingssamples en aanpassen sampling -strategie De opgemaakte trainingssamples kunnen worden verbeterd door het aantal samples omhoog te trekken, of door de sampling -strategie aan te passen. In dit voorbeeld tekenden we zelf polygonen in op basis van visuele inspectie, waardoor we enkel de pixels werden aangeduid waar we relatief zeker waren van de desbetreffende klasse. Een betere techniek zou erin bestaan om op voorhand willekeurige locaties door de computer aan te laten duiden, eventueel gestratificeerd per landbedekkingsklasse (' stratified sampling '). Toevoegen van Indices Ook door de input-banden aan te passen of extra banden toe te voegen, kan er voor zorgen dat de onderscheiding van de klassen wordt verbeterd. Zo kan: er hoogtedata (zgn. Digital Elevation Model of DEM) worden toegevoegd, waaruit bv de hellingsgraad van het terrein kan worden berekend. Zo kunnen land features met een specifieke topografie zoals basins, kanalen, toppen, dalen, hellingen gemakkelijker worden ge\u00efdentificeerd. er informatie uit berekende indices helpen voor versterking van verschillen tussen klassen, zoals NDVI, MNDWI of in de context van mangrove-classificatie de MVI. Opdracht Tracht je classificatie te verbeteren door: enkele indices toe aan het Sentinel-2 beeld van vorige oefening, zoals de MVI, NDVI en NDWI. Textuurmatrices toe te voegen. Zie hiervoor P5 - Textuur Gebruik deze keer enkel de Random Forest classifier. Bekijk de bekomen accuraatheid. Merk je verbeteringen op?","title":"Verbeteren van de classificatie"},{"location":"P6/P6-Improvement.html#aanpassen-van-het-aantal-trainingssamples-en-aanpassen-sampling-strategie","text":"De opgemaakte trainingssamples kunnen worden verbeterd door het aantal samples omhoog te trekken, of door de sampling -strategie aan te passen. In dit voorbeeld tekenden we zelf polygonen in op basis van visuele inspectie, waardoor we enkel de pixels werden aangeduid waar we relatief zeker waren van de desbetreffende klasse. Een betere techniek zou erin bestaan om op voorhand willekeurige locaties door de computer aan te laten duiden, eventueel gestratificeerd per landbedekkingsklasse (' stratified sampling ').","title":"Aanpassen van het aantal trainingssamples en aanpassen sampling-strategie"},{"location":"P6/P6-Improvement.html#toevoegen-van-indices","text":"Ook door de input-banden aan te passen of extra banden toe te voegen, kan er voor zorgen dat de onderscheiding van de klassen wordt verbeterd. Zo kan: er hoogtedata (zgn. Digital Elevation Model of DEM) worden toegevoegd, waaruit bv de hellingsgraad van het terrein kan worden berekend. Zo kunnen land features met een specifieke topografie zoals basins, kanalen, toppen, dalen, hellingen gemakkelijker worden ge\u00efdentificeerd. er informatie uit berekende indices helpen voor versterking van verschillen tussen klassen, zoals NDVI, MNDWI of in de context van mangrove-classificatie de MVI.","title":"Toevoegen van Indices"},{"location":"P6/P6-Improvement.html#opdracht","text":"Tracht je classificatie te verbeteren door: enkele indices toe aan het Sentinel-2 beeld van vorige oefening, zoals de MVI, NDVI en NDWI. Textuurmatrices toe te voegen. Zie hiervoor P5 - Textuur Gebruik deze keer enkel de Random Forest classifier. Bekijk de bekomen accuraatheid. Merk je verbeteringen op?","title":"Opdracht"},{"location":"P6/P6-Oefeningen.html","text":"OEF 6.1 - Classificatie van Belgi\u00eb Gegeven In volgende oefening maken we een landclassificatie van Belgi\u00eb op basis van een Landsat-8 beeldcomposiet. Om de grenzen van Belgi\u00eb te bekomen, maken we gebruik van volgende dataset: https://developers.google.com/earth-engine/datasets/catalog/USDOS_LSIB_SIMPLE_2017 Om hieruit Belgi\u00eb te filteren, maak je gebruik van onderstaande code: var countries = ee . FeatureCollection ( 'USDOS/LSIB_SIMPLE/2017' ); var bel = countries . filterMetadata ( 'country_na' , 'equals' , 'Belgium' ); Opdracht Maak \u00e9\u00e9n wolkenvrij Landsat-8 beeld aan voor 2019. Weerhoud enkel de bruikbare banden voor classificatie: voor Landsat-8 dit zijn volgende banden: blauw, groen, rood, nir, swir1, swir2. Clip je resulterende beelden op basis van de Belgische grens. Voer een classificatie uit met volgende klassen: bos, water, grasland, landbouw, urban, bodem . Maak hiervoor je eigen trainingssamples aan. Vergeet ze niet samen te voegen tot \u00e9\u00e9n FeatureCollection . Kies 2 classifiers naar keuze. Visualiseer je resultaat naar eigen keuze. Evalueer het visueel: is je classificatie geslaagd? Welke classifier lijkt beter? Bereken de Overall Accuracy van je classificatie.","title":"Oefeningen"},{"location":"P6/P6-Oefeningen.html#oef-61-classificatie-van-belgie","text":"","title":"OEF 6.1 - Classificatie van Belgi\u00eb"},{"location":"P6/P6-Oefeningen.html#gegeven","text":"In volgende oefening maken we een landclassificatie van Belgi\u00eb op basis van een Landsat-8 beeldcomposiet. Om de grenzen van Belgi\u00eb te bekomen, maken we gebruik van volgende dataset: https://developers.google.com/earth-engine/datasets/catalog/USDOS_LSIB_SIMPLE_2017 Om hieruit Belgi\u00eb te filteren, maak je gebruik van onderstaande code: var countries = ee . FeatureCollection ( 'USDOS/LSIB_SIMPLE/2017' ); var bel = countries . filterMetadata ( 'country_na' , 'equals' , 'Belgium' );","title":"Gegeven"},{"location":"P6/P6-Oefeningen.html#opdracht","text":"Maak \u00e9\u00e9n wolkenvrij Landsat-8 beeld aan voor 2019. Weerhoud enkel de bruikbare banden voor classificatie: voor Landsat-8 dit zijn volgende banden: blauw, groen, rood, nir, swir1, swir2. Clip je resulterende beelden op basis van de Belgische grens. Voer een classificatie uit met volgende klassen: bos, water, grasland, landbouw, urban, bodem . Maak hiervoor je eigen trainingssamples aan. Vergeet ze niet samen te voegen tot \u00e9\u00e9n FeatureCollection . Kies 2 classifiers naar keuze. Visualiseer je resultaat naar eigen keuze. Evalueer het visueel: is je classificatie geslaagd? Welke classifier lijkt beter? Bereken de Overall Accuracy van je classificatie.","title":"Opdracht"},{"location":"P6/P6-SpectralResponse.html","text":"Als laatste onderdeel van dit practicum, kijken we even naar de spectrale signaturen van onze klassen. Uit deze curves kunnen we volgende zaken afleiden (zie ook Practicum 1): Welke banden/indices zorgen voor onderscheid tussen klassen? Welk spectrale curves heeft elk van onze klassen? Kunnen we deze curves ook verklaren? Toevoegen van Spectrale Responsecurve aan de classificatie Werk verder met het script van de gesuperviseerde classificatie. Om zoveel mogelijk aan rekenkracht te besparen, dan verwijder of maskeer je best alle 'prints'. Maak eventueel een nieuw scriptje aan waar je dan alle prints verwijdert. Nu kunnen we een Chart-variabele aanmaken die de gemiddelde waarden per band per klasse van de classificatie zal plotten. Voeg hiervoor eerst de classificatie toe aan het gebruikte Sentinel-2 beeld. We gebruiken hier de Random Forest classificatie: // classificatie toevoegen aan Image var S2_im = S2_im . addBands ( classified_RF ) // Aanmaken van de Chart var spectraChart = ui . Chart . image . byClass ({ image : S2_im , classBand : 'classification' , reducer : ee . Reducer . mean (), scale : 30 , classLabels : [ '' , 'Mangrove' , 'other forest' , 'water' , 'Crop/Grass' , 'Urban' , 'Soil' ] //Klasse '0' is leeg }). setChartType ( 'ScatterChart' ) // Extra opties om de plot mooi te maken var plotOptions = { title : 'Sentinel-2 surface reflectance spectra' , hAxis : { title : 'Wavelength (nanometers)' }, vAxis : { title : 'Reflectance' }, lineWidth : 1 , pointSize : 4 , series : { 0 : { color : 'red' }, // Mangrove 1 : { color : 'green' }, // Forest 2 : { color : 'blue' }, // water 3 : { color : 'yellow' }, //crops/grass 4 : { color : 'black' }, //Urban 5 : { color : 'brown' } //soil }}; print ( spectraChart . setOptions ( plotOptions ))","title":"P6 SpectralResponse"},{"location":"P6/P6-SpectralResponse.html#toevoegen-van-spectrale-responsecurve-aan-de-classificatie","text":"Werk verder met het script van de gesuperviseerde classificatie. Om zoveel mogelijk aan rekenkracht te besparen, dan verwijder of maskeer je best alle 'prints'. Maak eventueel een nieuw scriptje aan waar je dan alle prints verwijdert. Nu kunnen we een Chart-variabele aanmaken die de gemiddelde waarden per band per klasse van de classificatie zal plotten. Voeg hiervoor eerst de classificatie toe aan het gebruikte Sentinel-2 beeld. We gebruiken hier de Random Forest classificatie: // classificatie toevoegen aan Image var S2_im = S2_im . addBands ( classified_RF ) // Aanmaken van de Chart var spectraChart = ui . Chart . image . byClass ({ image : S2_im , classBand : 'classification' , reducer : ee . Reducer . mean (), scale : 30 , classLabels : [ '' , 'Mangrove' , 'other forest' , 'water' , 'Crop/Grass' , 'Urban' , 'Soil' ] //Klasse '0' is leeg }). setChartType ( 'ScatterChart' ) // Extra opties om de plot mooi te maken var plotOptions = { title : 'Sentinel-2 surface reflectance spectra' , hAxis : { title : 'Wavelength (nanometers)' }, vAxis : { title : 'Reflectance' }, lineWidth : 1 , pointSize : 4 , series : { 0 : { color : 'red' }, // Mangrove 1 : { color : 'green' }, // Forest 2 : { color : 'blue' }, // water 3 : { color : 'yellow' }, //crops/grass 4 : { color : 'black' }, //Urban 5 : { color : 'brown' } //soil }}; print ( spectraChart . setOptions ( plotOptions ))","title":"Toevoegen van Spectrale Responsecurve aan de classificatie"},{"location":"P6/P6-SpectralResponse_oud.html","text":"Als laatste onderdeel van dit practicum, kijken we even naar de spectrale signaturen van onze klassen. Uit deze curves kunnen we volgende zaken afleiden (zie ook Practicum 1): Welke banden/indices zorgen voor onderscheid tussen klassen? Welk spectrale curves heeft elk van onze klassen? Kunnen we deze curves ook verklaren? Aanmaken voorbeeldFeatures Om spectrale curves te maken, hebben we eveneens voorbeeld-samples nodig van elk van de klassen. Dit kan door opnieuw nieuwe features in te tekenen (bv \u00e9\u00e9n punt voor 1 klasse). We doen dit bijvoorbeeld voor de klassen Mangrove, OtherForest, Water en Urban. Zoek \u00e9\u00e9n representatief punt voor elk van de klassen. Maak deze aan als het type Feature en geef het een property 'label', met een bijpassende naam Eenmaal de VoorbeeldFeatures zijn aangemaakt kun je ze samenvoegen in het script: var vbPol = ee . FeatureCollection ([ Mangrove , OtherForest , Water , Urban ]); S2-beeld aanmaken Maak ook opnieuw het Sentinel-2 beeld uit vorige oefening aan, met de indices: // -------------------------------------------------------------------- // STAP 1 - Inladen en klaarzetten van S2-beeld. M\u00e9t extra cloud-masking // ------------------------------------------------------------------- //Cloudprobability functie: // Functie die nieuwe CloudProbability collectie samenvoegt met S2 (sen2cloudless) // meer info: https://medium.com/sentinel-hub/cloud-masks-at-your-service-6e5b2cb2ce8a var getS2_SR_CLOUD_PROBABILITY = function () { var innerJoined = ee . Join . inner (). apply ({ primary : ee . ImageCollection ( \"COPERNICUS/S2_SR\" ), secondary : ee . ImageCollection ( \"COPERNICUS/S2_CLOUD_PROBABILITY\" ), condition : ee . Filter . equals ({ leftField : 'system:index' , rightField : 'system:index' }) }); var mergeImageBands = function ( joinResult ) { return ee . Image ( joinResult . get ( 'primary' )) . addBands ( joinResult . get ( 'secondary' )); }; var newCollection = innerJoined . map ( mergeImageBands ); return ee . ImageCollection ( newCollection ); }; // Mask out clouds var maskClouds = function ( image ) { var cloudProbabilityThreshold = 40 ; var cloudMask = image . select ( 'probability' ). lt ( cloudProbabilityThreshold ); return image . updateMask ( cloudMask ); }; //Aanmaken van een ImageCollection ter hoogte van Mangroves Paramaribo, Suriname var S2_coll = getS2_SR_CLOUD_PROBABILITY () . filterDate ( '2019-08-01' , '2019-10-30' ) // Filteren voor het jaar 2020, droge tijd . filterMetadata ( 'CLOUDY_PIXEL_PERCENTAGE' , 'less_than' , 50 ) //Voorselectie obv wolken . map ( maskClouds ) //toepassen van de cloudmaskfunctie . filterBounds ( ROI ); //collectie filteren obv de Kustzonegeometrie // ----------------------------------------------------------------------------- // TOEVOEGEN INDICES // ----------------------------------------------------------------------------- var addIndices = function ( image ) { var mvi = image . expression ( '(B8-B3)/(B11-B3)' , { 'B8' : image . select ( 'B8' ), 'B3' : image . select ( 'B3' ), 'B11' : image . select ( 'B11' ) }). float (). rename ( 'MVI' ); var ndvi = image . normalizedDifference ([ 'B8' , 'B4' ]). rename ( 'NDVI' ); var ndwi = image . normalizedDifference ([ 'B3' , 'B12' ]). rename ( 'NDWI' ); return image . addBands ( mvi ). addBands ( ndvi ). addBands ( ndwi ); }; //Toepassen indices + medianreducer + clippen var S2_im = S2_coll . map ( addIndices ). median (). clip ( Paramaribo ); Cloud mask methode In voorgaande code wordt opnieuw gebruik gemaakt van de extra S2-cloudmask methode. Je kunt evengoed gebruik maken van de andere strategi\u00ebn, zoals gezien in \"Cloud Masking\" van Practicum 4. Spectrale responsiecurve aanmaken Vervolgens kunnen we de Chart aanmaken. Tevens linken we de overeenkomstige golflengtes aan de banden, om zo een spectrum te krijgen met de golflengte in de X-as. // Golflengtes S2 //De overeenkomstige golflengte per band aangeven (zie bandenverdeling Sentinel-2). var wavelengths = [ 443.9 , 496.6 , 559 , 664.5 , 703.9 , 740.2 , 782.5 , 835.1 , 864.8 , 945 , 1613.7 , 2202.4 ] //Aanmaken Chart var Chart = ui . Chart . image . regions ({ image : S2_im . select ( bands ), regions : vbPol , reducer : ee . Reducer . mean (), scale : 10 , seriesProperty : 'label' , xLabels : wavelengths }) var plotOptions = { title : 'Sentinel-2 surface reflectance spectra' , hAxis : { title : 'Wavelength (nanometers)' }, vAxis : { title : 'Reflectance' }, lineWidth : 1 , pointSize : 4 , series : { 0 : { color : 'red' }, // Mangrove 1 : { color : 'green' }, // Forest 2 : { color : 'blue' }, // water 3 : { color : 'black' }, //crops/grass }}; print ( Chart . setOptions ( plotOptions ));","title":"Spectrale responsiecurves"},{"location":"P6/P6-SpectralResponse_oud.html#aanmaken-voorbeeldfeatures","text":"Om spectrale curves te maken, hebben we eveneens voorbeeld-samples nodig van elk van de klassen. Dit kan door opnieuw nieuwe features in te tekenen (bv \u00e9\u00e9n punt voor 1 klasse). We doen dit bijvoorbeeld voor de klassen Mangrove, OtherForest, Water en Urban. Zoek \u00e9\u00e9n representatief punt voor elk van de klassen. Maak deze aan als het type Feature en geef het een property 'label', met een bijpassende naam Eenmaal de VoorbeeldFeatures zijn aangemaakt kun je ze samenvoegen in het script: var vbPol = ee . FeatureCollection ([ Mangrove , OtherForest , Water , Urban ]);","title":"Aanmaken voorbeeldFeatures"},{"location":"P6/P6-SpectralResponse_oud.html#s2-beeld-aanmaken","text":"Maak ook opnieuw het Sentinel-2 beeld uit vorige oefening aan, met de indices: // -------------------------------------------------------------------- // STAP 1 - Inladen en klaarzetten van S2-beeld. M\u00e9t extra cloud-masking // ------------------------------------------------------------------- //Cloudprobability functie: // Functie die nieuwe CloudProbability collectie samenvoegt met S2 (sen2cloudless) // meer info: https://medium.com/sentinel-hub/cloud-masks-at-your-service-6e5b2cb2ce8a var getS2_SR_CLOUD_PROBABILITY = function () { var innerJoined = ee . Join . inner (). apply ({ primary : ee . ImageCollection ( \"COPERNICUS/S2_SR\" ), secondary : ee . ImageCollection ( \"COPERNICUS/S2_CLOUD_PROBABILITY\" ), condition : ee . Filter . equals ({ leftField : 'system:index' , rightField : 'system:index' }) }); var mergeImageBands = function ( joinResult ) { return ee . Image ( joinResult . get ( 'primary' )) . addBands ( joinResult . get ( 'secondary' )); }; var newCollection = innerJoined . map ( mergeImageBands ); return ee . ImageCollection ( newCollection ); }; // Mask out clouds var maskClouds = function ( image ) { var cloudProbabilityThreshold = 40 ; var cloudMask = image . select ( 'probability' ). lt ( cloudProbabilityThreshold ); return image . updateMask ( cloudMask ); }; //Aanmaken van een ImageCollection ter hoogte van Mangroves Paramaribo, Suriname var S2_coll = getS2_SR_CLOUD_PROBABILITY () . filterDate ( '2019-08-01' , '2019-10-30' ) // Filteren voor het jaar 2020, droge tijd . filterMetadata ( 'CLOUDY_PIXEL_PERCENTAGE' , 'less_than' , 50 ) //Voorselectie obv wolken . map ( maskClouds ) //toepassen van de cloudmaskfunctie . filterBounds ( ROI ); //collectie filteren obv de Kustzonegeometrie // ----------------------------------------------------------------------------- // TOEVOEGEN INDICES // ----------------------------------------------------------------------------- var addIndices = function ( image ) { var mvi = image . expression ( '(B8-B3)/(B11-B3)' , { 'B8' : image . select ( 'B8' ), 'B3' : image . select ( 'B3' ), 'B11' : image . select ( 'B11' ) }). float (). rename ( 'MVI' ); var ndvi = image . normalizedDifference ([ 'B8' , 'B4' ]). rename ( 'NDVI' ); var ndwi = image . normalizedDifference ([ 'B3' , 'B12' ]). rename ( 'NDWI' ); return image . addBands ( mvi ). addBands ( ndvi ). addBands ( ndwi ); }; //Toepassen indices + medianreducer + clippen var S2_im = S2_coll . map ( addIndices ). median (). clip ( Paramaribo ); Cloud mask methode In voorgaande code wordt opnieuw gebruik gemaakt van de extra S2-cloudmask methode. Je kunt evengoed gebruik maken van de andere strategi\u00ebn, zoals gezien in \"Cloud Masking\" van Practicum 4.","title":"S2-beeld aanmaken"},{"location":"P6/P6-SpectralResponse_oud.html#spectrale-responsiecurve-aanmaken","text":"Vervolgens kunnen we de Chart aanmaken. Tevens linken we de overeenkomstige golflengtes aan de banden, om zo een spectrum te krijgen met de golflengte in de X-as. // Golflengtes S2 //De overeenkomstige golflengte per band aangeven (zie bandenverdeling Sentinel-2). var wavelengths = [ 443.9 , 496.6 , 559 , 664.5 , 703.9 , 740.2 , 782.5 , 835.1 , 864.8 , 945 , 1613.7 , 2202.4 ] //Aanmaken Chart var Chart = ui . Chart . image . regions ({ image : S2_im . select ( bands ), regions : vbPol , reducer : ee . Reducer . mean (), scale : 10 , seriesProperty : 'label' , xLabels : wavelengths }) var plotOptions = { title : 'Sentinel-2 surface reflectance spectra' , hAxis : { title : 'Wavelength (nanometers)' }, vAxis : { title : 'Reflectance' }, lineWidth : 1 , pointSize : 4 , series : { 0 : { color : 'red' }, // Mangrove 1 : { color : 'green' }, // Forest 2 : { color : 'blue' }, // water 3 : { color : 'black' }, //crops/grass }}; print ( Chart . setOptions ( plotOptions ));","title":"Spectrale responsiecurve aanmaken"},{"location":"P6/P6-Supervised.html","text":"Intro Gesuperviseerde classificatie is gebaseerd op een door de gebruiker opgestelde trainingdataset. Deze trainingsdata zijn representatief voor de specifieke gewenste klassen. Het gesuperviseerde algoritme gebruikt dan deze trainingsdata als referentie, om onbekende pixels te classificeren. Een belangrijk element hierbij is dus voorkennis van het desbetreffende gebied. De trainingsamples die worden opgesteld worden ook wel ground truth data genoemd en komen overeen met gekende landbekking op locaties. Er zijn verschillende opties om aan gronddata te komen: Door middel van veldbezoek, waarbij GPS-data ter plaatse worden verzameld. Deze methode zorgt voor de meest kwalitatieve data en zekerheid, maar is vanzelfsprekend duur en arbeidintensief om uit te voeren. Door gebruik van referentiekaarten of oudere beschikbaar kaartmateriaal of beeldmateriaal van hogere resolutie , bijvoorbeeld beelden bekomen gedurende een drone-campagne. Door manuele interpretatie van de satellietbeelden , waarbij de gebruiker beschikt over enige expertkennis en voorgaande ervaring. Een ander aspect is dat de trainingdata zo dicht mogelijk opgenomen wordt bij het tijdstip van opname van het gebruike remote sensing beeld. Landbedekking kan immers snel veranderen: stadsontwikkeling, ontbossing, seizoenale impact, agrarische veranderingen, kusterosie, ... In deze oefening van gesuperviseerde classificatie zul je zelf trainingsamples moeten aanmaken, want er zijn geen GPS-punten beschikbaar. Wel beschik je over een beknopt verslag van een veldcampagne, dat je een idee kan geven van de aanwezige landbekkingsklassen en aangezien we ons beperken tot enkele brede klassen, zul je tevens gemakkelijk visueel trainingsamples kunnen aanmaken. Hiervoor is het aanmaken en analyseren van verschillende composieten aangewezen. Beeldcomposieten aanmaken Start opnieuw met het aanmaken van een wolkenvrije dataset, dewelke kan overgenomen worden van vorige oefening. Belangrijk hier is ook om de gewenste banden te selecteren, die we tijdens de classificatie gaan aanmaken. // -------------------------------------------------------------------- // STAP 1 - Inladen en klaarzetten van S2-beeld. M\u00e9t extra cloud-masking // ------------------------------------------------------------------- //Cloudprobability functie: // Functie die nieuwe CloudProbability collectie samenvoegt met S2 (sen2cloudless) // meer info: https://medium.com/sentinel-hub/cloud-masks-at-your-service-6e5b2cb2ce8a var getS2_SR_CLOUD_PROBABILITY = function () { var innerJoined = ee . Join . inner (). apply ({ primary : ee . ImageCollection ( \"COPERNICUS/S2_SR\" ), secondary : ee . ImageCollection ( \"COPERNICUS/S2_CLOUD_PROBABILITY\" ), condition : ee . Filter . equals ({ leftField : 'system:index' , rightField : 'system:index' }) }); var mergeImageBands = function ( joinResult ) { return ee . Image ( joinResult . get ( 'primary' )) . addBands ( joinResult . get ( 'secondary' )); }; var newCollection = innerJoined . map ( mergeImageBands ); return ee . ImageCollection ( newCollection ); }; // Mask out clouds var maskClouds = function ( image ) { var cloudProbabilityThreshold = 40 ; var cloudMask = image . select ( 'probability' ). lt ( cloudProbabilityThreshold ); return image . updateMask ( cloudMask ); }; //Aanmaken van een ImageCollection ter hoogte van Mangroves Paramaribo, Suriname var S2_coll = getS2_SR_CLOUD_PROBABILITY () . filterDate ( '2019-08-01' , '2019-10-30' ) // Filteren voor het jaar 2020, droge tijd . filterMetadata ( 'CLOUDY_PIXEL_PERCENTAGE' , 'less_than' , 50 ) //Voorselectie obv wolken . map ( maskClouds ) //toepassen van de cloudmaskfunctie . filterBounds ( ROI ); //collectie filteren obv de Kustzonegeometrie print ( S2_coll ) //Omzetten collectie naar een Image, door .median() te nemen. Hierna clippen we ook tot onze ROI //Ook selecteren we de banden waarmee we verder willen werken var bands = [ 'B2' , 'B3' , 'B4' , 'B5' , 'B6' , 'B7' , 'B8' , 'B8A' , 'B11' , 'B12' ]; var S2_im = S2_coll . median () . select ( bands ) . clip ( ROI ) //Bekijk de .clip-eigenschappen in de Docs Map . centerObject ( S2_im , 11 ) Map . addLayer ( S2_im ,{ min : 50 , max : 1800 , bands : 'B4,B3,B2' }, 'NormaleKleuren_2020' , 0 ) Map . addLayer ( S2_im ,{ min : 700 , max : 4500 , bands : 'B8,B4,B3' }, 'ValseKleuren_2020' , 0 ) Map . addLayer ( S2_im ,{ min : 500 , max : 4000 , bands : 'B8,B11,B2' }, 'Healthy_Vegetation_2020' , 0 ) Trainingsamples aanmaken Nadat het wolkenvrije Sentinel-2 beeld is ingeladen, kunnen we deze gebruiken om enkele representatieve samples te verzamelen van enkele landbedekkingklassen waar we in ge\u00efnteresseerd zijn. Er zijn 2 manieren om trainingsdata in Earth Engine op te laden: Door ze in te tekenen als polygonen binnen per klasse, zoals we in komend voorbeeld zullen toepassen. Door eerder ingetekende trainingssamples of GPS-punten op te laden als een 'Asset'. Dit kunnen shapefiles , of .csv -bestanden zijn. Hover met je muis over de 'Geometry Imports box, dat zich naast de geometrietools bevindt. Klik op *'+new layer'. Elke gewenste landbedekkingsklasse dient als een afzonderlijke laag te worden aangemaakt. Laat ons bijvoorbeeld starten met de eenvoudigste klasse 'water'. Zoom in op het beeld en teken polygonen in over oppervlaktes waar je zeker van bent dat het waterlichamen betreft. Het is goed hierin te vari\u00ebren binnen verschillende types van zowel zee als rivieren en andere waterlichamen. Teken ca. 10 polygonen in per klasse. Neem hiervoor zeker je tijd, gezien het belangrijk is dit zeer precies te doen. De kwaliteit van de inputdata bepaalt tevens de kwaliteit van de classificatie. Garbage in = Garbage out . Als je een fout gemaakt heb, kun je even op 'exit' duwen en de laats ingetekende polygoon verwijderen. class Landbekkingsklasse 1 Mangrove 2 OtherForest 3 Water 4 Crop 5 Urban 6 BareSoil Polygonen vs puntdata als traingdata Trainingdata kan bestaan uit puntdata of trainingdata. Beiden hebben hun voor- en nadeel. Puntdata gebruiken als inputdata kan bijvoorbeeld als je beschikt over een grote set GPS-veldpunten van locaties waar je exact weet tot welke landbedekkingsklasse een pixel hoort. Deze pixel wordt dan als referentie aanschouwt. Deze zekerheid van de trainingsdata is dus groot. Een nadeel bij pixels is dat het minder de vari\u00ebteit van de pixels binnen de klasse opneemt en de totale set aan trainingspixels beperkt blijft. Polygonen worden gebruikt om gebieden in te tekenen voor een bepaalde klasse. In earth engine wordt elke pixel binnen deze polygoon dan gebruikt als inputdata. Dit zorgt ervoor dat de trainingsset groter wordt en de variatie binnen een bepaalde klasse beter wordt omvat. Een nadeel is echter dat zo ook foute pixels kunnen worden meegenomen door het onzorgvuldig intekenen van de polygoon. Eenmaal je klaar bent met het intekenen van een klasse, kun je deze import configureren. Klink hiervoor op het tandwieltje naast de klasse. Geef het een gepaste naam. Daarna verander je de 'Import as' van geometry naar type FeatureCollection . Voeg daarna een property toe met de naam 'class', door te klikken op 'Add property' . De eerste klasse geef je waarde 1, de 2e 2, .... Zorg er wel voor dat je goed weet welke waarde je aan welke klasse geeft. Neem hierbij eventueel de class -numering over van bovenstaande tabel. Herhaal dit voor elke klasse. Uiteindelijk verschijnt elke klasse als een ' FeatureCollection ' bij de imports-lijst in je script: Voorbeeld trainingsamples voor Mangrove Nu de 5 klassen aangemaakt zijn, dienen we ze samen te vatten als een complete trainingsset in earth engine; een gezamenlijke FeatureCollection , waarbij de 'class'- property wordt overgenomen. //2. Trainingssamples samenvoegen tot 1 ```FeatureCollection``` var classNames = Mangrove . merge ( OtherForest ). merge ( Water ). merge ( Crop ). merge ( Urban ). merge ( BareSoil ); print ( classNames ) De trainingsdata aanmaken Nu hebben we reeds een FeatureCollection met trainingspolygonen, maar deze zeggen nog niks over de trainingspixels. In een volgende stap, extraheren we de trainingspixels per band uit het Sentinel-2 beeld op basis van de aangemaakte polygonen. Dit doen we met de sampleRegions() functie. Deze functie extraheert alle pixels binnen opgegeven polygonen, en schrijft elke pixel afzonderlijk naar een nieuw `Feature binnen een FeatureCollection , dewelke ook de class-property meekrijgen. Afhankelijk van de grootte van de polygonen, kan deze dataset dus zeer lijvig worden. //Trainingspixels extraheren naar featurecollection = trainingsdata var traindata = S2_im . sampleRegions ({ collection : classNames , //De trainingspolygonen properties : [ 'class' ], //Dit neemt de gewenste eigenschappen van de collection over scale : 10 }); print ( 'Aantal trainingspixels: ' , traindata . size ()); //print(traindata) //Niet doen print ( traindata . first ()) //Eerste waarde bekijken, De classifier trainen In een volgende stap maken we een classificatiemodel aan en trainen we deze op basis van de traindata. Er bestaan verschillende mogelijke classifiers en 'machine learning'-algoritmen. In Google Earth Engine zitten deze beschikbaar in de ee.Classifier -groep. We gebruiken er 3, waarna we kijken dewelke tot de meest accurate classificatie leidt o.b.v. de validatiedata. Minimum Distance Classifier In een eerste instantie dienen we de classifier te trainen, op basis van de opgestelde trainingsdata. We dienen ook aan te geven welke van de properties binnen de trainingssamples de 'class'-bevat, en welke properties gebruikt moeten worden om mee te classificeren (de banden). //4. De classifiers trainen en toepassen // A. Minimum Distance classifier (gebruik van default-waarde 'euclidische afstand') var MinDist = ee . Classifier . minimumDistance (). train ({ features : traindata , classProperty : 'class' , inputProperties : bands //verwijzing naar de eerder aangemaakte bands-lijst }); CART classifier // B. CART classifier var Cart = ee . Classifier . smileCart (). train ({ features : traindata , classProperty : 'class' , inputProperties : bands //verwijzing naar de eerder aangemaakte bands-lijst }); Random Forest classifier // C. Random Forest var RandomForest = ee . Classifier . smileRandomForest ({ numberOfTrees : 60 }). train ({ features : traindata , classProperty : 'class' , inputProperties : bands //verwijzing naar de eerder aangemaakte bands-lijst }); Beeld classificeren en visualiseren Eenmaal de classifier(s) opgesteld zijn, kunnen ze worden toegepast op het volledige S2-beeld. Elke pixel wordt dus toegekend tot een klasse, op basis van de kennis opgedaan uit de trainingsdata. // 5. Classifiers toepassen //MinimumDistance var classified_MD = S2_im . classify ( MinDist ) var classified_CART = S2_im . classify ( Cart ) var classified_RF = S2_im . classify ( RandomForest ) Bij het visualiseren willen we een visueel overzichtelijk resultaat krijgen. Aangezien we eindigen met discrete klassen, stellen we hiervoor een palette op, dat per klasse een kleur aangeeft. var palette = [ 'FF0000' , // mangrove (1) // rood '7CFC00' , // ander bos (2) // lichtgroen '1E90FF' , //water (3) // blauw 'FFFD10' , //crop (4) //geel '000000' , //stad // zwart '876829' , //BareSoil // bruin ]; var classvis = { min : 1 , max : 6 , palette : palette } Map . addLayer ( classified_MD , classvis , 'MinimumDistance' ) Map . addLayer ( classified_CART , classvis , 'CART' ) Map . addLayer ( classified_RF , classvis , 'RandomForest' ) Accuracy assessment In de accuracy assessment gaan we de accuraatheid van het model nagaan. Dit doen we o.b.v. een error matrix . Om deze op te stellen hebben we nood aan testdata. Training- Validation en Testdata Voordat we onze data gebruiken om het model te trainen, splitsen we het in trainings en validatiedata. Daarnaast wordt er vak nog gebruik gemaakt van een derde, volledig afzonderlijke dataset: de testdataset . In veel gevallen, zoals in komend voorbeeld, wordt de validatieset ook gebruikt als testdataset, maar dit is statistisch gezien niet de beste praktijk. Traindata = de data gebruikt om het model te trainen en dus te fitten . Het model bekijkt en leert van deze data. Validatiedata = Het deel van de data dat gebruikt zal worden om na te gaan hoe goed het model werkt op onbekende data. Dit deel zal dus niet gebruikt worden om het model te trainen. Hierdoor kunnen verschillende classificatiemodellen en parameters binnen het model tegenover elkaar worden afgewogen en het model zo worden geperfectioneerd. Dit wordt ook wel parameter tuning genoemd. Validatiedata wordt dus gebruikt tijdens de ontwikkeling en het zoeken van het beste model. Aangezien deze data van dezelfde dataset komt treedt er spatiale autocorrelatie op, waarbij de validatiepixels veelal buurpixels zijn van trainpixels. Testdata = Deze afzonderlijke dataset wordt gebruikt om bij een finaal model accuraatheidsmaten van de bekomen classificatie te berekenen. Testdatasets worden meestal ook zeer goed verzorgd en zijn goed verzamelde (veld)datapunten. De spatiale autocorrelatie vervalt hier. In voorliggend voorbeeld maken we gebruik van een extra testdataset. Gezien we geen optimalisatie van parameters gaan doorvoeren, maken we geen gebruik van validatiedata en wordt de volledige trainingscollectie trainingsdata. Dit doen we aan de hand van onze validatie dataset als een externe testset. Deze set bestaat uit kleine polygonen met een diameter van 25m en representeren GPS-punten genomen op veldbezoek. De 25m-buffer werd genomen om voldoende testpixels te weerhouden voor de accuracy assessment. Je kunt de shape-file hier downloaden: P6_testdata (shapefile) Om deze toe te voegen aan Earth Engine, laad je deze op via de 'Asset'-tab. Daarna kun je het importeren als een FeatureCollection in je script. Noem dit 'Testdata_pol'. Bekijk ook even deze polygonen. Onder welke property zitten de klassen hier opgeslagen? Vervolgens extraheren we de pixelwaarden op basis van deze testpolygonen, net zoals we dit gedaan hebben bij de trainingspixels: //Na inlezen van validatiedata: maak een testdatacollectie, zoals bij het opmaken van traindata print ( 'Testpolygonen' , Testdata_pol ) //bekijk de properties //Testpixels extraheren naar featurecollection var testdata = S2_im . sampleRegions ({ collection : Testdata_pol , //De trainingspolygonen properties : [ 'val' ], //Dit neemt de gewenste eigenschappen van de collection over scale : 10 }); print ( 'Aantal testpixels: ' , traindata . size ()); Nu we de testpixelwaarden ge\u00ebxtraheerd hebben, kunnen we deze vergelijken met de geclassificeerde pixels in een error matrix . In onderstaand stukje code staat het voorbeeld voor de Minimum Distance classifier. Pas dit toe voor alle classifiers. Welke classifier heeft de grootste algemene accuraatheid ( Overall accuracy )? // Accuracy assessment uitvoeren per classifier // Validatie met de validatiedata var val_MinDist = testdata . classify ( MinDist ); var ErrorMatrix_MinDist = val_MinDist . errorMatrix ( 'val' , 'classification' ) print ( 'Validation error matrix: ' , ErrorMatrix_MinDist ); print ( 'Validation overall accuracy: ' , ErrorMatrix_MinDist . accuracy ()); Exporteren van de Error Matrix In Google Earth Engine is de weergave van de error matrix niet zo handig. Om verdere accuraatheidsmaten uit te rekenen en een betere interpretatie te kunnen uitvoeren kan het handig zijn om de error matrix te exporteren als een .csv-bestand, dewelke in andere software (zoals excel) geopend kan worden. Met de Export.table.toDrive() -functie kunnen we de matrix exporteren naar onze Google Drive. Hiervoor dienen we dit eerst om te zetten naar een feature . //Omzetten naar een Feature var ErrorMatrix_MinDist = ee . Feature ( null , { matrix : ErrorMatrix_MinDist . array ()}); //Exporteren van de errormatrix Export . table . toDrive ({ collection : ee . FeatureCollection ( ErrorMatrix_MinDist ), description : 'P6_Errormatrix' , fileFormat : 'CSV' , folder : 'TELEDETECTIE_2020' });","title":"Gesuperviseerde classificatie"},{"location":"P6/P6-Supervised.html#intro","text":"Gesuperviseerde classificatie is gebaseerd op een door de gebruiker opgestelde trainingdataset. Deze trainingsdata zijn representatief voor de specifieke gewenste klassen. Het gesuperviseerde algoritme gebruikt dan deze trainingsdata als referentie, om onbekende pixels te classificeren. Een belangrijk element hierbij is dus voorkennis van het desbetreffende gebied. De trainingsamples die worden opgesteld worden ook wel ground truth data genoemd en komen overeen met gekende landbekking op locaties. Er zijn verschillende opties om aan gronddata te komen: Door middel van veldbezoek, waarbij GPS-data ter plaatse worden verzameld. Deze methode zorgt voor de meest kwalitatieve data en zekerheid, maar is vanzelfsprekend duur en arbeidintensief om uit te voeren. Door gebruik van referentiekaarten of oudere beschikbaar kaartmateriaal of beeldmateriaal van hogere resolutie , bijvoorbeeld beelden bekomen gedurende een drone-campagne. Door manuele interpretatie van de satellietbeelden , waarbij de gebruiker beschikt over enige expertkennis en voorgaande ervaring. Een ander aspect is dat de trainingdata zo dicht mogelijk opgenomen wordt bij het tijdstip van opname van het gebruike remote sensing beeld. Landbedekking kan immers snel veranderen: stadsontwikkeling, ontbossing, seizoenale impact, agrarische veranderingen, kusterosie, ... In deze oefening van gesuperviseerde classificatie zul je zelf trainingsamples moeten aanmaken, want er zijn geen GPS-punten beschikbaar. Wel beschik je over een beknopt verslag van een veldcampagne, dat je een idee kan geven van de aanwezige landbekkingsklassen en aangezien we ons beperken tot enkele brede klassen, zul je tevens gemakkelijk visueel trainingsamples kunnen aanmaken. Hiervoor is het aanmaken en analyseren van verschillende composieten aangewezen.","title":"Intro"},{"location":"P6/P6-Supervised.html#beeldcomposieten-aanmaken","text":"Start opnieuw met het aanmaken van een wolkenvrije dataset, dewelke kan overgenomen worden van vorige oefening. Belangrijk hier is ook om de gewenste banden te selecteren, die we tijdens de classificatie gaan aanmaken. // -------------------------------------------------------------------- // STAP 1 - Inladen en klaarzetten van S2-beeld. M\u00e9t extra cloud-masking // ------------------------------------------------------------------- //Cloudprobability functie: // Functie die nieuwe CloudProbability collectie samenvoegt met S2 (sen2cloudless) // meer info: https://medium.com/sentinel-hub/cloud-masks-at-your-service-6e5b2cb2ce8a var getS2_SR_CLOUD_PROBABILITY = function () { var innerJoined = ee . Join . inner (). apply ({ primary : ee . ImageCollection ( \"COPERNICUS/S2_SR\" ), secondary : ee . ImageCollection ( \"COPERNICUS/S2_CLOUD_PROBABILITY\" ), condition : ee . Filter . equals ({ leftField : 'system:index' , rightField : 'system:index' }) }); var mergeImageBands = function ( joinResult ) { return ee . Image ( joinResult . get ( 'primary' )) . addBands ( joinResult . get ( 'secondary' )); }; var newCollection = innerJoined . map ( mergeImageBands ); return ee . ImageCollection ( newCollection ); }; // Mask out clouds var maskClouds = function ( image ) { var cloudProbabilityThreshold = 40 ; var cloudMask = image . select ( 'probability' ). lt ( cloudProbabilityThreshold ); return image . updateMask ( cloudMask ); }; //Aanmaken van een ImageCollection ter hoogte van Mangroves Paramaribo, Suriname var S2_coll = getS2_SR_CLOUD_PROBABILITY () . filterDate ( '2019-08-01' , '2019-10-30' ) // Filteren voor het jaar 2020, droge tijd . filterMetadata ( 'CLOUDY_PIXEL_PERCENTAGE' , 'less_than' , 50 ) //Voorselectie obv wolken . map ( maskClouds ) //toepassen van de cloudmaskfunctie . filterBounds ( ROI ); //collectie filteren obv de Kustzonegeometrie print ( S2_coll ) //Omzetten collectie naar een Image, door .median() te nemen. Hierna clippen we ook tot onze ROI //Ook selecteren we de banden waarmee we verder willen werken var bands = [ 'B2' , 'B3' , 'B4' , 'B5' , 'B6' , 'B7' , 'B8' , 'B8A' , 'B11' , 'B12' ]; var S2_im = S2_coll . median () . select ( bands ) . clip ( ROI ) //Bekijk de .clip-eigenschappen in de Docs Map . centerObject ( S2_im , 11 ) Map . addLayer ( S2_im ,{ min : 50 , max : 1800 , bands : 'B4,B3,B2' }, 'NormaleKleuren_2020' , 0 ) Map . addLayer ( S2_im ,{ min : 700 , max : 4500 , bands : 'B8,B4,B3' }, 'ValseKleuren_2020' , 0 ) Map . addLayer ( S2_im ,{ min : 500 , max : 4000 , bands : 'B8,B11,B2' }, 'Healthy_Vegetation_2020' , 0 )","title":"Beeldcomposieten aanmaken"},{"location":"P6/P6-Supervised.html#trainingsamples-aanmaken","text":"Nadat het wolkenvrije Sentinel-2 beeld is ingeladen, kunnen we deze gebruiken om enkele representatieve samples te verzamelen van enkele landbedekkingklassen waar we in ge\u00efnteresseerd zijn. Er zijn 2 manieren om trainingsdata in Earth Engine op te laden: Door ze in te tekenen als polygonen binnen per klasse, zoals we in komend voorbeeld zullen toepassen. Door eerder ingetekende trainingssamples of GPS-punten op te laden als een 'Asset'. Dit kunnen shapefiles , of .csv -bestanden zijn. Hover met je muis over de 'Geometry Imports box, dat zich naast de geometrietools bevindt. Klik op *'+new layer'. Elke gewenste landbedekkingsklasse dient als een afzonderlijke laag te worden aangemaakt. Laat ons bijvoorbeeld starten met de eenvoudigste klasse 'water'. Zoom in op het beeld en teken polygonen in over oppervlaktes waar je zeker van bent dat het waterlichamen betreft. Het is goed hierin te vari\u00ebren binnen verschillende types van zowel zee als rivieren en andere waterlichamen. Teken ca. 10 polygonen in per klasse. Neem hiervoor zeker je tijd, gezien het belangrijk is dit zeer precies te doen. De kwaliteit van de inputdata bepaalt tevens de kwaliteit van de classificatie. Garbage in = Garbage out . Als je een fout gemaakt heb, kun je even op 'exit' duwen en de laats ingetekende polygoon verwijderen. class Landbekkingsklasse 1 Mangrove 2 OtherForest 3 Water 4 Crop 5 Urban 6 BareSoil Polygonen vs puntdata als traingdata Trainingdata kan bestaan uit puntdata of trainingdata. Beiden hebben hun voor- en nadeel. Puntdata gebruiken als inputdata kan bijvoorbeeld als je beschikt over een grote set GPS-veldpunten van locaties waar je exact weet tot welke landbedekkingsklasse een pixel hoort. Deze pixel wordt dan als referentie aanschouwt. Deze zekerheid van de trainingsdata is dus groot. Een nadeel bij pixels is dat het minder de vari\u00ebteit van de pixels binnen de klasse opneemt en de totale set aan trainingspixels beperkt blijft. Polygonen worden gebruikt om gebieden in te tekenen voor een bepaalde klasse. In earth engine wordt elke pixel binnen deze polygoon dan gebruikt als inputdata. Dit zorgt ervoor dat de trainingsset groter wordt en de variatie binnen een bepaalde klasse beter wordt omvat. Een nadeel is echter dat zo ook foute pixels kunnen worden meegenomen door het onzorgvuldig intekenen van de polygoon. Eenmaal je klaar bent met het intekenen van een klasse, kun je deze import configureren. Klink hiervoor op het tandwieltje naast de klasse. Geef het een gepaste naam. Daarna verander je de 'Import as' van geometry naar type FeatureCollection . Voeg daarna een property toe met de naam 'class', door te klikken op 'Add property' . De eerste klasse geef je waarde 1, de 2e 2, .... Zorg er wel voor dat je goed weet welke waarde je aan welke klasse geeft. Neem hierbij eventueel de class -numering over van bovenstaande tabel. Herhaal dit voor elke klasse. Uiteindelijk verschijnt elke klasse als een ' FeatureCollection ' bij de imports-lijst in je script: Voorbeeld trainingsamples voor Mangrove Nu de 5 klassen aangemaakt zijn, dienen we ze samen te vatten als een complete trainingsset in earth engine; een gezamenlijke FeatureCollection , waarbij de 'class'- property wordt overgenomen. //2. Trainingssamples samenvoegen tot 1 ```FeatureCollection``` var classNames = Mangrove . merge ( OtherForest ). merge ( Water ). merge ( Crop ). merge ( Urban ). merge ( BareSoil ); print ( classNames )","title":"Trainingsamples aanmaken"},{"location":"P6/P6-Supervised.html#de-trainingsdata-aanmaken","text":"Nu hebben we reeds een FeatureCollection met trainingspolygonen, maar deze zeggen nog niks over de trainingspixels. In een volgende stap, extraheren we de trainingspixels per band uit het Sentinel-2 beeld op basis van de aangemaakte polygonen. Dit doen we met de sampleRegions() functie. Deze functie extraheert alle pixels binnen opgegeven polygonen, en schrijft elke pixel afzonderlijk naar een nieuw `Feature binnen een FeatureCollection , dewelke ook de class-property meekrijgen. Afhankelijk van de grootte van de polygonen, kan deze dataset dus zeer lijvig worden. //Trainingspixels extraheren naar featurecollection = trainingsdata var traindata = S2_im . sampleRegions ({ collection : classNames , //De trainingspolygonen properties : [ 'class' ], //Dit neemt de gewenste eigenschappen van de collection over scale : 10 }); print ( 'Aantal trainingspixels: ' , traindata . size ()); //print(traindata) //Niet doen print ( traindata . first ()) //Eerste waarde bekijken,","title":"De trainingsdata aanmaken"},{"location":"P6/P6-Supervised.html#de-classifier-trainen","text":"In een volgende stap maken we een classificatiemodel aan en trainen we deze op basis van de traindata. Er bestaan verschillende mogelijke classifiers en 'machine learning'-algoritmen. In Google Earth Engine zitten deze beschikbaar in de ee.Classifier -groep. We gebruiken er 3, waarna we kijken dewelke tot de meest accurate classificatie leidt o.b.v. de validatiedata.","title":"De classifier trainen"},{"location":"P6/P6-Supervised.html#minimum-distance-classifier","text":"In een eerste instantie dienen we de classifier te trainen, op basis van de opgestelde trainingsdata. We dienen ook aan te geven welke van de properties binnen de trainingssamples de 'class'-bevat, en welke properties gebruikt moeten worden om mee te classificeren (de banden). //4. De classifiers trainen en toepassen // A. Minimum Distance classifier (gebruik van default-waarde 'euclidische afstand') var MinDist = ee . Classifier . minimumDistance (). train ({ features : traindata , classProperty : 'class' , inputProperties : bands //verwijzing naar de eerder aangemaakte bands-lijst });","title":"Minimum Distance Classifier"},{"location":"P6/P6-Supervised.html#cart-classifier","text":"// B. CART classifier var Cart = ee . Classifier . smileCart (). train ({ features : traindata , classProperty : 'class' , inputProperties : bands //verwijzing naar de eerder aangemaakte bands-lijst });","title":"CART classifier"},{"location":"P6/P6-Supervised.html#random-forest-classifier","text":"// C. Random Forest var RandomForest = ee . Classifier . smileRandomForest ({ numberOfTrees : 60 }). train ({ features : traindata , classProperty : 'class' , inputProperties : bands //verwijzing naar de eerder aangemaakte bands-lijst });","title":"Random Forest classifier"},{"location":"P6/P6-Supervised.html#beeld-classificeren-en-visualiseren","text":"Eenmaal de classifier(s) opgesteld zijn, kunnen ze worden toegepast op het volledige S2-beeld. Elke pixel wordt dus toegekend tot een klasse, op basis van de kennis opgedaan uit de trainingsdata. // 5. Classifiers toepassen //MinimumDistance var classified_MD = S2_im . classify ( MinDist ) var classified_CART = S2_im . classify ( Cart ) var classified_RF = S2_im . classify ( RandomForest ) Bij het visualiseren willen we een visueel overzichtelijk resultaat krijgen. Aangezien we eindigen met discrete klassen, stellen we hiervoor een palette op, dat per klasse een kleur aangeeft. var palette = [ 'FF0000' , // mangrove (1) // rood '7CFC00' , // ander bos (2) // lichtgroen '1E90FF' , //water (3) // blauw 'FFFD10' , //crop (4) //geel '000000' , //stad // zwart '876829' , //BareSoil // bruin ]; var classvis = { min : 1 , max : 6 , palette : palette } Map . addLayer ( classified_MD , classvis , 'MinimumDistance' ) Map . addLayer ( classified_CART , classvis , 'CART' ) Map . addLayer ( classified_RF , classvis , 'RandomForest' )","title":"Beeld classificeren en visualiseren"},{"location":"P6/P6-Supervised.html#accuracy-assessment","text":"In de accuracy assessment gaan we de accuraatheid van het model nagaan. Dit doen we o.b.v. een error matrix . Om deze op te stellen hebben we nood aan testdata.","title":"Accuracy assessment"},{"location":"P6/P6-Supervised.html#training-validation-en-testdata","text":"Voordat we onze data gebruiken om het model te trainen, splitsen we het in trainings en validatiedata. Daarnaast wordt er vak nog gebruik gemaakt van een derde, volledig afzonderlijke dataset: de testdataset . In veel gevallen, zoals in komend voorbeeld, wordt de validatieset ook gebruikt als testdataset, maar dit is statistisch gezien niet de beste praktijk. Traindata = de data gebruikt om het model te trainen en dus te fitten . Het model bekijkt en leert van deze data. Validatiedata = Het deel van de data dat gebruikt zal worden om na te gaan hoe goed het model werkt op onbekende data. Dit deel zal dus niet gebruikt worden om het model te trainen. Hierdoor kunnen verschillende classificatiemodellen en parameters binnen het model tegenover elkaar worden afgewogen en het model zo worden geperfectioneerd. Dit wordt ook wel parameter tuning genoemd. Validatiedata wordt dus gebruikt tijdens de ontwikkeling en het zoeken van het beste model. Aangezien deze data van dezelfde dataset komt treedt er spatiale autocorrelatie op, waarbij de validatiepixels veelal buurpixels zijn van trainpixels. Testdata = Deze afzonderlijke dataset wordt gebruikt om bij een finaal model accuraatheidsmaten van de bekomen classificatie te berekenen. Testdatasets worden meestal ook zeer goed verzorgd en zijn goed verzamelde (veld)datapunten. De spatiale autocorrelatie vervalt hier. In voorliggend voorbeeld maken we gebruik van een extra testdataset. Gezien we geen optimalisatie van parameters gaan doorvoeren, maken we geen gebruik van validatiedata en wordt de volledige trainingscollectie trainingsdata. Dit doen we aan de hand van onze validatie dataset als een externe testset. Deze set bestaat uit kleine polygonen met een diameter van 25m en representeren GPS-punten genomen op veldbezoek. De 25m-buffer werd genomen om voldoende testpixels te weerhouden voor de accuracy assessment. Je kunt de shape-file hier downloaden: P6_testdata (shapefile) Om deze toe te voegen aan Earth Engine, laad je deze op via de 'Asset'-tab. Daarna kun je het importeren als een FeatureCollection in je script. Noem dit 'Testdata_pol'. Bekijk ook even deze polygonen. Onder welke property zitten de klassen hier opgeslagen? Vervolgens extraheren we de pixelwaarden op basis van deze testpolygonen, net zoals we dit gedaan hebben bij de trainingspixels: //Na inlezen van validatiedata: maak een testdatacollectie, zoals bij het opmaken van traindata print ( 'Testpolygonen' , Testdata_pol ) //bekijk de properties //Testpixels extraheren naar featurecollection var testdata = S2_im . sampleRegions ({ collection : Testdata_pol , //De trainingspolygonen properties : [ 'val' ], //Dit neemt de gewenste eigenschappen van de collection over scale : 10 }); print ( 'Aantal testpixels: ' , traindata . size ()); Nu we de testpixelwaarden ge\u00ebxtraheerd hebben, kunnen we deze vergelijken met de geclassificeerde pixels in een error matrix . In onderstaand stukje code staat het voorbeeld voor de Minimum Distance classifier. Pas dit toe voor alle classifiers. Welke classifier heeft de grootste algemene accuraatheid ( Overall accuracy )? // Accuracy assessment uitvoeren per classifier // Validatie met de validatiedata var val_MinDist = testdata . classify ( MinDist ); var ErrorMatrix_MinDist = val_MinDist . errorMatrix ( 'val' , 'classification' ) print ( 'Validation error matrix: ' , ErrorMatrix_MinDist ); print ( 'Validation overall accuracy: ' , ErrorMatrix_MinDist . accuracy ());","title":"Training- Validation en Testdata"},{"location":"P6/P6-Supervised.html#exporteren-van-de-error-matrix","text":"In Google Earth Engine is de weergave van de error matrix niet zo handig. Om verdere accuraatheidsmaten uit te rekenen en een betere interpretatie te kunnen uitvoeren kan het handig zijn om de error matrix te exporteren als een .csv-bestand, dewelke in andere software (zoals excel) geopend kan worden. Met de Export.table.toDrive() -functie kunnen we de matrix exporteren naar onze Google Drive. Hiervoor dienen we dit eerst om te zetten naar een feature . //Omzetten naar een Feature var ErrorMatrix_MinDist = ee . Feature ( null , { matrix : ErrorMatrix_MinDist . array ()}); //Exporteren van de errormatrix Export . table . toDrive ({ collection : ee . FeatureCollection ( ErrorMatrix_MinDist ), description : 'P6_Errormatrix' , fileFormat : 'CSV' , folder : 'TELEDETECTIE_2020' });","title":"Exporteren van de Error Matrix"},{"location":"P6/P6-SurinameCase.html","text":"Over Mangroves Defenitie en verspreiding Het Mangrove-ecosysteem dat kan gezien worden als een soort moerasecosyteem, gedomineerd door mangrovebomen/bossen. Het Mangrove-ecosysteem is de zone die zich doorgaans bevindt het intergetijdenzone tussen zee en land in zowel tropische als subtropische gebieden. Binnen dit Mangrove-ecosyteem bevinden zich de Mangrovebomen zelf ( 'True Mangroves' ) en de geassocieerde soorten ( Mangrove associates ). Hoewel er geen vastgelegde defenitie bestaat van mangrovebossen, kan deze van Tomilson (2016) worden gezien als de meest geaccepteerde: True Mangroves are plant species that: 1. Occur only in mangrove forests and are not found in terrestrial communities 2. Play a major role in the structure of the mangrove community, sometimes forming pure stands. 3. Have morphological specialisations to the mangrove environment 4. Have some mechanism for salt exclusion In totaal bestaan er zo'n 55 mangrovesoorten die aan bovenstaande defenitie voldoen, waarvan de hoogste soortendiversiteit zich voordoet rond het westelijk deel van de Stille Oceaan. Globale inschattingen van dit ecosysteem vari\u00ebren tussen 110.000 tot 240.000 km\u00b2 (Giri, 2016). Mangroves in Suriname In Suriname bestaan er \"slechts\" 5 mangrovesoorten waarvan 2 soorten de grootste dominantie vertonen: de zwarte mangrove of 'Parwa' ( Avicennia germinans ) die hoofdzakelijk langs de kustlijnen groeit in homogene bestanden en de rode mangrove of 'Mangro' ( Rhizopora mangle ), die landinwaarste en langs rivieroevers terug te vinden is. Verspreiding van de 2 dominerende Mangrovesoorten in 2018 (SBB, 2019). Parwa ( Avicennia germinans ) Mangro ( Rhizopora Mangle ) Mangrove mapping LU/LC klassen In de voorbeelde voor beeldclassificatie gaan we zelf een LU/LC-kaart maken voor de kustzone ter hoogte van Paramaribo, de hoofdstad van Suriname. Hierin gaan we onderscheid maken tussen volgende klassen, zodat we Mangroves kunnen onderscheiden: Mangrove Ander bos Water Crops/grasvegetatie Urban (stedelijke omgeving) Naakte Bodem (soil) Onderscheiden van Mangroves op satellietbeelden Als laatste voorbereidende stap gaan we na op welke manier we Mangroves (visueel) kunnen onderscheiden van de andere klassen (met in het bijzonder aangrenzend inlands bos). Healthy Vegetation Composite Het aanmaken van verschillende beeldcomposieten helpt om visueel de verschillende landbedekkingsklassen te onderscheiden. Naas de Normale Kleurencomposiet (gemakkelijkst interpreteerbaar voor het menselijk oog) en de Valse Kleurencomposiet (benadrukking verschil vegetatie - andere klassen) kan ook de healthy vegetation composite nuttig zijn. Deze composiet is gemaakt met volgende banden: RGB = NIR, SWIR en Blauw, wat met de resepectievelijke Sentinel-2 banden B8, B11, B2 kan worden aangemaakt. In deze composiet kleurt 'gezonde vegetatie' met toetsen van rood, orangje en geel. Doordat de SWIR-band wordt gebruik, kunnen verschillende fases van plantengroei en/of stress worden nagegaan. Mangrovebossen, kan op deze composiet worden onderscheden van andere bossen door hun specifieke watergebonden milieu. Aanmaken van Beeldcomposieten Maak een 'Healthy Vegetation'-composiet aan van de Surinaamse kustlijn. Bekijk met welke stretch de mangrovezone het best tot uiting komt. Mangrove Vegetation Index (MVI) Doorheen de jaren werden ook enkele specifieke Mangroves indices ontwikkeld, om de detectie van Mangroves te vergemakkelijken. Een goed voorbeeld is de 'Mangrove Vegetation Index' of MVI ( Baloloy, 2020 ), dat recentelijk werd ontwikkeld op basis van de Sentinel-2 banden B2, B8, B11, die de specifieke 'groenheid' en vochtomstandigheid die de mangrovebossen typeert in de verf zet, om zo een onderscheid te kunnen maken met de naburige terrestrische bossen en vegetaties. De MVI wordt als volgt geformuleerd: MVI = { NIR - Green \\over SWIR1 - Green}. Wat zicht vertaald in Sentinel-2 banden als: MVI = { B8 - B3 \\over B11 - B3}.","title":"CASE - Mangrove monitoring in Suriname"},{"location":"P6/P6-SurinameCase.html#over-mangroves","text":"","title":"Over Mangroves"},{"location":"P6/P6-SurinameCase.html#defenitie-en-verspreiding","text":"Het Mangrove-ecosysteem dat kan gezien worden als een soort moerasecosyteem, gedomineerd door mangrovebomen/bossen. Het Mangrove-ecosysteem is de zone die zich doorgaans bevindt het intergetijdenzone tussen zee en land in zowel tropische als subtropische gebieden. Binnen dit Mangrove-ecosyteem bevinden zich de Mangrovebomen zelf ( 'True Mangroves' ) en de geassocieerde soorten ( Mangrove associates ). Hoewel er geen vastgelegde defenitie bestaat van mangrovebossen, kan deze van Tomilson (2016) worden gezien als de meest geaccepteerde: True Mangroves are plant species that: 1. Occur only in mangrove forests and are not found in terrestrial communities 2. Play a major role in the structure of the mangrove community, sometimes forming pure stands. 3. Have morphological specialisations to the mangrove environment 4. Have some mechanism for salt exclusion In totaal bestaan er zo'n 55 mangrovesoorten die aan bovenstaande defenitie voldoen, waarvan de hoogste soortendiversiteit zich voordoet rond het westelijk deel van de Stille Oceaan. Globale inschattingen van dit ecosysteem vari\u00ebren tussen 110.000 tot 240.000 km\u00b2 (Giri, 2016).","title":"Defenitie en verspreiding"},{"location":"P6/P6-SurinameCase.html#mangroves-in-suriname","text":"In Suriname bestaan er \"slechts\" 5 mangrovesoorten waarvan 2 soorten de grootste dominantie vertonen: de zwarte mangrove of 'Parwa' ( Avicennia germinans ) die hoofdzakelijk langs de kustlijnen groeit in homogene bestanden en de rode mangrove of 'Mangro' ( Rhizopora mangle ), die landinwaarste en langs rivieroevers terug te vinden is. Verspreiding van de 2 dominerende Mangrovesoorten in 2018 (SBB, 2019). Parwa ( Avicennia germinans ) Mangro ( Rhizopora Mangle )","title":"Mangroves in Suriname"},{"location":"P6/P6-SurinameCase.html#mangrove-mapping","text":"","title":"Mangrove mapping"},{"location":"P6/P6-SurinameCase.html#lulc-klassen","text":"In de voorbeelde voor beeldclassificatie gaan we zelf een LU/LC-kaart maken voor de kustzone ter hoogte van Paramaribo, de hoofdstad van Suriname. Hierin gaan we onderscheid maken tussen volgende klassen, zodat we Mangroves kunnen onderscheiden: Mangrove Ander bos Water Crops/grasvegetatie Urban (stedelijke omgeving) Naakte Bodem (soil)","title":"LU/LC klassen"},{"location":"P6/P6-SurinameCase.html#onderscheiden-van-mangroves-op-satellietbeelden","text":"Als laatste voorbereidende stap gaan we na op welke manier we Mangroves (visueel) kunnen onderscheiden van de andere klassen (met in het bijzonder aangrenzend inlands bos).","title":"Onderscheiden van Mangroves op satellietbeelden"},{"location":"P6/P6-SurinameCase.html#healthy-vegetation-composite","text":"Het aanmaken van verschillende beeldcomposieten helpt om visueel de verschillende landbedekkingsklassen te onderscheiden. Naas de Normale Kleurencomposiet (gemakkelijkst interpreteerbaar voor het menselijk oog) en de Valse Kleurencomposiet (benadrukking verschil vegetatie - andere klassen) kan ook de healthy vegetation composite nuttig zijn. Deze composiet is gemaakt met volgende banden: RGB = NIR, SWIR en Blauw, wat met de resepectievelijke Sentinel-2 banden B8, B11, B2 kan worden aangemaakt. In deze composiet kleurt 'gezonde vegetatie' met toetsen van rood, orangje en geel. Doordat de SWIR-band wordt gebruik, kunnen verschillende fases van plantengroei en/of stress worden nagegaan. Mangrovebossen, kan op deze composiet worden onderscheden van andere bossen door hun specifieke watergebonden milieu. Aanmaken van Beeldcomposieten Maak een 'Healthy Vegetation'-composiet aan van de Surinaamse kustlijn. Bekijk met welke stretch de mangrovezone het best tot uiting komt.","title":"Healthy Vegetation Composite"},{"location":"P6/P6-SurinameCase.html#mangrove-vegetation-index-mvi","text":"Doorheen de jaren werden ook enkele specifieke Mangroves indices ontwikkeld, om de detectie van Mangroves te vergemakkelijken. Een goed voorbeeld is de 'Mangrove Vegetation Index' of MVI ( Baloloy, 2020 ), dat recentelijk werd ontwikkeld op basis van de Sentinel-2 banden B2, B8, B11, die de specifieke 'groenheid' en vochtomstandigheid die de mangrovebossen typeert in de verf zet, om zo een onderscheid te kunnen maken met de naburige terrestrische bossen en vegetaties. De MVI wordt als volgt geformuleerd: MVI = { NIR - Green \\over SWIR1 - Green}. Wat zicht vertaald in Sentinel-2 banden als: MVI = { B8 - B3 \\over B11 - B3}.","title":"Mangrove Vegetation Index (MVI)"},{"location":"P6/P6-Unsupervised.html","text":"Niet-gesuperviseerde classificatie is de procedure waarbij pixel worden ingedeeld volgens spectraal gelijkende klassen zonder dat hierbij trainingsklassen nodig zijn. De resulterende klassen worden dan ook spectrale klassen genoemd. Het is aan de gebruiker om de resulterende klassen te interpreteren en te labelen. Er bestaan verschillende niet-gesuperviseerde algoritmen, maar de meest gekende groep is deze van de clustering . In een clusteranalyse worden pixels met gelijkende spectrale kenmerken tot dezelfde klasse gerekend. K-Means Clustering Een van de meest gebruikte cluster-algoritmen is de \"K-means\" clustering, waarbij de gebruiker op voorhand een aantal beginclusters opgeeft waarmee het algoritme arbitrair dat aantal clusters in de multi-dimensionale ruimte. Elke pixel wordt dan in een eerste fase toegekend tot de cluster waar die pixels zich gemiddeld het dichtst bij bevindt. Na deze eerste 'run' worden de clusters herberekend, waarbij de variantie binnen elke cluster wordt geminimaliseerd. Hierna worden de pixels opnieuw toegekend tot de 'best passende' cluster. Deze procedure wordt herhaald (iteraties) totdat er zich geen significante verplaatsing van de clustercentra meer voordoet en de variantie binnen elke cluster dus ook niet meer significant daalt. Principe van de K-means clustering in een 2-dimensionaal vlak. (Bron: dashee87.github.io ) Classificatie van de Surinaamse kustzone In Earth Engine zit de clustering ook vervat in ee.Clusterer . We maken in volgend voorbeeld gebruik van de weka k-means cluster. Maak een nieuw script aan: P6_UnsupervisedClass. Zoals steeds filteren en reducen we een satellietbeeld, om met een wolkenvrije image verder te kunnen werken. We focussen ons in dit voorbeeld op de kustlijn van Suriname, ter hoogte van de hoofdstad: Paramaribo. We focussen hierbij op maanden binnen de grote droge tijd (Augustus - December), aangezien de wolkbedekking dan beperkter zou moeten zijn in vergelijking met de natte tijden. Maak hiervoor eerst een polygoon aan met de locatie van Paramaribo. Eventueel kun je hiervoor onderstaande code gebruiken: var Paramaribo = /* color: #d63000 */ /* shown: false */ /* displayProperties: [ { \"type\": \"rectangle\" } ] */ ee . Geometry . Polygon ( [[[ - 55.31615692285674 , 6.000339363352038 ], [ - 55.31615692285674 , 5.8043169248564865 ], [ - 54.91446930078643 , 5.8043169248564865 ], [ - 54.91446930078643 , 6.000339363352038 ]]], null , false ); Start Daarna met het aanmaken van het S2-beeld // -------------------------------------------------------------------- // STAP 1 - Inladen en klaarzetten van S2-beeld. M\u00e9t extra cloud-masking // ------------------------------------------------------------------- //Cloudprobability functie: // Functie die nieuwe CloudProbability collectie samenvoegt met S2 (sen2cloudless) // meer info: https://medium.com/sentinel-hub/cloud-masks-at-your-service-6e5b2cb2ce8a var getS2_SR_CLOUD_PROBABILITY = function () { var innerJoined = ee . Join . inner (). apply ({ primary : ee . ImageCollection ( \"COPERNICUS/S2_SR\" ), secondary : ee . ImageCollection ( \"COPERNICUS/S2_CLOUD_PROBABILITY\" ), condition : ee . Filter . equals ({ leftField : 'system:index' , rightField : 'system:index' }) }); var mergeImageBands = function ( joinResult ) { return ee . Image ( joinResult . get ( 'primary' )) . addBands ( joinResult . get ( 'secondary' )); }; var newCollection = innerJoined . map ( mergeImageBands ); return ee . ImageCollection ( newCollection ); }; // Mask out clouds var maskClouds = function ( image ) { var cloudProbabilityThreshold = 40 ; var cloudMask = image . select ( 'probability' ). lt ( cloudProbabilityThreshold ); return image . updateMask ( cloudMask ); }; //Aanmaken van een ImageCollection ter hoogte van de kustlijn met mangroves en de hoofdstad Paramaribo, Suriname var S2_coll = getS2_SR_CLOUD_PROBABILITY () . filterDate ( '2019-08-01' , '2019-10-30' ) // Filteren voor het jaar 2020, droge tijd . filterMetadata ( 'CLOUDY_PIXEL_PERCENTAGE' , 'less_than' , 50 ) //Voorselectie obv wolken . map ( maskClouds ) //toepassen van de cloudmaskfunctie . filterBounds ( Paramaribo ); //collectie filteren obv de Kustzonegeometrie //Omzetten collectie naar een Image, door de.median() reducer toe te passen. Hierna clippen we ook tot onze ROI var S2_im = S2_coll . median () . clip ( Paramaribo ) //Bekijk de .clip-eigenschappen in de Docs De .clip() functie De .clip() -functie wordt toegepast om het resulterende beeld bij te snijden naar de exacte grenzen van de aangemaakte polygoon (ROI). .clip() is enkel toepasbaar op beelden van het image -type, maar kan niet worden toegepast op een ``\u00ccmageCollection , gezien dit een te grote rekencapaciteit zou vergen. Daarom wordt de functie .FilterBounds()```gebruikt, waarbij enkel gefilterd wordt op basis van een spatiaal feature (punt, lijn of polygoon) maar geen beelden worden bijgesneden. De Earth Engine clusterer volgt het algoritme van Weka , een open-source machine learning softwarepakket. In dit algoritme worden eerst pixels uit het beeld 'gesampled', waarop het k-means algoritme wordt losgelaten. Eenmaal het algoritme op punt is, wordt het toegepast op de rest van het beeld. M.a.w. wordt er een niet-gesuperviseerd model getrained op een willekeurige sample van pixels, die representatief wordt geacht voor de rest van het beeld. Het aantal te sampelen pixels moet dus voldoende groot gekozen worden, maar indien het te groot wordt, zal earth engine een foutmelding geven: clusters: Layer error: Computed value is too large. De maximale capaciteit van earth engine ligt bij ca. 1 miljoen pixels. // Aanmaken\"training\" dataset. var training = S2_im . sample ({ region : Paramaribo , scale : 10 , numPixels : 5000 }); Eenmaal de pixels geselecteerd zijn die gebruikt gaan worden voor het aanmaken van de k clusters, kan het 'K-means cluster-algoritme' worden getraind. Deze bevat enkele parameters die de gebruiker nog kan instellen: nClusters : het eerste en het enige verplicht aan te geven argument dat het aantal gewenste clusters aangeeft. init : de initialisatiemethode. Hiermee kan de manier waarop de initi\u00eble clustercentra worden gekozen. De default -instelling kiest ad random de beginpunten. Deze zullen we in deze oefening gebruiken. distanceFuntion : welke afstandsberekening moet worden toegepast: Euclidisch of Manhattan . De euclidische afstand is default . maxIterations : het maximale aantal iteraties, indien opgegeven. De clusteranalyse stopt na dit aantal iteraties. // Clusterer opstellen en trainen. Opgeven van 5 klassen, de andere parameters laten we op default. var Kmeans_cl = ee . Clusterer . wekaKMeans ( 5 ). train ( training ); // Laat de cluster los op het volledige beeld var classified = S2_im . cluster ( Kmeans_cl ); * Finaal visualiseren we ook het resultaat. Om de bekomen klassen snel een afzonderlijke kleur te geven, maken we gebruik van .randomVisualizer() . // Display the clusters with random colors. Map . addLayer ( classified . randomVisualizer (), {}, 'clusters' ); Voorbeeldresultaat van de K-means clustering. Opdrachten Interpreteer de bekomen klassen en tracht ze te linken aan landbedekkingsklassen. Gebruik hiervoor ook een Normale Kleuren, Valse Kleuren en 'Healthy Vegetation' (RGB = B8,B11,B2) composiet. Herhaal bovenstaande clustering enkele keren, met onderstaande parameters. Vergelijk de resultaten ook steeds met elkaar: Je het aantal clusters optrekt naar 10. Je de parameter maxIterations binen de ee.ClustererwekaKMeans() functie toevoegd. Test de waarden 1, 10 en 30 Je de factor numPixels vergroot (naar bv 10000).","title":"Niet-gesuperviseerde classificatie"},{"location":"P6/P6-Unsupervised.html#k-means-clustering","text":"Een van de meest gebruikte cluster-algoritmen is de \"K-means\" clustering, waarbij de gebruiker op voorhand een aantal beginclusters opgeeft waarmee het algoritme arbitrair dat aantal clusters in de multi-dimensionale ruimte. Elke pixel wordt dan in een eerste fase toegekend tot de cluster waar die pixels zich gemiddeld het dichtst bij bevindt. Na deze eerste 'run' worden de clusters herberekend, waarbij de variantie binnen elke cluster wordt geminimaliseerd. Hierna worden de pixels opnieuw toegekend tot de 'best passende' cluster. Deze procedure wordt herhaald (iteraties) totdat er zich geen significante verplaatsing van de clustercentra meer voordoet en de variantie binnen elke cluster dus ook niet meer significant daalt. Principe van de K-means clustering in een 2-dimensionaal vlak. (Bron: dashee87.github.io )","title":"K-Means Clustering"},{"location":"P6/P6-Unsupervised.html#classificatie-van-de-surinaamse-kustzone","text":"In Earth Engine zit de clustering ook vervat in ee.Clusterer . We maken in volgend voorbeeld gebruik van de weka k-means cluster. Maak een nieuw script aan: P6_UnsupervisedClass. Zoals steeds filteren en reducen we een satellietbeeld, om met een wolkenvrije image verder te kunnen werken. We focussen ons in dit voorbeeld op de kustlijn van Suriname, ter hoogte van de hoofdstad: Paramaribo. We focussen hierbij op maanden binnen de grote droge tijd (Augustus - December), aangezien de wolkbedekking dan beperkter zou moeten zijn in vergelijking met de natte tijden. Maak hiervoor eerst een polygoon aan met de locatie van Paramaribo. Eventueel kun je hiervoor onderstaande code gebruiken: var Paramaribo = /* color: #d63000 */ /* shown: false */ /* displayProperties: [ { \"type\": \"rectangle\" } ] */ ee . Geometry . Polygon ( [[[ - 55.31615692285674 , 6.000339363352038 ], [ - 55.31615692285674 , 5.8043169248564865 ], [ - 54.91446930078643 , 5.8043169248564865 ], [ - 54.91446930078643 , 6.000339363352038 ]]], null , false ); Start Daarna met het aanmaken van het S2-beeld // -------------------------------------------------------------------- // STAP 1 - Inladen en klaarzetten van S2-beeld. M\u00e9t extra cloud-masking // ------------------------------------------------------------------- //Cloudprobability functie: // Functie die nieuwe CloudProbability collectie samenvoegt met S2 (sen2cloudless) // meer info: https://medium.com/sentinel-hub/cloud-masks-at-your-service-6e5b2cb2ce8a var getS2_SR_CLOUD_PROBABILITY = function () { var innerJoined = ee . Join . inner (). apply ({ primary : ee . ImageCollection ( \"COPERNICUS/S2_SR\" ), secondary : ee . ImageCollection ( \"COPERNICUS/S2_CLOUD_PROBABILITY\" ), condition : ee . Filter . equals ({ leftField : 'system:index' , rightField : 'system:index' }) }); var mergeImageBands = function ( joinResult ) { return ee . Image ( joinResult . get ( 'primary' )) . addBands ( joinResult . get ( 'secondary' )); }; var newCollection = innerJoined . map ( mergeImageBands ); return ee . ImageCollection ( newCollection ); }; // Mask out clouds var maskClouds = function ( image ) { var cloudProbabilityThreshold = 40 ; var cloudMask = image . select ( 'probability' ). lt ( cloudProbabilityThreshold ); return image . updateMask ( cloudMask ); }; //Aanmaken van een ImageCollection ter hoogte van de kustlijn met mangroves en de hoofdstad Paramaribo, Suriname var S2_coll = getS2_SR_CLOUD_PROBABILITY () . filterDate ( '2019-08-01' , '2019-10-30' ) // Filteren voor het jaar 2020, droge tijd . filterMetadata ( 'CLOUDY_PIXEL_PERCENTAGE' , 'less_than' , 50 ) //Voorselectie obv wolken . map ( maskClouds ) //toepassen van de cloudmaskfunctie . filterBounds ( Paramaribo ); //collectie filteren obv de Kustzonegeometrie //Omzetten collectie naar een Image, door de.median() reducer toe te passen. Hierna clippen we ook tot onze ROI var S2_im = S2_coll . median () . clip ( Paramaribo ) //Bekijk de .clip-eigenschappen in de Docs De .clip() functie De .clip() -functie wordt toegepast om het resulterende beeld bij te snijden naar de exacte grenzen van de aangemaakte polygoon (ROI). .clip() is enkel toepasbaar op beelden van het image -type, maar kan niet worden toegepast op een ``\u00ccmageCollection , gezien dit een te grote rekencapaciteit zou vergen. Daarom wordt de functie .FilterBounds()```gebruikt, waarbij enkel gefilterd wordt op basis van een spatiaal feature (punt, lijn of polygoon) maar geen beelden worden bijgesneden. De Earth Engine clusterer volgt het algoritme van Weka , een open-source machine learning softwarepakket. In dit algoritme worden eerst pixels uit het beeld 'gesampled', waarop het k-means algoritme wordt losgelaten. Eenmaal het algoritme op punt is, wordt het toegepast op de rest van het beeld. M.a.w. wordt er een niet-gesuperviseerd model getrained op een willekeurige sample van pixels, die representatief wordt geacht voor de rest van het beeld. Het aantal te sampelen pixels moet dus voldoende groot gekozen worden, maar indien het te groot wordt, zal earth engine een foutmelding geven: clusters: Layer error: Computed value is too large. De maximale capaciteit van earth engine ligt bij ca. 1 miljoen pixels. // Aanmaken\"training\" dataset. var training = S2_im . sample ({ region : Paramaribo , scale : 10 , numPixels : 5000 }); Eenmaal de pixels geselecteerd zijn die gebruikt gaan worden voor het aanmaken van de k clusters, kan het 'K-means cluster-algoritme' worden getraind. Deze bevat enkele parameters die de gebruiker nog kan instellen: nClusters : het eerste en het enige verplicht aan te geven argument dat het aantal gewenste clusters aangeeft. init : de initialisatiemethode. Hiermee kan de manier waarop de initi\u00eble clustercentra worden gekozen. De default -instelling kiest ad random de beginpunten. Deze zullen we in deze oefening gebruiken. distanceFuntion : welke afstandsberekening moet worden toegepast: Euclidisch of Manhattan . De euclidische afstand is default . maxIterations : het maximale aantal iteraties, indien opgegeven. De clusteranalyse stopt na dit aantal iteraties. // Clusterer opstellen en trainen. Opgeven van 5 klassen, de andere parameters laten we op default. var Kmeans_cl = ee . Clusterer . wekaKMeans ( 5 ). train ( training ); // Laat de cluster los op het volledige beeld var classified = S2_im . cluster ( Kmeans_cl ); * Finaal visualiseren we ook het resultaat. Om de bekomen klassen snel een afzonderlijke kleur te geven, maken we gebruik van .randomVisualizer() . // Display the clusters with random colors. Map . addLayer ( classified . randomVisualizer (), {}, 'clusters' ); Voorbeeldresultaat van de K-means clustering.","title":"Classificatie van de Surinaamse kustzone"},{"location":"P6/P6-Unsupervised.html#opdrachten","text":"Interpreteer de bekomen klassen en tracht ze te linken aan landbedekkingsklassen. Gebruik hiervoor ook een Normale Kleuren, Valse Kleuren en 'Healthy Vegetation' (RGB = B8,B11,B2) composiet. Herhaal bovenstaande clustering enkele keren, met onderstaande parameters. Vergelijk de resultaten ook steeds met elkaar: Je het aantal clusters optrekt naar 10. Je de parameter maxIterations binen de ee.ClustererwekaKMeans() functie toevoegd. Test de waarden 1, 10 en 30 Je de factor numPixels vergroot (naar bv 10000).","title":"Opdrachten"},{"location":"P6/P6-intro.html","text":"Practicum 3: Beeldclassificatie Doelstelling In dit practicum verdiepen we ons in het proces van de beeldclassificatie, waarbij manieren om remote sensing beelden te vertalen in landgebruikskaarten. Als voorbeeld nemen we een classificatie van mangroves. Beeldclassificatie: introductie Beeldclassificatie (Engels: 'Image Classification' ) is de (complexe) procedure waarbij een multiband rasterbeeld wordt ingedeeld in klassen om hieruit spatiale informatie te ontrekken. Het is wellicht de meest belangrijke handeling bij digitale beeldanalyse. Het resulterende beeld kan gebruikt worden om thematische kaarten aan te maken. Gedurende de classificatie worden pixels ingedeeld in groepen, op basis van hun gemeenschappelijke karakteristieken. Er bestaan twee grote groepen van classificatiemethoden: Supervised of gesuperviseerde en Unsupervised of niet-gesuperviseerde classificatie. In een niet-gesuperviseerde classificatie deelt de computer pixels in die spectraal gezien gelijkend zijn aan elkaar in een aantal klassen. Dit aantal kan op voorhand worden vastgelegd, of in de classificatieprocedure worden bepaald. Belangrijk is dat de analyse wordt uitgevoerd zonder dat de gebruiker hiervoor voorbeeldklassen of trainingsdata als input moet voorzien. De enige noodzakelijke input door de gebruiker betreft de keuze van het algoritme en eventueel het aantal gewenste resulterende klasses. In een gesuperviseerde classificatie wordt de classifier getrained op basis van enkele voorbeeldklassen of training data . Nieuwe, ongeklasseerde data wordt dan aan het algoritme voorgelegd, die dit dan op basis van het getrainde algoritme gaat indelen. In een finale stap wordt dan een andere portie van gekende data, de validatiedata , gebruikt om de accuraatheid ( accuracy ) van de classificatie kwantitatief te bepalen. In dit type classificatie is het aantal resulturende klassen eveneens bepaald op basis van het aantal klassen meegegeven tijdens het trainen van de data. Supergeviseerde vs niet-gesuperviseerde classifiers Niet-gesuperviseerde classificatie Gesuperviseerde classificatie + + Er is geen voorkennis nodig van het gebied. De trainingsklassen komen overeen met de eigenlijke landbekking Minimale input van menselijke fouten Trainingdata is herbruikbaar, tenzij de landbedekking verandert. Gemakkelijk uit te voeren Resultaat is meteen bruikbaar, gezien de klassen gekend zijn. - - De resulterende klassen komen niet per s\u00e9 overeen met gewenste landbekkingsklassen De opgegeven klassen komen niet altijd overeen met de spectrale klassen Spatiale relaties of spatiale context wordt niet meegenomen in de data Zekerheid/consistentie is niet over alle klassen gelijk Interpretatie kan moeilijk zijn Verzamelen van (voldoende) trainingdata kost moeite, geld en tijd","title":"Intro"},{"location":"P6/P6-intro.html#practicum-3-beeldclassificatie","text":"","title":"Practicum 3: Beeldclassificatie"},{"location":"P6/P6-intro.html#doelstelling","text":"In dit practicum verdiepen we ons in het proces van de beeldclassificatie, waarbij manieren om remote sensing beelden te vertalen in landgebruikskaarten. Als voorbeeld nemen we een classificatie van mangroves.","title":"Doelstelling"},{"location":"P6/P6-intro.html#beeldclassificatie-introductie","text":"Beeldclassificatie (Engels: 'Image Classification' ) is de (complexe) procedure waarbij een multiband rasterbeeld wordt ingedeeld in klassen om hieruit spatiale informatie te ontrekken. Het is wellicht de meest belangrijke handeling bij digitale beeldanalyse. Het resulterende beeld kan gebruikt worden om thematische kaarten aan te maken. Gedurende de classificatie worden pixels ingedeeld in groepen, op basis van hun gemeenschappelijke karakteristieken. Er bestaan twee grote groepen van classificatiemethoden: Supervised of gesuperviseerde en Unsupervised of niet-gesuperviseerde classificatie. In een niet-gesuperviseerde classificatie deelt de computer pixels in die spectraal gezien gelijkend zijn aan elkaar in een aantal klassen. Dit aantal kan op voorhand worden vastgelegd, of in de classificatieprocedure worden bepaald. Belangrijk is dat de analyse wordt uitgevoerd zonder dat de gebruiker hiervoor voorbeeldklassen of trainingsdata als input moet voorzien. De enige noodzakelijke input door de gebruiker betreft de keuze van het algoritme en eventueel het aantal gewenste resulterende klasses. In een gesuperviseerde classificatie wordt de classifier getrained op basis van enkele voorbeeldklassen of training data . Nieuwe, ongeklasseerde data wordt dan aan het algoritme voorgelegd, die dit dan op basis van het getrainde algoritme gaat indelen. In een finale stap wordt dan een andere portie van gekende data, de validatiedata , gebruikt om de accuraatheid ( accuracy ) van de classificatie kwantitatief te bepalen. In dit type classificatie is het aantal resulturende klassen eveneens bepaald op basis van het aantal klassen meegegeven tijdens het trainen van de data.","title":"Beeldclassificatie: introductie"},{"location":"P6/P6-intro.html#supergeviseerde-vs-niet-gesuperviseerde-classifiers","text":"Niet-gesuperviseerde classificatie Gesuperviseerde classificatie + + Er is geen voorkennis nodig van het gebied. De trainingsklassen komen overeen met de eigenlijke landbekking Minimale input van menselijke fouten Trainingdata is herbruikbaar, tenzij de landbedekking verandert. Gemakkelijk uit te voeren Resultaat is meteen bruikbaar, gezien de klassen gekend zijn. - - De resulterende klassen komen niet per s\u00e9 overeen met gewenste landbekkingsklassen De opgegeven klassen komen niet altijd overeen met de spectrale klassen Spatiale relaties of spatiale context wordt niet meegenomen in de data Zekerheid/consistentie is niet over alle klassen gelijk Interpretatie kan moeilijk zijn Verzamelen van (voldoende) trainingdata kost moeite, geld en tijd","title":"Supergeviseerde vs niet-gesuperviseerde classifiers"},{"location":"P7/P7-Intro.html","text":"Tot nu toe zijn we enkel bezig geweest met.... Tijdserie analyse is een veel gebruikte operatie in Remote Sensing. Het draagt immers bij tot het modelleren van seizoenale patronen en het monitoren van landcover. Earth Engine is gezien het grote (historische) aanbod aan RS data een zeer geschikt medium om tijdseries te gaan analyseren. Tijdserieplots aanmaken in Google Earth Engine De NDSI index In onderstaand voorbeeldje bekijken we de aanwezigheid van sneeuw in Joensuu, Finland doorheen de afgelopen jaren. Hiervoor maken we gebruik van alweer een nieuwe index: de Normalized Difference Snow Index (NDSI) . Deze index wordt gebruikt om de aanwezigheid van sneeuw/ijs te benadrukken, ten opzichte van de andere landbekkingsklassen waaronder wolken. De NDSI maakt echter geen onderscheid tussen waterlichamen en sneeuw. (Meer info:) De NDSI wordt berekend als: NDSI = { GREEN - SWIR \\over GREEN + SWIR}. NDSI-tijdseries Als voordeel nemen we een punt in de Zwitsere Alpen. Zet ergens een willekeurig punt in de bergen. Vervolgens initieren we een Sentinel-2 collectie, maar ditmaal gebruik makend van de TOA-collectie (dus niet atmosferisch gecorrigeerd). De achterliggende reden is dat deze collectie zich langer uitstrekt binnen Earth Engine, terwijl de Sentinel-2 'Surfance Reflectance' pas sinds 2019 wereldwijd systematisch wordt toegevoegd. In volgende code wordt: De NDSI berekend en toegevoegd over de beeldcollectie De beeldcollectie wordt gefilterd op basis van de ROI. Het beeld met minste wolkbedekking gevisualiseerd De NDSI voor dit beeld gevisualiseerd. //CloudMask + NDSI berekenen function maskS2clouds ( image ) { var qa = image . select ( 'QA60' ); var cloudBitMask = 1 << 10 ; var cirrusBitMask = 1 << 11 ; var mask = qa . bitwiseAnd ( cloudBitMask ). eq ( 0 ) . and ( qa . bitwiseAnd ( cirrusBitMask ). eq ( 0 )); return image . updateMask ( mask ); } // Indicesberekenen var addNDSI = function ( image ) { var ndsi = image . normalizedDifference ([ 'B3' , 'B11' ]). rename ( 'NDSI' ); return image . addBands ( ndsi ); }; //Satellietdata klaarzetten var S2 = ee . ImageCollection ( 'COPERNICUS/S2' ) //Gebruik van TOA, gezien groter temporele bereik . filterBounds ( ROI ) . map ( maskS2clouds ) . map ( addNDSI ); var visParams = { bands : [ 'B4' , 'B3' , 'B2' ], min : 0 , max : 3000 , gamma : 1.4 , }; Map . centerObject ( ROI , 9 ); Map . addLayer ( S2 . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ). first (), visParams ); //NDSI: geschikt voor onderscheid sneeuw, maar niet voor onderscheid met water Map . addLayer ( S2 . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ). first (). select ( 'NDSI' ), NDSI_params , 'NDSI' ) Vervolgens maken we een Chart aan van de NDSI, over de hele collectie: // Create and display a time series chart var fontS = 18 ; var Chart_NDSI = ui . Chart . image . series ( S2 . select ([ 'NDSI' ]), ROI , ee . Reducer . mean (), 100 ); print ( Chart_NDSI . setOptions ({ fontSize : fontS , hAxis : { title : 'Time (-)' }, vAxis : { title : 'Index mean (-)' }, lineWidth : 2 , pointSize : 4 , interpolateNulls : true , legend : { position : 'upper right' } })); Een andere visualisatiemogelijkheid is om de NDSI per jaar te plotten als 'Day of Year' (DOY): // Create and display a DOY time series chart print ( ui . Chart . image . doySeriesByYear ( S2 , 'NDSI' , ROI , ee . Reducer . mean (), 10 ). setOptions ({ fontSize : fontS , hAxis : { title : 'DOY (-)' }, vAxis : { title : 'NDSI (-)' }, lineWidth : 2 , pointSize : 4 , interpolateNulls : true , legend : { position : 'upper right' } })); Opdracht Visualiseer en analyseer de landdynamica op 3 locaties gebaseerd op basis van Sentinel-2 beelden: * Verdeel onder je break-out room volgende locaties : - Greenland - Amazon - Botswana (Okavango delta) - Portugal - Namibia (Etosha pan) - Indonesia - Slovenia Plot verschillende indices: NDVI, MNDWI en NDSI Deel de resultaten met elkaar bespreek de variatie binnen doorheen de verschillende jaren. Bedankt! Bedankt aan Lisa Landuyt voor de input van deze scriptjes.","title":"P7 Intro"},{"location":"P7/P7-Intro.html#tijdserieplots-aanmaken-in-google-earth-engine","text":"","title":"Tijdserieplots aanmaken in Google Earth Engine"},{"location":"P7/P7-Intro.html#de-ndsi-index","text":"In onderstaand voorbeeldje bekijken we de aanwezigheid van sneeuw in Joensuu, Finland doorheen de afgelopen jaren. Hiervoor maken we gebruik van alweer een nieuwe index: de Normalized Difference Snow Index (NDSI) . Deze index wordt gebruikt om de aanwezigheid van sneeuw/ijs te benadrukken, ten opzichte van de andere landbekkingsklassen waaronder wolken. De NDSI maakt echter geen onderscheid tussen waterlichamen en sneeuw. (Meer info:) De NDSI wordt berekend als: NDSI = { GREEN - SWIR \\over GREEN + SWIR}.","title":"De NDSI index"},{"location":"P7/P7-Intro.html#ndsi-tijdseries","text":"Als voordeel nemen we een punt in de Zwitsere Alpen. Zet ergens een willekeurig punt in de bergen. Vervolgens initieren we een Sentinel-2 collectie, maar ditmaal gebruik makend van de TOA-collectie (dus niet atmosferisch gecorrigeerd). De achterliggende reden is dat deze collectie zich langer uitstrekt binnen Earth Engine, terwijl de Sentinel-2 'Surfance Reflectance' pas sinds 2019 wereldwijd systematisch wordt toegevoegd. In volgende code wordt: De NDSI berekend en toegevoegd over de beeldcollectie De beeldcollectie wordt gefilterd op basis van de ROI. Het beeld met minste wolkbedekking gevisualiseerd De NDSI voor dit beeld gevisualiseerd. //CloudMask + NDSI berekenen function maskS2clouds ( image ) { var qa = image . select ( 'QA60' ); var cloudBitMask = 1 << 10 ; var cirrusBitMask = 1 << 11 ; var mask = qa . bitwiseAnd ( cloudBitMask ). eq ( 0 ) . and ( qa . bitwiseAnd ( cirrusBitMask ). eq ( 0 )); return image . updateMask ( mask ); } // Indicesberekenen var addNDSI = function ( image ) { var ndsi = image . normalizedDifference ([ 'B3' , 'B11' ]). rename ( 'NDSI' ); return image . addBands ( ndsi ); }; //Satellietdata klaarzetten var S2 = ee . ImageCollection ( 'COPERNICUS/S2' ) //Gebruik van TOA, gezien groter temporele bereik . filterBounds ( ROI ) . map ( maskS2clouds ) . map ( addNDSI ); var visParams = { bands : [ 'B4' , 'B3' , 'B2' ], min : 0 , max : 3000 , gamma : 1.4 , }; Map . centerObject ( ROI , 9 ); Map . addLayer ( S2 . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ). first (), visParams ); //NDSI: geschikt voor onderscheid sneeuw, maar niet voor onderscheid met water Map . addLayer ( S2 . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ). first (). select ( 'NDSI' ), NDSI_params , 'NDSI' ) Vervolgens maken we een Chart aan van de NDSI, over de hele collectie: // Create and display a time series chart var fontS = 18 ; var Chart_NDSI = ui . Chart . image . series ( S2 . select ([ 'NDSI' ]), ROI , ee . Reducer . mean (), 100 ); print ( Chart_NDSI . setOptions ({ fontSize : fontS , hAxis : { title : 'Time (-)' }, vAxis : { title : 'Index mean (-)' }, lineWidth : 2 , pointSize : 4 , interpolateNulls : true , legend : { position : 'upper right' } })); Een andere visualisatiemogelijkheid is om de NDSI per jaar te plotten als 'Day of Year' (DOY): // Create and display a DOY time series chart print ( ui . Chart . image . doySeriesByYear ( S2 , 'NDSI' , ROI , ee . Reducer . mean (), 10 ). setOptions ({ fontSize : fontS , hAxis : { title : 'DOY (-)' }, vAxis : { title : 'NDSI (-)' }, lineWidth : 2 , pointSize : 4 , interpolateNulls : true , legend : { position : 'upper right' } }));","title":"NDSI-tijdseries"},{"location":"P7/P7-Intro.html#opdracht","text":"Visualiseer en analyseer de landdynamica op 3 locaties gebaseerd op basis van Sentinel-2 beelden: * Verdeel onder je break-out room volgende locaties : - Greenland - Amazon - Botswana (Okavango delta) - Portugal - Namibia (Etosha pan) - Indonesia - Slovenia Plot verschillende indices: NDVI, MNDWI en NDSI Deel de resultaten met elkaar bespreek de variatie binnen doorheen de verschillende jaren.","title":"Opdracht"},{"location":"P7/P7-Intro.html#bedankt","text":"Bedankt aan Lisa Landuyt voor de input van deze scriptjes.","title":"Bedankt!"},{"location":"P7/P7-SAR-tijdseries.html","text":"Sentinel-1 composite Mato Groso, Brazil. (Source: [BELSPO](https://eo.belspo.be/nl/news/sentinel-1-captures-mato-grosso)) SAR: 'Synthetic Aperture Radar' Dit laatste onderdeel vormt een introductie tot het verwerken en bekijken van SAR-data in Google Earth Engine. SAR staat voor 'Synthetic Aperture Radar' (NL: Apertuursyntheseradar). Het is wellicht het meest gebruikte type Radar-systeem binnen de Remote Sensing en biedt hoge resolutie radarbeelden. Het voordeel van Radar ten opzichte van multispectrale beelden: SAR-golven penetreren door wolkbedekking, waardoor wolken geen belemmering vormen tijden het monitoren van landbekking. Zeker in tropische landen biedt deze capaciteit van een sterk voordeel bv tegen illegale ontbossing gedurende grote regenperiodes. Afhankelijk van het type SAR, penetreren de SAR-golven (gedeeltelijk) doorheen het kronendak, waardoor betere biomassa- en structuurinschattingen mogelijk zijn. SAR remote sensing is een actieve vorm van teledetectie: de satelliet zorgt zelf voor energiebron. Hierdoor is het ook tijdsonafhankelijk en kan het zowel gedurende de dag- als nacht opereren. Bijgevolg is SAR een uitstekend middel om aan bosmonitoring, flood mapping en disaster management te doen. Voor de ge\u00efnteresseerden: The SAR handbook (NASA, 2019) vormt een uitstekend handboek waar zowel de theoretische achtergrond als enkele toepassingen worden uitgelicht. Onderstaande voorbeelden hebben als doel slechts een introductie te bieden tot het gebruik van SAR in Google Earth Engine. Er wordt gewerkt met Sentinel-1 data. Over Sentinel-1 Sentinel-1 is een onderdeel van het ESA Copernicus-programma. Deze missie bestaat net zoals Sentinel-2 uit twee satellieten die 180\u00b0 tegenoverklaar in orbit rond de aarde zweven. De waarnemingen gebeuren in de C-band (ca. 5,405 GHZ). Door deze positie dekt de missie om de twee weken de totale aarde. De meest voorkomende S1-doelstellingen zijn: Het opvolgen van zee-ijs verschuivingen In kaart brengen van humanitaire hulp in crisistijd Monitoring van het mariene milieu Monitoring van landbouw en bosbouw Voorbeeld 1 - Near-Real Time Forest Monitoring (NRTM) Over NRTM Near-Real Time Forest Monitoring of NRTM is het principe waarbij bosbedekking in gebieden met veel illegale houtkap, van nabij wordt gemonitord waarbij steeds meest recente beschikbare satellietdata wordt gebruikt om eventuele wijzigingen in het kronendak op te sporen. Hierdoor kunnen eventuele illegale houtkapactiviteiten worden onderschept en stopgezet, om erger te vermijden. Visualiseren van Sentinel-1 data Open een nieuw scriptje in Earth Engine. Zoek naar Sentinel-1 in de zoekbalk en lees kort de achtergrondinformatie door. Sentinel-1 beelden bevat verschillende polarisaties: HH, HV, VV, VH. Uit de theorieles weten we dat bijvoorbeeld 'VV' staat voor verticaal gepolariseerd signaal uit en verticaal gepolariseerd resultaat ontvangen. We starten met de Sentinel-1 collectie (in earth engine: COPERNICUS/S1_GRD) in te laden en te filteren op basis van een Region of interest. In dit geval focussen we ons op een stuk tropisch regenwoud, ten oosten van het Brokopondo-meer in Suriname. // Load Sentinel 1 C band SAR Ground Range collection (log scale, VV, descending) Map . centerObject ( ROI , 13 ); //VV-polarisatie var VV_coll = ee . ImageCollection ( 'COPERNICUS/S1_GRD' ) . filter ( ee . Filter . listContains ( 'transmitterReceiverPolarisation' , 'VV' )) . filter ( ee . Filter . eq ( 'instrumentMode' , 'IW' )) . filter ( ee . Filter . eq ( 'orbitProperties_pass' , 'DESCENDING' )) . select ( 'VV' ) . map ( function ( image ) { var edge = image . lt ( - 30.0 ); var maskedImage = image . mask (). and ( edge . not ()); return image . updateMask ( maskedImage ); }); //VH-polarisatie var VH_coll = ee . ImageCollection ( 'COPERNICUS/S1_GRD' ) . filter ( ee . Filter . listContains ( 'transmitterReceiverPolarisation' , 'VH' )) . filter ( ee . Filter . eq ( 'instrumentMode' , 'IW' )) . filter ( ee . Filter . eq ( 'orbitProperties_pass' , 'DESCENDING' )) . select ( 'VH' ) . map ( function ( image ) { var edge = image . lt ( - 30.0 ); var maskedImage = image . mask (). and ( edge . not ()); return image . updateMask ( maskedImage ); }); In dit voorbeeld bekijke we voor 3 tijdstippen de verandering in bosbedekking van dit gebiedje; 2017, 2018, 2019. Maak 3 VV_beelden aan per jaar. Hieronder een voorbeeldje voor 2017. We nemen de gemiddelde waarde voor de periode Augustus. // Filteren op basis van locatie en tijdstip var VV_2017 = VV_coll . filterBounds ( ROI ). filterDate ( '2017-08-01' , '2017-08-31' ). mean () Laat ons eens bekijken hoe elk afzonderlijk VV-gepolariseerd beeld per jaar er uit ziet: var VV_Param = {\"opacity\":1,\"bands\":[\"VV\"],\"min\":-12.695602621422937,\"max\":-2.5938492158251467,\"gamma\":1}; // Afzonderlijke beelden mappen: visueel weinig verschil te zien Map.addLayer(VV_2017,VV_Param,'VV_2017',0) Map.addLayer(VV_2018,VV_Param,'VV_2018',0) Map.addLayer(VV_2018,VV_Param,'VV_2019',0) Doe nu hetzelfde, maar voor de VH-polarisatie. Bekijk in welke maat de sensitiviteit voor verschillende landoppervlakten verschilt in beide polarisaties. Op zich bieden de beelden afzonderlijk weinig informatie. In een volgende stappen cre\u00eberen we een beeldcomposiet, met volgende samenstelling: RGB= ['VV_2017', 'VV_2018', 'VV_2019']. Wat valt je op? //Afzonderlijke jaren samenvoegen var VV_yearly = ee . Image ( VV_2017 ). addBands ( VV_2018 ). addBands ( VV_2019 ); Map . addLayer ( VV_yearly , { min : - 20 , max : - 5 }, 'VV_2017_2018_2019' ) Tracht nu ook hetzelfde te doen voor de VH_polarisaties. Laat ons nu even kijken naar de verschillen tussen 2 jaren. Onderstaande redenering is als volgt: door het aftrekken van VV_2017 van VV_2019, worden de pixels waar in 2017 wel nog bos waren, maar in 2019 niet meer negatief. Uit trial-and-error weten we dat bij grove benadering de verschilwaarden van (VV_2019-VV_2017) tussen -6 en -15 overeenkomen met ontbossing. //Tussen 2 jaren: verschil nemen (2019 - 2017) var VV_20172019 = VV_2019 . subtract ( VV_2017 ) //Verschilbeeld illustreren Map . addLayer ( VV_20172019 ,{ min :- 13 , max : 10 }, 'Diff_2017_2019' ) // REMAP: om verlies te bepalen (-6 tot -15 komt grofweg overeen met ontbossing) var VV_20172019 = VV_20172019 . ceil (); // afronden naar boven var loss_20172019 = VV_20172019 . remap ([ - 6 , - 7 , - 8 , - 9 , - 10 , - 11 , - 12 , - 13 , - 14 , - 15 ],[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]) Map . addLayer ( loss_20172019 ,{ palette : 'red' }, 'Loss_2017_2019' ) Plot tevens ook 2 S2-beelden (van 2017 en 2019) en bekijk ook hier de ontbossing. Opdracht 1: Mangrove monitoring met SAR Bekijk aan de hand van bovenstaand voorbeeld ook de wijzigingen in het mangrovegebied langs de volledige kustlijn in Suriname. Tussen 2018-2020. Wat valt je op? Welk type ontbossing is dit? Benader ook de aanwas van mangrove tussen deze jaren. De verschilwaarden tussen 7 en 13 kun je als grove grenzen nemen als wat 'aanwas' van mangrove is. Voorbeeld 2 - Kartering van overstromingen met Sentinel-1 Link naar het volledige script: https://code.earthengine.google.com/b5b90aa2d98aa8c3677679d22e601d19 Achtergrond SAR-gebaseerde overstromingskartering is een betrouwbare manier om op een snelle manier een inschatting te verkrijgen van de oppervlak waarover een overstroming zich heeft uitgestrekt. Gezien de onafhankelijkheid van de weersomstandigheden, kan deze cruciale informatie steeds asap worden afgeleid. In wat volgt bekijken we een methode om op een snelle en eenvoudige manier dergelijke inschatting te maken, op basis van een analyse op 2 tijdstippen: voor en na een overstroming. Het studiegebied betreft Beira, waar in 2019 een hevige cycloon het landschap danig aanttaste. De tol: 350 doden en 15000 die hun woning zagen verdwijnen ( Bron ). ROI: var ROI = ee.Geometry.Polygon( [[[35.53377589953368, -19.6674648789114], [34.50106105578368, -18.952058786515526], [33.63314113390868, -19.87423907259203], [34.74825343859618, -20.61123742951084]]]); Visualiseren van Sentinel-1 data Als eerste stap, filteren we de S1 collectie. Hierbij werken we uit eenvoud vooral met de VV polarisatie. Let op de naamgeving van de AOI/ROI. var S1_VV = ee . ImageCollection ( 'COPERNICUS/S1_GRD' ) //.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) . filter ( ee . Filter . eq ( 'instrumentMode' , 'IW' )) . filterBounds ( ROI ) . select ( 'VV' ) . map ( function ( image ) { var edge = image . lt ( - 30.0 ); var maskedImage = image . mask (). and ( edge . not ()); return image . updateMask ( maskedImage ); }); * Vervolgens geven we de data aan waarin we ge\u00efnteresseerd zijn: //Before dates var before_start = '2019-03-01' ; var before_end = '2019-03-10' ; //After dates var after_start = '2019-03-10' ; var after_end = '2019-03-23' ; //Selecteer data voor en na het event var before_collection = S1_VV . filterDate ( before_start , before_end ); var after_collection = S1_VV . filterDate ( after_start , after_end ); // Bekijken van de geselecteerde collecties: print ( 'Tiles selected: Before Flood' , before_collection ); //5 beelden print ( 'Tiles selected: After Flood' , after_collection ); //12 beelden Hierna kunnen we 2 VV-beelden aanmaken met een mean()-reducer: Voor- en na. Tevens zorgen we voor een 'speckle'-filter dat het peper-zout effect typerend aan radar wat reduceert. Hierna kunnen we het beeld visualiseren. //------------------------------ DISPLAY PRODUCTS ----------------------------------// // Before and after flood SAR mosaic Map . centerObject ( ROI , 8 ); Map . addLayer ( before_filtered , { min :- 25 , max : 0 }, 'Before Flood' , 0 ); Map . addLayer ( after_filtered , { min :- 25 , max : 0 }, 'After Flood' , 1 ); We hebben nu de 2 VV-beelden (voor/na de overstroming). Op basis van het verschil van beide, kunnen we informatie over de impact van de overstroming achterhalen. //------------------------------- FLOOD EXTENT CALCULATION -------------------------------// // Calculate the difference between the before and after images var difference = after_filtered . divide ( before_filtered ); var threshold = 1.25 ; //Via Trial- and error dit bekomen var difference_binary = difference . gt ( threshold ); Volgend stukje code verfijnt het verschilbeeld op basis van additionele datasets. Hierdoor worden de seizoenale overstromingen (die dus geen onderdeel van de ramp zijn) uit het verschilbeeld weerhouden. Daarnaast zorgt een connectiveitsberekening ook dat enkele pixels worden gewist en enkel de 'grotere' vlakken overblijven. // Refine flood result using additional datasets // Include JRC layer on surface water seasonality to mask flood pixels from areas // of \"permanent\" water (where there is water > 10 months of the year) var swater = ee . Image ( 'JRC/GSW1_0/GlobalSurfaceWater' ). select ( 'seasonality' ); var swater_mask = swater . gte ( 10 ). updateMask ( swater . gte ( 10 )); //Flooded layer where perennial water bodies (water > 10 mo/yr) is assigned a 0 value var flooded_mask = difference_binary . where ( swater_mask , 0 ); // final flooded area without pixels in perennial waterbodies var flooded = flooded_mask . updateMask ( flooded_mask ); // Compute connectivity of pixels to eliminate those connected to 8 or fewer neighbours // This operation reduces noise of the flood extent product var connections = flooded . connectedPixelCount (); var flooded = flooded . updateMask ( connections . gte ( 8 )); Op basis van het verkregen flooded -beeld kunnen we oppervlakteberekeningen uitvoeren. // Calculate flood extent area // Create a raster layer containing the area information of each pixel var flood_pixelarea = flooded . select ( 'VV' ) . multiply ( ee . Image . pixelArea ()); // Sum the areas of flooded pixels // default is set to 'bestEffort: true' in order to reduce compuation time, for a more // accurate result set bestEffort to false and increase 'maxPixels'. var flood_stats = flood_pixelarea . reduceRegion ({ reducer : ee . Reducer . sum (), geometry : ROI , scale : 10 , // native resolution //maxPixels: 1e9, bestEffort : true }); // Convert the flood extent to hectares (area calculations are originally given in meters) var flood_area_ha = flood_stats . getNumber ( 'VV' ) . divide ( 10000 ) . round (); Tot slot visualiseren we ons resultaat: // Difference layer Map . addLayer ( difference ,{ min : 0 , max : 2 }, \"Difference Layer\" , 0 ); // Flooded areas Map . addLayer ( flooded ,{ palette : \"0000FF\" }, 'Flooded areas' ); print ( 'Oppervlakte overstromingsgebied' , flood_area_ha ) Bronvermelding Inspiratie voor bovenstaande code via UN-Spider . Opdracht Bekijk bovenstaande code voor het gebied. Lees het even door en ga na welke stappen er in de procedure werden opgenomen. Bereken het overstromingsgebied voor de overstromingen die plaatsvonden in Kerala, India in Augustus 2018. Dit was een zeldzame overstroming, die ervoor zorgde dat ongeveer een miljoen inwoners in het gebied hun woningen zagen worden weggespoeld. Een Landsat-8 foto van voor en na de overstroming vind je via de website van visible earth . Oplossing Via dit scriptje: https://code.earthengine.google.com/70bf32b42c7f233120352fc5fc1134f5","title":"SAR-analyse"},{"location":"P7/P7-SAR-tijdseries.html#sar-synthetic-aperture-radar","text":"Dit laatste onderdeel vormt een introductie tot het verwerken en bekijken van SAR-data in Google Earth Engine. SAR staat voor 'Synthetic Aperture Radar' (NL: Apertuursyntheseradar). Het is wellicht het meest gebruikte type Radar-systeem binnen de Remote Sensing en biedt hoge resolutie radarbeelden. Het voordeel van Radar ten opzichte van multispectrale beelden: SAR-golven penetreren door wolkbedekking, waardoor wolken geen belemmering vormen tijden het monitoren van landbekking. Zeker in tropische landen biedt deze capaciteit van een sterk voordeel bv tegen illegale ontbossing gedurende grote regenperiodes. Afhankelijk van het type SAR, penetreren de SAR-golven (gedeeltelijk) doorheen het kronendak, waardoor betere biomassa- en structuurinschattingen mogelijk zijn. SAR remote sensing is een actieve vorm van teledetectie: de satelliet zorgt zelf voor energiebron. Hierdoor is het ook tijdsonafhankelijk en kan het zowel gedurende de dag- als nacht opereren. Bijgevolg is SAR een uitstekend middel om aan bosmonitoring, flood mapping en disaster management te doen. Voor de ge\u00efnteresseerden: The SAR handbook (NASA, 2019) vormt een uitstekend handboek waar zowel de theoretische achtergrond als enkele toepassingen worden uitgelicht. Onderstaande voorbeelden hebben als doel slechts een introductie te bieden tot het gebruik van SAR in Google Earth Engine. Er wordt gewerkt met Sentinel-1 data. Over Sentinel-1 Sentinel-1 is een onderdeel van het ESA Copernicus-programma. Deze missie bestaat net zoals Sentinel-2 uit twee satellieten die 180\u00b0 tegenoverklaar in orbit rond de aarde zweven. De waarnemingen gebeuren in de C-band (ca. 5,405 GHZ). Door deze positie dekt de missie om de twee weken de totale aarde. De meest voorkomende S1-doelstellingen zijn: Het opvolgen van zee-ijs verschuivingen In kaart brengen van humanitaire hulp in crisistijd Monitoring van het mariene milieu Monitoring van landbouw en bosbouw","title":"SAR: 'Synthetic Aperture Radar'"},{"location":"P7/P7-SAR-tijdseries.html#voorbeeld-1-near-real-time-forest-monitoring-nrtm","text":"","title":"Voorbeeld 1 - Near-Real Time Forest Monitoring (NRTM)"},{"location":"P7/P7-SAR-tijdseries.html#over-nrtm","text":"Near-Real Time Forest Monitoring of NRTM is het principe waarbij bosbedekking in gebieden met veel illegale houtkap, van nabij wordt gemonitord waarbij steeds meest recente beschikbare satellietdata wordt gebruikt om eventuele wijzigingen in het kronendak op te sporen. Hierdoor kunnen eventuele illegale houtkapactiviteiten worden onderschept en stopgezet, om erger te vermijden.","title":"Over NRTM"},{"location":"P7/P7-SAR-tijdseries.html#visualiseren-van-sentinel-1-data","text":"Open een nieuw scriptje in Earth Engine. Zoek naar Sentinel-1 in de zoekbalk en lees kort de achtergrondinformatie door. Sentinel-1 beelden bevat verschillende polarisaties: HH, HV, VV, VH. Uit de theorieles weten we dat bijvoorbeeld 'VV' staat voor verticaal gepolariseerd signaal uit en verticaal gepolariseerd resultaat ontvangen. We starten met de Sentinel-1 collectie (in earth engine: COPERNICUS/S1_GRD) in te laden en te filteren op basis van een Region of interest. In dit geval focussen we ons op een stuk tropisch regenwoud, ten oosten van het Brokopondo-meer in Suriname. // Load Sentinel 1 C band SAR Ground Range collection (log scale, VV, descending) Map . centerObject ( ROI , 13 ); //VV-polarisatie var VV_coll = ee . ImageCollection ( 'COPERNICUS/S1_GRD' ) . filter ( ee . Filter . listContains ( 'transmitterReceiverPolarisation' , 'VV' )) . filter ( ee . Filter . eq ( 'instrumentMode' , 'IW' )) . filter ( ee . Filter . eq ( 'orbitProperties_pass' , 'DESCENDING' )) . select ( 'VV' ) . map ( function ( image ) { var edge = image . lt ( - 30.0 ); var maskedImage = image . mask (). and ( edge . not ()); return image . updateMask ( maskedImage ); }); //VH-polarisatie var VH_coll = ee . ImageCollection ( 'COPERNICUS/S1_GRD' ) . filter ( ee . Filter . listContains ( 'transmitterReceiverPolarisation' , 'VH' )) . filter ( ee . Filter . eq ( 'instrumentMode' , 'IW' )) . filter ( ee . Filter . eq ( 'orbitProperties_pass' , 'DESCENDING' )) . select ( 'VH' ) . map ( function ( image ) { var edge = image . lt ( - 30.0 ); var maskedImage = image . mask (). and ( edge . not ()); return image . updateMask ( maskedImage ); }); In dit voorbeeld bekijke we voor 3 tijdstippen de verandering in bosbedekking van dit gebiedje; 2017, 2018, 2019. Maak 3 VV_beelden aan per jaar. Hieronder een voorbeeldje voor 2017. We nemen de gemiddelde waarde voor de periode Augustus. // Filteren op basis van locatie en tijdstip var VV_2017 = VV_coll . filterBounds ( ROI ). filterDate ( '2017-08-01' , '2017-08-31' ). mean () Laat ons eens bekijken hoe elk afzonderlijk VV-gepolariseerd beeld per jaar er uit ziet: var VV_Param = {\"opacity\":1,\"bands\":[\"VV\"],\"min\":-12.695602621422937,\"max\":-2.5938492158251467,\"gamma\":1}; // Afzonderlijke beelden mappen: visueel weinig verschil te zien Map.addLayer(VV_2017,VV_Param,'VV_2017',0) Map.addLayer(VV_2018,VV_Param,'VV_2018',0) Map.addLayer(VV_2018,VV_Param,'VV_2019',0) Doe nu hetzelfde, maar voor de VH-polarisatie. Bekijk in welke maat de sensitiviteit voor verschillende landoppervlakten verschilt in beide polarisaties. Op zich bieden de beelden afzonderlijk weinig informatie. In een volgende stappen cre\u00eberen we een beeldcomposiet, met volgende samenstelling: RGB= ['VV_2017', 'VV_2018', 'VV_2019']. Wat valt je op? //Afzonderlijke jaren samenvoegen var VV_yearly = ee . Image ( VV_2017 ). addBands ( VV_2018 ). addBands ( VV_2019 ); Map . addLayer ( VV_yearly , { min : - 20 , max : - 5 }, 'VV_2017_2018_2019' ) Tracht nu ook hetzelfde te doen voor de VH_polarisaties. Laat ons nu even kijken naar de verschillen tussen 2 jaren. Onderstaande redenering is als volgt: door het aftrekken van VV_2017 van VV_2019, worden de pixels waar in 2017 wel nog bos waren, maar in 2019 niet meer negatief. Uit trial-and-error weten we dat bij grove benadering de verschilwaarden van (VV_2019-VV_2017) tussen -6 en -15 overeenkomen met ontbossing. //Tussen 2 jaren: verschil nemen (2019 - 2017) var VV_20172019 = VV_2019 . subtract ( VV_2017 ) //Verschilbeeld illustreren Map . addLayer ( VV_20172019 ,{ min :- 13 , max : 10 }, 'Diff_2017_2019' ) // REMAP: om verlies te bepalen (-6 tot -15 komt grofweg overeen met ontbossing) var VV_20172019 = VV_20172019 . ceil (); // afronden naar boven var loss_20172019 = VV_20172019 . remap ([ - 6 , - 7 , - 8 , - 9 , - 10 , - 11 , - 12 , - 13 , - 14 , - 15 ],[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]) Map . addLayer ( loss_20172019 ,{ palette : 'red' }, 'Loss_2017_2019' ) Plot tevens ook 2 S2-beelden (van 2017 en 2019) en bekijk ook hier de ontbossing.","title":"Visualiseren van Sentinel-1 data"},{"location":"P7/P7-SAR-tijdseries.html#opdracht-1-mangrove-monitoring-met-sar","text":"Bekijk aan de hand van bovenstaand voorbeeld ook de wijzigingen in het mangrovegebied langs de volledige kustlijn in Suriname. Tussen 2018-2020. Wat valt je op? Welk type ontbossing is dit? Benader ook de aanwas van mangrove tussen deze jaren. De verschilwaarden tussen 7 en 13 kun je als grove grenzen nemen als wat 'aanwas' van mangrove is.","title":"Opdracht 1: Mangrove monitoring met SAR"},{"location":"P7/P7-SAR-tijdseries.html#voorbeeld-2-kartering-van-overstromingen-met-sentinel-1","text":"Link naar het volledige script: https://code.earthengine.google.com/b5b90aa2d98aa8c3677679d22e601d19","title":"Voorbeeld 2 - Kartering van overstromingen met Sentinel-1"},{"location":"P7/P7-SAR-tijdseries.html#achtergrond","text":"SAR-gebaseerde overstromingskartering is een betrouwbare manier om op een snelle manier een inschatting te verkrijgen van de oppervlak waarover een overstroming zich heeft uitgestrekt. Gezien de onafhankelijkheid van de weersomstandigheden, kan deze cruciale informatie steeds asap worden afgeleid. In wat volgt bekijken we een methode om op een snelle en eenvoudige manier dergelijke inschatting te maken, op basis van een analyse op 2 tijdstippen: voor en na een overstroming. Het studiegebied betreft Beira, waar in 2019 een hevige cycloon het landschap danig aanttaste. De tol: 350 doden en 15000 die hun woning zagen verdwijnen ( Bron ). ROI: var ROI = ee.Geometry.Polygon( [[[35.53377589953368, -19.6674648789114], [34.50106105578368, -18.952058786515526], [33.63314113390868, -19.87423907259203], [34.74825343859618, -20.61123742951084]]]);","title":"Achtergrond"},{"location":"P7/P7-SAR-tijdseries.html#visualiseren-van-sentinel-1-data_1","text":"Als eerste stap, filteren we de S1 collectie. Hierbij werken we uit eenvoud vooral met de VV polarisatie. Let op de naamgeving van de AOI/ROI. var S1_VV = ee . ImageCollection ( 'COPERNICUS/S1_GRD' ) //.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) . filter ( ee . Filter . eq ( 'instrumentMode' , 'IW' )) . filterBounds ( ROI ) . select ( 'VV' ) . map ( function ( image ) { var edge = image . lt ( - 30.0 ); var maskedImage = image . mask (). and ( edge . not ()); return image . updateMask ( maskedImage ); }); * Vervolgens geven we de data aan waarin we ge\u00efnteresseerd zijn: //Before dates var before_start = '2019-03-01' ; var before_end = '2019-03-10' ; //After dates var after_start = '2019-03-10' ; var after_end = '2019-03-23' ; //Selecteer data voor en na het event var before_collection = S1_VV . filterDate ( before_start , before_end ); var after_collection = S1_VV . filterDate ( after_start , after_end ); // Bekijken van de geselecteerde collecties: print ( 'Tiles selected: Before Flood' , before_collection ); //5 beelden print ( 'Tiles selected: After Flood' , after_collection ); //12 beelden Hierna kunnen we 2 VV-beelden aanmaken met een mean()-reducer: Voor- en na. Tevens zorgen we voor een 'speckle'-filter dat het peper-zout effect typerend aan radar wat reduceert. Hierna kunnen we het beeld visualiseren. //------------------------------ DISPLAY PRODUCTS ----------------------------------// // Before and after flood SAR mosaic Map . centerObject ( ROI , 8 ); Map . addLayer ( before_filtered , { min :- 25 , max : 0 }, 'Before Flood' , 0 ); Map . addLayer ( after_filtered , { min :- 25 , max : 0 }, 'After Flood' , 1 ); We hebben nu de 2 VV-beelden (voor/na de overstroming). Op basis van het verschil van beide, kunnen we informatie over de impact van de overstroming achterhalen. //------------------------------- FLOOD EXTENT CALCULATION -------------------------------// // Calculate the difference between the before and after images var difference = after_filtered . divide ( before_filtered ); var threshold = 1.25 ; //Via Trial- and error dit bekomen var difference_binary = difference . gt ( threshold ); Volgend stukje code verfijnt het verschilbeeld op basis van additionele datasets. Hierdoor worden de seizoenale overstromingen (die dus geen onderdeel van de ramp zijn) uit het verschilbeeld weerhouden. Daarnaast zorgt een connectiveitsberekening ook dat enkele pixels worden gewist en enkel de 'grotere' vlakken overblijven. // Refine flood result using additional datasets // Include JRC layer on surface water seasonality to mask flood pixels from areas // of \"permanent\" water (where there is water > 10 months of the year) var swater = ee . Image ( 'JRC/GSW1_0/GlobalSurfaceWater' ). select ( 'seasonality' ); var swater_mask = swater . gte ( 10 ). updateMask ( swater . gte ( 10 )); //Flooded layer where perennial water bodies (water > 10 mo/yr) is assigned a 0 value var flooded_mask = difference_binary . where ( swater_mask , 0 ); // final flooded area without pixels in perennial waterbodies var flooded = flooded_mask . updateMask ( flooded_mask ); // Compute connectivity of pixels to eliminate those connected to 8 or fewer neighbours // This operation reduces noise of the flood extent product var connections = flooded . connectedPixelCount (); var flooded = flooded . updateMask ( connections . gte ( 8 )); Op basis van het verkregen flooded -beeld kunnen we oppervlakteberekeningen uitvoeren. // Calculate flood extent area // Create a raster layer containing the area information of each pixel var flood_pixelarea = flooded . select ( 'VV' ) . multiply ( ee . Image . pixelArea ()); // Sum the areas of flooded pixels // default is set to 'bestEffort: true' in order to reduce compuation time, for a more // accurate result set bestEffort to false and increase 'maxPixels'. var flood_stats = flood_pixelarea . reduceRegion ({ reducer : ee . Reducer . sum (), geometry : ROI , scale : 10 , // native resolution //maxPixels: 1e9, bestEffort : true }); // Convert the flood extent to hectares (area calculations are originally given in meters) var flood_area_ha = flood_stats . getNumber ( 'VV' ) . divide ( 10000 ) . round (); Tot slot visualiseren we ons resultaat: // Difference layer Map . addLayer ( difference ,{ min : 0 , max : 2 }, \"Difference Layer\" , 0 ); // Flooded areas Map . addLayer ( flooded ,{ palette : \"0000FF\" }, 'Flooded areas' ); print ( 'Oppervlakte overstromingsgebied' , flood_area_ha ) Bronvermelding Inspiratie voor bovenstaande code via UN-Spider .","title":"Visualiseren van Sentinel-1 data"},{"location":"P7/P7-SAR-tijdseries.html#opdracht","text":"Bekijk bovenstaande code voor het gebied. Lees het even door en ga na welke stappen er in de procedure werden opgenomen. Bereken het overstromingsgebied voor de overstromingen die plaatsvonden in Kerala, India in Augustus 2018. Dit was een zeldzame overstroming, die ervoor zorgde dat ongeveer een miljoen inwoners in het gebied hun woningen zagen worden weggespoeld. Een Landsat-8 foto van voor en na de overstroming vind je via de website van visible earth . Oplossing Via dit scriptje: https://code.earthengine.google.com/70bf32b42c7f233120352fc5fc1134f5","title":"Opdracht"},{"location":"P7/P7-Timeseries.html","text":"Tijdserie analyse is een veel gebruikte operatie in Remote Sensing. Het draagt immers bij tot het modelleren van temporele (seizoenaal of lange termijn) patronen en het monitoren van landcover. Gezien het grote (historische) aanbod aan RS data is Google Earth Engine een zeer geschikt medium om tijdseries te gaan analyseren. Tijdserieplots aanmaken in Google Earth Engine De Normalized Difference Snow Index (NDSI) In onderstaand voorbeeldje bekijken we de aanwezigheid van sneeuw in Joensuu, Finland doorheen de afgelopen jaren. Hiervoor maken we gebruik van alweer een nieuwe index: de Normalized Difference Snow Index (NDSI) . Deze index wordt gebruikt om de aanwezigheid van sneeuw/ijs te benadrukken, ten opzichte van de andere landbekkingsklassen waaronder wolken. De NDSI maakt echter geen onderscheid tussen waterlichamen en sneeuw. (Meer info:) De NDSI wordt berekend als: NDSI = { GREEN - SWIR \\over GREEN + SWIR}. NDSI-tijdseries Als voordeel nemen we een punt in de Zwitsere Alpen. Zet ergens een willekeurig punt in de bergen. Vervolgens initieren we een Sentinel-2 collectie, maar ditmaal gebruik makend van de TOA-collectie (dus niet atmosferisch gecorrigeerd). De achterliggende reden is dat deze collectie zich langer uitstrekt binnen Earth Engine, terwijl de Sentinel-2 'Surfance Reflectance' pas sinds 2019 wereldwijd systematisch wordt toegevoegd. In volgende code wordt: De NDSI berekend en toegevoegd over de beeldcollectie De beeldcollectie wordt gefilterd op basis van de ROI. Het beeld met minste wolkbedekking gevisualiseerd De NDSI voor dit beeld gevisualiseerd. //CloudMask + NDSI berekenen function maskS2clouds ( image ) { var qa = image . select ( 'QA60' ); var cloudBitMask = 1 << 10 ; var cirrusBitMask = 1 << 11 ; var mask = qa . bitwiseAnd ( cloudBitMask ). eq ( 0 ) . and ( qa . bitwiseAnd ( cirrusBitMask ). eq ( 0 )); return image . updateMask ( mask ); } // Indicesberekenen var addNDSI = function ( image ) { var ndsi = image . normalizedDifference ([ 'B3' , 'B11' ]). rename ( 'NDSI' ); return image . addBands ( ndsi ); }; //Satellietdata klaarzetten var S2 = ee . ImageCollection ( 'COPERNICUS/S2' ) //Gebruik van TOA, gezien groter temporele bereik . filterBounds ( ROI ) . map ( maskS2clouds ) . map ( addNDSI ); var visParams = { bands : [ 'B4' , 'B3' , 'B2' ], min : 0 , max : 3000 , gamma : 1.4 , }; Map . centerObject ( ROI , 9 ); Map . addLayer ( S2 . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ). first (), visParams ); //NDSI: geschikt voor onderscheid sneeuw, maar niet voor onderscheid met water Map . addLayer ( S2 . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ). first (). select ( 'NDSI' ), NDSI_params , 'NDSI' ) Vervolgens maken we een Chart aan van de NDSI, over de hele collectie: // Create and display a time series chart var fontS = 18 ; var Chart_NDSI = ui . Chart . image . series ( S2 . select ([ 'NDSI' ]), ROI , ee . Reducer . mean (), 100 ); print ( Chart_NDSI . setOptions ({ fontSize : fontS , hAxis : { title : 'Time (-)' }, vAxis : { title : 'Index mean (-)' }, lineWidth : 2 , pointSize : 4 , interpolateNulls : true , legend : { position : 'upper right' } })); Een andere visualisatiemogelijkheid is om de NDSI per jaar te plotten als 'Day of Year' (DOY): // Create and display a DOY time series chart print ( ui . Chart . image . doySeriesByYear ( S2 , 'NDSI' , ROI , ee . Reducer . mean (), 10 ). setOptions ({ fontSize : fontS , hAxis : { title : 'DOY (-)' }, vAxis : { title : 'NDSI (-)' }, lineWidth : 2 , pointSize : 4 , interpolateNulls : true , legend : { position : 'upper right' } })); Opdracht Visualiseer en analyseer de landdynamica op 3 locaties gebaseerd op basis van Sentinel-2 beelden: * Verdeel onder je break-out room volgende locaties : - Greenland - Amazon - Botswana (Okavango delta) - Portugal - Namibia (Etosha pan) - Indonesia - Slovenia Plot verschillende indices: NDVI, MNDWI en NDSI Deel de resultaten met elkaar bespreek de variatie binnen doorheen de verschillende jaren. Bedankt! Bedankt aan Lisa Landuyt voor de input van deze scriptjes.","title":"Tijdserie-analyse"},{"location":"P7/P7-Timeseries.html#tijdserieplots-aanmaken-in-google-earth-engine","text":"","title":"Tijdserieplots aanmaken in Google Earth Engine"},{"location":"P7/P7-Timeseries.html#de-normalized-difference-snow-index-ndsi","text":"In onderstaand voorbeeldje bekijken we de aanwezigheid van sneeuw in Joensuu, Finland doorheen de afgelopen jaren. Hiervoor maken we gebruik van alweer een nieuwe index: de Normalized Difference Snow Index (NDSI) . Deze index wordt gebruikt om de aanwezigheid van sneeuw/ijs te benadrukken, ten opzichte van de andere landbekkingsklassen waaronder wolken. De NDSI maakt echter geen onderscheid tussen waterlichamen en sneeuw. (Meer info:) De NDSI wordt berekend als: NDSI = { GREEN - SWIR \\over GREEN + SWIR}.","title":"De Normalized Difference Snow Index (NDSI)"},{"location":"P7/P7-Timeseries.html#ndsi-tijdseries","text":"Als voordeel nemen we een punt in de Zwitsere Alpen. Zet ergens een willekeurig punt in de bergen. Vervolgens initieren we een Sentinel-2 collectie, maar ditmaal gebruik makend van de TOA-collectie (dus niet atmosferisch gecorrigeerd). De achterliggende reden is dat deze collectie zich langer uitstrekt binnen Earth Engine, terwijl de Sentinel-2 'Surfance Reflectance' pas sinds 2019 wereldwijd systematisch wordt toegevoegd. In volgende code wordt: De NDSI berekend en toegevoegd over de beeldcollectie De beeldcollectie wordt gefilterd op basis van de ROI. Het beeld met minste wolkbedekking gevisualiseerd De NDSI voor dit beeld gevisualiseerd. //CloudMask + NDSI berekenen function maskS2clouds ( image ) { var qa = image . select ( 'QA60' ); var cloudBitMask = 1 << 10 ; var cirrusBitMask = 1 << 11 ; var mask = qa . bitwiseAnd ( cloudBitMask ). eq ( 0 ) . and ( qa . bitwiseAnd ( cirrusBitMask ). eq ( 0 )); return image . updateMask ( mask ); } // Indicesberekenen var addNDSI = function ( image ) { var ndsi = image . normalizedDifference ([ 'B3' , 'B11' ]). rename ( 'NDSI' ); return image . addBands ( ndsi ); }; //Satellietdata klaarzetten var S2 = ee . ImageCollection ( 'COPERNICUS/S2' ) //Gebruik van TOA, gezien groter temporele bereik . filterBounds ( ROI ) . map ( maskS2clouds ) . map ( addNDSI ); var visParams = { bands : [ 'B4' , 'B3' , 'B2' ], min : 0 , max : 3000 , gamma : 1.4 , }; Map . centerObject ( ROI , 9 ); Map . addLayer ( S2 . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ). first (), visParams ); //NDSI: geschikt voor onderscheid sneeuw, maar niet voor onderscheid met water Map . addLayer ( S2 . sort ( 'CLOUDY_PIXEL_PERCENTAGE' ). first (). select ( 'NDSI' ), NDSI_params , 'NDSI' ) Vervolgens maken we een Chart aan van de NDSI, over de hele collectie: // Create and display a time series chart var fontS = 18 ; var Chart_NDSI = ui . Chart . image . series ( S2 . select ([ 'NDSI' ]), ROI , ee . Reducer . mean (), 100 ); print ( Chart_NDSI . setOptions ({ fontSize : fontS , hAxis : { title : 'Time (-)' }, vAxis : { title : 'Index mean (-)' }, lineWidth : 2 , pointSize : 4 , interpolateNulls : true , legend : { position : 'upper right' } })); Een andere visualisatiemogelijkheid is om de NDSI per jaar te plotten als 'Day of Year' (DOY): // Create and display a DOY time series chart print ( ui . Chart . image . doySeriesByYear ( S2 , 'NDSI' , ROI , ee . Reducer . mean (), 10 ). setOptions ({ fontSize : fontS , hAxis : { title : 'DOY (-)' }, vAxis : { title : 'NDSI (-)' }, lineWidth : 2 , pointSize : 4 , interpolateNulls : true , legend : { position : 'upper right' } }));","title":"NDSI-tijdseries"},{"location":"P7/P7-Timeseries.html#opdracht","text":"Visualiseer en analyseer de landdynamica op 3 locaties gebaseerd op basis van Sentinel-2 beelden: * Verdeel onder je break-out room volgende locaties : - Greenland - Amazon - Botswana (Okavango delta) - Portugal - Namibia (Etosha pan) - Indonesia - Slovenia Plot verschillende indices: NDVI, MNDWI en NDSI Deel de resultaten met elkaar bespreek de variatie binnen doorheen de verschillende jaren.","title":"Opdracht"},{"location":"P7/P7-Timeseries.html#bedankt","text":"Bedankt aan Lisa Landuyt voor de input van deze scriptjes.","title":"Bedankt!"}]}